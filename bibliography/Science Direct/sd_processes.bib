@article{RIEG20232790,
title = {Use of Online MIS in Management Accounting – Initial Results from an Empirical Study},
journal = {Procedia Computer Science},
volume = {225},
pages = {2790-2797},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.271},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014291},
author = {Robert Rieg and Patrick Sven Ulrich and Carmen Finckh},
keywords = {Management accounting, Management Information System, Online MIS, Digitization},
abstract = {Managerial accountants spend a large part of their working time on more operational activities in cost accounting, reporting, and operational planning and budgeting. In all these areas, there has been increasing discussion in recent years, both in theory and practice, about using more digital technologies. For reporting, this means not only an intensified discussion of technologies such as RPA and AI but also more intensive changes to existing reporting systems. In particular, management information systems (MIS), which are maintained by managerial accountants and used by managers for corporate management, should be mentioned here. Based on an empirical survey in a large German company, this article discusses the requirements and assessments of users when switching from a regular MIS to a cloud-based system.}
}
@article{DEVIATIAROVA2023254,
title = {Analyzing Proficiency Patterns in Test Results Using Clustering and Augmentation Algorithms},
journal = {Procedia Computer Science},
volume = {229},
pages = {254-264},
year = {2023},
note = {12th International Young Scientists Conference in Computational Science, YSC2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923020173},
author = {Ekaterina Deviatiarova and Sergei Fadeev and Alexey Dukhanov},
keywords = {student clustering, test results, educational assessments, proficiency analysis, cluster interpretation, data augmentation},
abstract = {This research investigates the utilization of student clustering based on test results and its potential impact on student grouping interpretation. The primary objective is to discern significant patterns in student performance by leveraging clustering methods. Through a careful exploration of the obtained clustering results, this study seeks to assess the feasibility of interpreting the outcomes. The research methodology involves the application of popular clustering algorithms on a dataset comprising student machine learning (ML) test results. The experiment aims to identify distinct proficiency patterns among students, shedding light on performance variations across different groups. These proficiency patterns offer valuable implications for educational data analysis, opening new avenues for personalized learning and test design improvements.}
}
@article{LIU20241323,
title = {A Study on Fake Review Detection Based on RoBERTa and Behavioral Features},
journal = {Procedia Computer Science},
volume = {242},
pages = {1323-1330},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.131},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018507},
author = {Jinhao Liu and Pei Quan and Wen Zhang},
keywords = {Fake review detection, Large language model, Imbalanced data},
abstract = {With the rapid development of e-commerce platforms, how to better identify and filter fake reviews has become an urgent issue for the healthy development of the e-commerce industry. However, traditional fake review identification methods cannot effectively solve this problem. They are easily affected by text changes, language differences, and context, and fake reviewers may take measures to blur their behavioral characteristics, making them difficult to detect by behavior-based algorithms. Large language models can capture the contextual relationships of the entire text through self-attention mechanisms, thereby understanding the overall meaning and emotional tendency of the review, which enables them to more effectively judge the authenticity of the review. In addition, large models have strong generalization ability and a certain degree of interpretability, which also makes them suitable for research on fake review text identification. Since fake reviews have an asymmetric distribution feature, this paper uses the RoBERTa model to extract information and combine it with the behavioral features in traditional research for model training. Compared with traditional methods, the accuracy rate is improved by nearly 3%.}
}
@article{ONDEN2024108378,
title = {Exploring the adoption of the metaverse and chat generative pre-trained transformer: A single-valued neutrosophic Dombi Bonferroni-based method for the selection of software development strategies},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108378},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108378},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624005360},
author = {Abdullah Önden and Karahan Kara and İsmail Önden and Galip Cihan Yalçın and Vladimir Simic and Dragan Pamucar},
keywords = {Virtual reality, Metaverse, Natural language processing, Single-valued neutrosophic sets, Alternative ranking order method accounting for two-step normalization},
abstract = {The contemporary era has witnessed remarkable developments that seek to transform and reshape traditional software development methodologies. Notably, artificial intelligence (AI) supported software development as well as software development in virtual reality environments have gained considerable prominence. This article introduces software development strategies to examine how software developers and companies respond to this transformation. Also, an advanced decision model is developed using the alternative ranking order method accounting for two-step normalization (AROMAN) method and further analyzed with the single-valued neutrosophic set-based AROMAN technique. The single-valued neutrosophic weighted Dombi Bonferroni operator is employed in the analysis process. This research offers two case studies investigating the preferences of developers and managers in software development strategies. The first case study examines the preferences of developers, while the second focuses on the preferences of managers. In both case studies, three fundamental software development methods are presented. These include the “traditional developers approach”, “AI-supported developers approach”, and “mixed reality and AI-supported developers approach”. These methods are ranked based on expert opinions concerning 10 criteria that influence the software development process. In both case studies, “output quality” is identified as the most influential criterion. From the perspective of software development methods, in both case studies, the “mixed reality and AI-supported developers approach” is identified as the most effective. Recommendations are provided for developers and managers. The findings also have significant implications for guiding developers and managers in making informed decisions and optimizing software development practices to align with the evolving AI and virtual reality landscape.}
}
@article{XU2022105396,
title = {An overview of visualization and visual analytics applications in water resources management},
journal = {Environmental Modelling & Software},
volume = {153},
pages = {105396},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105396},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001025},
author = {Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal},
keywords = {Big Data, Hydroinformatics, Visual Analytics, Visualization, Human-Computer Interaction},
abstract = {Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for water resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of hydroinformatics applications.}
}
@article{GUO2024114051,
title = {Measuring service quality based on customer emotion: An explainable AI approach},
journal = {Decision Support Systems},
volume = {176},
pages = {114051},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114051},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001264},
author = {Yiting Guo and Yilin Li and De Liu and Sean Xin Xu},
keywords = {Service quality, Emotional intelligence, Explainable AI, Referral, Time series classification},
abstract = {This paper develops an explainable artificial intelligence (AI) approach to measuring service quality in voice-based service encounters. Drawing from the psychology and computer science literature, we construct features of a customer's emotion dynamics during a service encounter. Using real-world call center data from a large insurance company, we train an ensemble model with these emotion dynamics features to predict service quality. The model has higher prediction performance than the two benchmark approaches using quality-assurance evaluation and operational indices. Our method for emotion dynamics classification outperforms a host of state-of-the-art time series classification algorithms. We further apply explainable AI methods to identify the most important features of emotion dynamics and show how they are related to service quality. For example, the location where the last emotion episode appears in a service call has a U-shaped relationship to low quality. Finally, to demonstrate utility, we design an IT artifact to automatically measure service quality after service encounters in the call center and use the measure to predict a customer's referral intention.}
}
@incollection{2024465,
title = {Index},
editor = {Chandrakant D. Patel and Chun-Hsien Chen},
booktitle = {Digital Manufacturing},
publisher = {Elsevier},
pages = {465-485},
year = {2024},
isbn = {978-0-443-13812-6},
doi = {https://doi.org/10.1016/B978-0-443-13812-6.20001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443138126200018}
}
@article{UNSALALTUNCAN2024511,
title = {A hybrid forecasting model to predict the duration and cost performance of projects with Bayesian Networks},
journal = {European Journal of Operational Research},
volume = {315},
number = {2},
pages = {511-527},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723009797},
author = {Izel Ünsal-Altuncan and Mario Vanhoucke},
keywords = {Project scheduling, Project forecasting, Risk models, Earned value management},
abstract = {This paper presents a new hybrid forecasting model to predict the final time and cost of a project using input parameters from the project scheduling and risk analysis literature. The hybrid method integrates two well-known risk models. A Structural Equation Modeling constructs and validates a theoretical risk model to represent known relations between project indicators and the project performance. A Bayesian Networks is used to train the theoretical model using artificial project data from the literature. These two integrated models are then used to predict the final duration and cost of a new unseen project. The accuracy of this integrated model is compared with other well-known forecasting methods from the literature. The computational experiments on a set of 33 empirical projects show that risk models demonstrate a noteworthy advantage for time and cost forecasting. To show the usefulness of this method, it is compared with a set of known machine learning forecasting algorithms. These static predictions of risk models are also compared with some well-known dynamic forecasting methods that continuously update the time/cost predictions along the project progress. These dynamic models make use of predictors from the earned value management and earned duration management literature. The results show that the static risk models offer more precise forecasts than the dynamic methods in the first half of the project progress for time forecasting, but then loose their power in favor of the dynamic forecasts.}
}
@article{MIKALEF2023114039,
title = {All eyes on me: Predicting consumer intentions on social commerce platforms using eye-tracking data and ensemble learning},
journal = {Decision Support Systems},
volume = {175},
pages = {114039},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114039},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001148},
author = {Patrick Mikalef and Kshitij Sharma and Sheshadri Chatterjee and Ranjan Chaudhuri and Vinit Parida and Shivam Gupta},
keywords = {Eye-tracking, Social commerce, Ensemble learning, Prediction, Machine learning},
abstract = {Understanding what information is important for consumers when making a purchase-related decision has been a key question for researchers and practitioners ever since the advent of empirical research in commerce. Nevertheless, our knowledge of what information is important has been formed primarily through post-purchase conscious capturing approaches, such as surveys and questionnaires. To overcome these limitations, we ground this research on an exploratory study that captures eye-tracking data during a decision-making task of product selection. Grounded on the dynamic attention theory, we utilize different information types and formats present on a popular social commerce platform, to identify elements which are important when deciding about online product purchase decision. Specifically, we employ a series of prediction algorithms and use an ensemble learning setup to predict the aspects that contribute to product selection by consumers. Our analysis highlights the most important informational cues to accurately predict product selection among alternatives. In addition, the results showcase how such elements shift in importance during the temporal sequence of comparing different product alternatives. Our results provide insight into how we can understand the journey of decision-making for social commerce customers when navigating through information to select a product. In addition, it opens the discussion about the shifts that eye-tracking in combination with machine learning can create for researchers and marketers.}
}
@article{SCHULTZ2024110138,
title = {ConvGeN: A convex space learning approach for deep-generative oversampling and imbalanced classification of small tabular datasets},
journal = {Pattern Recognition},
volume = {147},
pages = {110138},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110138},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300835X},
author = {Kristian Schultz and Saptarshi Bej and Waldemar Hahn and Markus Wolfien and Prashant Srivastava and Olaf Wolkenhauer},
keywords = {Imbalanced data, Convex space learning, LoRAS, GAN, Tabular data},
abstract = {Oversampling is commonly used to improve classifier performance for small tabular imbalanced datasets. State-of-the-art linear interpolation approaches can be used to generate synthetic samples from the convex space of the minority class. Generative networks are common deep learning approaches for synthetic sample generation. However, their scope on synthetic tabular data generation in the context of imbalanced classification is not adequately explored. In this article, we show that existing deep generative models perform poorly compared to linear interpolation-based approaches for imbalanced classification problems on small tabular datasets. To overcome this, we propose a deep generative model, ConvGeN that combines the idea of convex space learning with deep generative models. ConvGeN learns coefficients for the convex combinations of the minority class samples, such that the synthetic data is distinct enough from the majority class. Our benchmarking experiments demonstrate that our proposed model ConvGeN improves imbalanced classification on such small datasets, as compared to existing deep generative models, while being on par with the existing linear interpolation approaches. Moreover, we discuss how our model can be used for synthetic tabular data generation in general, even outside the scope of data imbalance, and thus improves the overall applicability of convex space learning.}
}
@article{2023100123,
title = {The Third BenchCouncil International Symposium on Intelligent Computers, Algorithms, and Applications (IC 2023) Call for Papers},
journal = {BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
volume = {3},
number = {2},
pages = {100123},
year = {2023},
issn = {2772-4859},
doi = {https://doi.org/10.1016/j.tbench.2023.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2772485923000406},
keywords = {IC 2023, IC23, Call for papers},
abstract = {Sponsored and organized by the International Open Benchmark Council (BenchCouncil), the IC conference is to provide a pioneering technology map through searching and advancing state-of-the-art and state-of-the-practice in processors, systems, algorithms, and applications for machine learning, deep learning, spiking neural network and other AI techniques across multidisciplinary and interdisciplinary areas. IC 2023 invites manuscripts describing original work in the above areas and topics. All accepted papers will be presented at the IC 2023 conference and published by Springer CCIS (Indexed by EI). The IC conferences have been successfully held for two series from 2019 to 2022 and attracted plenty of paper submissions and participants. IC 2023 will be held on December 4-6, 2023 in Sanya and invites manuscripts describing original work in processors, systems, algorithms, and applications for AI techniques across multidisciplinary and interdisciplinary areas. The conference website is https://www.benchcouncil.org/ic2023/. Important Dates: Paper Submission: July 31, 2023, at 11:59 PM AoE Notification: September 30, 2023, at 11:59 PM AoE Final Papers Due: October 31, 2023, at 11:59 PM AoE Conference Date: December 4-6, 2023 Submission Site: https://ic2023.hotcrp.com/}
}
@article{NG2021101246,
title = {A systematic literature review on intelligent automation: Aligning concepts from theory, practice, and future perspectives},
journal = {Advanced Engineering Informatics},
volume = {47},
pages = {101246},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101246},
url = {https://www.sciencedirect.com/science/article/pii/S147403462100001X},
author = {Kam K.H. Ng and Chun-Hsien Chen and C.K.M. Lee and Jianxin (Roger) Jiao and Zhi-Xin Yang},
keywords = {Intelligent automation, Innovative robotic process automation, Artificial intelligence, Adaptive decision making},
abstract = {With the recent developments in robotic process automation (RPA) and artificial intelligence (AI), academics and industrial practitioners are now pursuing robust and adaptive decision making (DM) in real-life engineering applications and automated business workflows and processes to accommodate context awareness, adaptation to environment and customisation. The emerging research via RPA, AI and soft computing offers sophisticated decision analysis methods, data-driven DM and scenario analysis with regard to the consideration of decision choices and provides benefits in numerous engineering applications. The emerging intelligent automation (IA) – the combination of RPA, AI and soft computing – can further transcend traditional DM to achieve unprecedented levels of operational efficiency, decision quality and system reliability. RPA allows an intelligent agent to eliminate operational errors and mimic manual routine decisions, including rule-based, well-structured and repetitive decisions involving enormous data, in a digital system, while AI has the cognitive capabilities to emulate the actions of human behaviour and process unstructured data via machine learning, natural language processing and image processing. Insights from IA drive new opportunities in providing automated DM processes, fault diagnosis, knowledge elicitation and solutions under complex decision environments with the presence of context-aware data, uncertainty and customer preferences. This sophisticated review attempts to deliver the relevant research directions and applications from the selected literature to the readers and address the key contributions of the selected literature, IA’s benefits, implementation considerations, challenges and potential IA applications to foster the relevant research development in the domain.}
}
@article{DAVIS2024103896,
title = {Fostering security-related citizenship through the employee-supervisor relationship: An examination of supervisor security embodiment},
journal = {Computers & Security},
volume = {142},
pages = {103896},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103896},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824001986},
author = {Joshua M. Davis and Deepti Agrawal and Rebekah Austin},
keywords = {Supervisor security embodiment, Social identity theory of leadership, Leader-member exchange, Behavioral information security},
abstract = {Organizational information security performance is increasingly dependent on employees’ security-related citizenship behaviors that stretch beyond the scope of formal organizational prescription and control. Unfortunately, cultivating enactment of these valued behaviors has proven challenging for many companies. The literature has recognized workplace relationships as important determinants of behavioral security outcomes and extra-role security behaviors (ERBs) in particular. Taken further, an employee's relationship with the immediate supervisor is recognized as one of the most influential relational factors shaping a variety of workplace behaviors, including those related to security. Consistent with these notions, scholars have called for making the employee-supervisor relationship a more central component of behavioral security research and practice. Currently however, beyond recognition of this relationship's importance, the knowledge base is unclear about how it shapes ERB enactment. Because employees view supervisors as both organizational agents and as individuals in their own rights, this relationship has the potential to drive productive or counterproductive security behaviors, depending on how aligned the supervisor's security values are with those of the organization. Yet, the security literature has given surprisingly little consideration to the notion that employees can differ in the extent to which they perceive supervisors as embodying organizational information security values. Responding to this gap, the current study examines how employee-supervisor relations and perceived security-related value alignment between supervisors and the broader organization shape employees’ commitment to organizational information security and ultimately, ERB enactment. Grounded in the social identity theory of leadership (SITL), a research model is developed that positions high-quality employee-supervisor exchange as a direct antecedent of affective commitment to organizational information security, which then serves as a central intrinsic motivational mechanism driving ERB enactment. Further, rooted in SITL's principles on leader prototypicality and supervisor organizational embodiment, employee-perceived value alignment between the immediate supervisor and the organization as a whole—referred to here as supervisor security embodiment (SSE)—is introduced as a critical boundary condition influencing the extent to which employee-supervisor relations drive commitment. Results from model testing empirically demonstrate the value of SSE in explicating how this important relationship shapes workplace ERB enactment, through its influence on affective commitment to organizational information security performance.}
}
@article{AGGARWAL2020106858,
title = {Landslide data analysis using various time-series forecasting models},
journal = {Computers & Electrical Engineering},
volume = {88},
pages = {106858},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106858},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620307114},
author = {Akarsh Aggarwal and Mohammed Alshehri and Manoj Kumar and Osama Alfarraj and Purushottam Sharma and Kamal Raj Pardasani},
keywords = {Time series forecasting, ARIMA, GARCH, Dynamic neural network, Landslide data, Data analysis},
abstract = {Landslides are among the many devastating natural calamities that cause damage to life and property. Predicting landslides is an important task to enable preventive measures to be made on time. This paper presents an analysis of univariate time-series forecasting data using an auto regressive integrated moving average (ARIMA) model, a generalized autoregressive conditional heteroskedasticity (GARCH) model, and a dynamic neural network (DNN) model. These techniques rely on the objective of the forecasting, the type of forecasted data, and whether an automatic or manual approach is to be used for forecasting. Different techniques were analyzed on 15-meter landslide sensor data. The objective of this paper is to suggest a best method among well-known models for landslide forecasting. The demonstrated result shows that a dynamic neural network model is best in class for time-series landslide forecasting. Furthermore, upon objectively evaluating the three well-known techniques, the DNN model exhibited a minimum error rate of approximately 0.01 in comparison to other implemented techniques.}
}
@article{LIYANAGE2022103362,
title = {A survey on Zero touch network and Service Management (ZSM) for 5G and beyond networks},
journal = {Journal of Network and Computer Applications},
volume = {203},
pages = {103362},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103362},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000297},
author = {Madhusanka Liyanage and Quoc-Viet Pham and Kapal Dev and Sweta Bhattacharya and Praveen Kumar Reddy Maddikunta and Thippa Reddy Gadekallu and Gokul Yenduri},
keywords = {Zero-touch network and Service Management, Machine learning, Artificial intelligence, Security, 5G, 6G, Service management, Automation, Orchestration},
abstract = {Faced with the rapid increase in smart Internet-of-Things (IoT) devices and the high demand for new business-oriented services in the fifth-generation (5G) and beyond network, the management of mobile networks is getting complex. Thus, traditional Network Management and Orchestration (MANO) approaches cannot keep up with rapidly evolving application requirements. This challenge has motivated the adoption of the Zero-touch network and Service Management (ZSM) concept to adapt the automation into network services management. By automating network and service management, ZSM offers efficiency to control network resources and enhance network performance visibility. The ultimate target of the ZSM concept is to enable an autonomous network system capable of self-configuration, self-monitoring, self-healing, and self-optimization based on service-level policies and rules without human intervention. Thus, the paper focuses on conducting a comprehensive survey of E2E ZSM architecture and solutions for 5G and beyond networks. The article begins by presenting the fundamental ZSM architecture and its essential components and interfaces. Then, a comprehensive review of the state-of-the-art for key technical areas, i.e., ZSM automation, cross-domain E2E service lifecycle management, and security aspects, are presented. Furthermore, the paper contains a summary of recent standardization efforts and research projects towards the ZSM realization in 5G and beyond networks. Finally, several lessons learned from the literature and open research problems related to ZSM realization are also discussed in this paper.}
}
@article{CHOU2023107665,
title = {Development of AIoT System for facility asparagus cultivation},
journal = {Computers and Electronics in Agriculture},
volume = {206},
pages = {107665},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.107665},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923000534},
author = {Cheng-Ying Chou and Shan-Cheng Chang and Zi-Ping Zhong and Ming-Chi Guo and Ming-Hsien Hsieh and Jui-Chu Peng and Ling-Chieh Tai and Ping-Liang Chung and Jen-Cheng Wang and Joe-Air Jiang},
keywords = {Asparagus, Greenhouse, IoT, AI, Whitefly, Thrips},
abstract = {Asparagus is a high economic value crop sensitive to environmental factors and pest outbreaks, so asparagus cultivation in Taiwan is usually grown in greenhouses. This study established an AIoT system specifically for asparagus growth and monitoring of pests and diseases, providing planting guidelines for farmers to grow asparagus and preventing outbreaks of pests and diseases. We have installed many IoT and AI sensors to track environmental changes and pest populations. In the summer, the fans are automatically turned on and off according to the temperature measured by the sensor. The cooling effect is observed on the temperature distribution interpolated with kriging. By correlation analysis, the relationship between environmental factors and asparagus yield and the relationship between environmental changes and pests were obtained, which can help to optimize the planting environment in terms of sunlight and temperature regulation, drip fertigation, irrigation, and pest control strategies. The deep learning model we developed can detect pests and count pest numbers with high accuracy. The model detection results show that with an IOU threshold of 0.5, precision, recall, mAP, and pest count accuracy reach 93.8%, 91.9%, 95.3%, and 95.8%, respectively. Finally, our research and development results can provide farmers with automated and optimized asparagus planting methods, reduce insect pests, optimize the growing environment, and provide good irrigation and soil management to improve yield and quality.}
}
@article{COLOMBARI2024109377,
title = {Disentangling the socio-technical impacts of digitalization: What changes for shop-floor decision-makers?},
journal = {International Journal of Production Economics},
volume = {276},
pages = {109377},
year = {2024},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2024.109377},
url = {https://www.sciencedirect.com/science/article/pii/S0925527324002342},
author = {Ruggero Colombari and Paolo Neirotti and Jasmina Berbegal-Mirabent},
keywords = {Data-driven decision-making, Digitalization, Lean production, Operations management, Socio-technical systems theory},
abstract = {With the diffusion of Industry 4.0 technologies, firms can decentralize operational decisions, fostering a data-driven Digital Transformation (DT) across all organizational levels. Digitalized shopfloors can leverage an unprecedented availability of data for better and faster decision-making, resulting in enhanced operational performance. However, limited research has investigated the organizational and individual implications for those who run the manufacturing lines: production managers, supervisors, team leaders, and workers. By adopting a socio-technical framework, this study aims to disentangle the effects that digitalization has on shopfloors’ organizational structures, decision-making processes, and individual competencies, as well as the interdependencies among them. An exploratory approach was adopted, based on an empirical cross-country study involving 34 semi-structured interviews conducted in the Italian and Spanish automotive sectors. Analyzed through the lenses of information-processing, knowledge-based, and dynamic capability theories, our findings reveal through five propositions how digitalization induces a “polarization” of operational decision-making: shopfloors are run by knowledgeable data-empowered production managers and autonomous information-processing team leaders on the front line, with a reduced importance of supervisors. Upskilling needs appear for team leaders but not production workers, whose involvement, however, emerges as a key factor for a successful digitalization and overall performance in initial DT stages. This study contributes to literature on digitalization by exposing managerial tensions and dynamic capabilities, along with a deeper understanding of the micro-foundations of DT in terms of implications for shop-floor decision-makers. Managerial implications are directed at creating awareness about the centrality of production team leaders, and future research avenues are proposed.}
}
@article{WANG2024127226,
title = {A dual-population multiobjective co-evolutionary matching ensemble learning for product multi-indicator prediction in continuous annealing},
journal = {Neurocomputing},
volume = {572},
pages = {127226},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127226},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223013498},
author = {Yao Wang and Xianpeng Wang},
keywords = {Iron and steel industry, Dual-population co-evolutionary, Multiobjective optimization, Ensemble learning, Multi-indicator prediction},
abstract = {Achieving simultaneous and precise prediction of multiple product performance indicators is crucial to ensure stable production in the continuous annealing process. However, the majority of existing quality prediction modeling methods concentrate on developing single-output regression models that are limited to single-indicator prediction scenarios. When dealing with the multi-indicator prediction scenarios, these methods usually require the construction of a separate model for each performance indicator, which leads to more computational and storage resources and more complex maintenance in practical applications. To address this issue, a dual-population multiobjective co-evolutionary matching ensemble learning method called DP-CEMEL is developed. It utilizes dual-population co-optimization to enable the joint learning of the algorithm on input–output relationships and inter-indicator correlations. More specifically, a two-stage learning based individual encoding and decoding method and a performance boosting degree (PBD) based individual evaluation strategy are designed to achieve co-optimization of the dual populations to evolve a set of individuals (i.e., base learners) with good accuracy and low complexity. Then, a PBD based matching ensemble method is applied to get the final multi-indicator prediction model by the optimal matching and ensemble of the evolved individuals. Experimental results on both benchmark data and practical strip production data demonstrate that the proposed DP-CEMEL attains competitive or better performance compared to other state-of-the-art learning methods.}
}
@article{RADOVANOVIC2022257,
title = {Do we Reach Desired Disparate Impact with In-Processing Fairness Techniques?},
journal = {Procedia Computer Science},
volume = {214},
pages = {257-264},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.173},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201883X},
author = {Sandro Radovanović and Boris Delibasić and Milija Suknović},
keywords = {Algorithmic decision-making, Fairness, Disparate Impact, Machine Learning, Logistic Regression, Fairness Gap},
abstract = {Using machine learning algorithms in social environments and systems requires stricter and more detailed control. More specifically, the cost of error in such systems is much higher. Therefore, one should ensure that important decisions, such as whether to convict a person or not based on the previous criminal record, are by the legal requirements and not biased toward a group of people. One can find many many papers in the literature aimed at mitigating or eliminating unwanted bias in machine learning models. A significant part of these efforts add fairness constraint to the mathematical model or adds a regularization term to the loss function. In this paper, we show that optimizing the loss function given the fairness constraint or regularization for unfairness can surprisingly yield unfair solutions. This is due to the linear relaxation of the fairness function. By analyzing the gap between the true value of fairness and the one obtained using linear relaxation, we found that the gap can be as high as around 21% for the COMPAS dataset, and around 35% for the Adult dataset. In addition, we show that the fairness gap is consistent regardless of the strength of the fairness constraint or regularization.}
}
@article{NIROOMAND2023106848,
title = {Smart investigation of artificial intelligence in renewable energy system technologies by natural language processing: Insightful pattern for decision-makers},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106848},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106848},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623010321},
author = {Kamran Niroomand and Noori M. Cata Saady and Carlos Bazan and Sohrab Zendehboudi and Amilcar Soares and Talib M. Albayati},
keywords = {Natural language processing, Artificial intelligence, Text mining, Topic modeling, Pattern identification, Renewable energy},
abstract = {This study aims to provide a framework which enables decision-makers and researchers to identify AI technology patterns in renewable energy systems from a massive data set of textual data. However, the study was challenged by the Scopus database limitation that allows users to retrieve only 2000 documents per query. Therefore, we developed a search engine based on the Scopus Application Programming Interface (API) that enables us to download an unlimited number of documents per query based on our desirable settings. We extracted 5661 renewable energy systems-related publications from Scopus database and leveraged Natural Language Processing (NLP) and unsupervised algorithms to identify the most frequent computational science models and dense meta-topics and investigate their evolution throughout the period 2000-2021. Our findings showed 7 meta-topics based on the class-based Term Frequency-Inverse Document Frequency (c-TD-IDF) score and term score decline graph. Emerging advanced algorithms, such as different deep learning architectures, directly impacted growing meta-topics involving problems with uncertainty and dynamic conditions.}
}
@article{TALAEIKHOEI2024102975,
title = {How does incorporating ChatGPT within a firm reinforce agility-mediated performance? The moderating role of innovation infusion and firms’ ethical identity},
journal = {Technovation},
volume = {132},
pages = {102975},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2024.102975},
url = {https://www.sciencedirect.com/science/article/pii/S0166497224000257},
author = {Amir Talaei-Khoei and Alan T. Yang and Masialeti Masialeti},
keywords = {ChatGPT, Operational agility, Market agility, Ethical identity, Performance.},
abstract = {The expansion of ChatGPT has sparked substantial discussions on the capabilities of generative artificial intelligence AI to profoundly reshape the business environment. However, empirical data on ChatGPT and its effects on firm performance are still lacking. In accordance with relevant literature, this study investigates the influence of ChatGPT-enabled agility, encompassing both operational and market facets, on firm performance. This work studies the moderating roles of innovative infusion of ChatGPT and ethical identity in the association between incorporating ChatGPT into standard processes with operational and market agilities. Data from a survey of IT executives were analyzed to validate our proposed hypotheses. We found that while the infusion of ChatGPT in an organization moderates the relationship between ChatGPT incorporation and operational agility, infusion does not significantly impact market agilities. In addition, we found that a firm's ethical identity moderates ChatGPT-enabled market agilities but not operational agility. We also found that both types of agility play mediating roles in improving firm performance. This study adds to the growing body of literature on dynamic capabilities by presenting and testing a theory on the influence of a firm's capability to incorporate ChatGPT on agility, firm performance, and the role of ChatGPT infusion and firm ethical identity.}
}
@article{WATANABE2022113635,
title = {Visual analytics of set data for knowledge discovery and member selection support},
journal = {Decision Support Systems},
volume = {152},
pages = {113635},
year = {2022},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113635},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621001457},
author = {Ryuji Watanabe and Hideaki Ishibashi and Tetsuo Furukawa},
keywords = {Visual analytics, Set data, Manifold modeling, Team formation support, Interactive visualization},
abstract = {Visual analytics (VA) is a visually assisted exploratory analysis approach in which knowledge discovery is executed interactively between the user and system in a human-centered manner. The purpose of this study is to develop a method for the VA of set data aimed at supporting knowledge discovery and member selection. A typical target application is a visual support system for team analysis and member selection, by which users can analyze past teams and examine candidate lineups for new teams. Because there are several difficulties, such as the combinatorial explosion problem, developing a VA system of set data is challenging. In this study, we first define the requirements that the target system should satisfy and clarify the accompanying challenges. Then we propose a method for the VA of set data, which satisfies the requirements. The key idea is to model the generation process of sets and their outputs using a manifold network model. The proposed method visualizes the relevant factors as a set of topographic maps on which various information is visualized. Furthermore, using the topographic maps as a bidirectional interface, users can indicate their targets of interest in the system on these maps. We demonstrate the proposed method by applying it to basketball teams, and compare with a benchmark system for outcome prediction and lineup reconstruction tasks. Because the method can be adapted to individual application cases by extending the network structure, it can be a general method by which practical systems can be built.}
}
@incollection{2021xxi,
title = {Editors biography},
editor = {Pradeep N and Sandeep Kautish and Sheng-Lung Peng},
booktitle = {Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics},
publisher = {Academic Press},
pages = {xxi-xxiii},
year = {2021},
isbn = {978-0-12-821633-0},
doi = {https://doi.org/10.1016/B978-0-12-821633-0.09995-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128216330099955}
}
@article{MOFOLASAYO2022934,
title = {How to adapt lean practices in SMEs to support Industry 4.0 in manufacturing},
journal = {Procedia Computer Science},
volume = {200},
pages = {934-943},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.291},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003003},
author = {Adekunle Mofolasayo and Steven Young and Pablo Martinez and Rafiq Ahmad},
keywords = {Industry 4.0, lean manufacturing, lean 4.0, change management},
abstract = {Industry 4.0 promises to increase productivity and flexibility for the manufacturing industry, which translates into greater customer value and lower costs. Lean manufacturing has long championed principles and tools with a focus on value adding activities, elimination of waste and continuous improvement. Even the most successful lean manufacturing firms, in terms of efficiency and quality achieved through waste reduction, will acknowledge there is a lot of opportunity for improvement. This review evaluates how lean principles can support Industry 4.0 in pursuit of greater customer value and manufacturing excellence. Leveraging the opportunities that exist within Industry 4.0 umbrella of technologies can help to further reduce the nine wastes of lean. While large corporations have access to extensive capital markets and can capture economies of scale offered by leading edge technologies of Industry 4.0, small and medium sized enterprises (SMEs) with greater capital restraints face the challenge of justifying Industry 4.0 technologies and cannot risk being at the bleeding edge of technology. Using a case study of a small electronics manufacturing firm, the opportunities, challenges, and the implementation strategy for technology are examined. This paper finds that SMEs pursuit of process efficiencies and waste reduction can be best achieved through a focus on foundational digitalization & data management then taking a stepwise approach towards the cyber-physical systems of Industry 4.0.}
}
@article{THAVANESAN2024108978,
title = {Insights from explainable AI in oesophageal cancer team decisions},
journal = {Computers in Biology and Medicine},
volume = {180},
pages = {108978},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108978},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010631},
author = {Navamayooran Thavanesan and Arya Farahi and Charlotte Parfitt and Zehor Belkhatir and Tayyaba Azim and Elvira Perez Vallejos and Zoë Walters and Sarvapali Ramchurn and Timothy J. Underwood and Ganesh Vigneswaran},
keywords = {Machine learning, Oesophageal cancer, Multidisciplinary teams, Decision-making},
abstract = {Background
Clinician-led quality control into oncological decision-making is crucial for optimising patient care. Explainable artificial intelligence (XAI) techniques provide data-driven approaches to unravel how clinical variables influence this decision-making. We applied global XAI techniques to examine the impact of key clinical decision-drivers when mapped by a machine learning (ML) model, on the likelihood of receiving different oesophageal cancer (OC) treatment modalities by the multidisciplinary team (MDT).
Methods
Retrospective analysis of 893 OC patients managed between 2010 and 2022 at our tertiary unit, used a random forests (RF) classifier to predict four possible treatment pathways as determined by the MDT: neoadjuvant chemotherapy followed by surgery (NACT + S), neoadjuvant chemoradiotherapy followed by surgery (NACRT + S), surgery-alone, and palliative management. Variable importance and partial dependence (PD) analyses then examined the influence of targeted high-ranking clinical variables within the ML model on treatment decisions as a surrogate model of the MDT decision-making dynamic.
Results
Amongst guideline-variables known to determine treatments, such as Tumour-Node-Metastasis (TNM) staging, age also proved highly important to the RF model (16.1 % of total importance) on variable importance analysis. PD subsequently revealed that predicted probabilities for all treatment modalities change significantly after 75 years (p < 0.001). Likelihood of surgery-alone and palliative therapies increased for patients aged 75–85yrs but lowered for NACT/NACRT. Performance status divided patients into two clusters which influenced all predicted outcomes in conjunction with age.
Conclusion
XAI techniques delineate the relationship between clinical factors and OC treatment decisions. These techniques identify advanced age as heavily influencing decisions based on our model with a greater role in patients with specific tumour characteristics. This study methodology provides the means for exploring conscious/subconscious bias and interrogating inconsistencies in team-based decision-making within the era of AI-driven decision support.}
}
@article{BARGONI2024102945,
title = {Highway to hell or paradise city? Exploring the role of growth hacking in learning from innovation failure},
journal = {Technovation},
volume = {131},
pages = {102945},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2023.102945},
url = {https://www.sciencedirect.com/science/article/pii/S0166497223002560},
author = {Augusto Bargoni and Luboš Smrčka and Gabriele Santoro and Alberto Ferraris},
keywords = {Growth hacking, Innovation failure, Iterative experimentation, Big data, Decision-making},
abstract = {The purpose of this study is to conceptualize the role of growth hacking, a data-driven iterative experimentation process, in minimizing the likelihood of innovation failure within firms. Drawing upon existing literature on innovation and growth hacking, we provide a conceptual background to frame our research. To investigate this phenomenon, we employ a qualitative approach that combines the Gioia method and phenomenography. Our primary data source consists of in-depth interviews conducted with managers and practitioners who possess extensive experience in innovation management and growth hacking. Through a systematic inductive concept development approach and a multilevel analysis, we develop a novel conceptualization that illustrates how growth hacking strategies can be effectively implemented across four levels of analysis: market, organization, project, and product. Our findings highlight the importance of adopting growth hacking practices to minimize the likelihood of innovation failure in each of these domains. From a practical perspective, we offer recommendations on the strategies that companies should employ to effectively learn from the challenges associated with innovation. By leveraging these insights, firms can enhance their ability to overcome potential obstacles and optimize their innovation processes.}
}
@article{CHANG2024117652,
title = {COLREG and MASS: Analytical review to identify research trends and gaps in the Development of Autonomous Collision Avoidance},
journal = {Ocean Engineering},
volume = {302},
pages = {117652},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117652},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824009892},
author = {Chia-Hsun Chang and Isuru Bandara Wijeratne and Christos Kontovas and Zaili Yang},
keywords = {COLREG, MASS, Collision avoidance, Ship collision},
abstract = {Maritime Autonomous Surface Ships (MASS) face regulatory challenges, with some suggesting that the existing Collision Regulations (COLREG) present linguistic barriers for autonomous vessels' development and implementation. While academic research has focused on developing autonomous collision avoidance (CA), it is producing inconsistent results compared to conventional navigation practices. This study aims to identify trends and weaknesses in recent studies on CA for MASS by conducting a systematic review and analysis of the most relevant literature. The Conventional-Collision-Avoidance-Process (CCAP), which benchmarks manned modern ships' capacity for CA compliance under COLREG and industry requirements, is used to break down a ship's collision avoidance process into 53 CA functions under eight main categories. A total of 32 papers were chosen through filtering based on keywords, publication period, language, and relevance. The content of the recent academic literature was then grouped under appropriate CCAP codes. Statistical and graphical interpretations were generated using the collected literature content data and evaluated statistics of the existing digital contribution of CCAP. The study uncovers significant trends, inconsistencies, and weaknesses that could guide future academic research towards comprehensive CA solutions for MASS.}
}
@article{STRIMOVSKAYA20231809,
title = {Multi-level conceptual model of efficiency control in supply chain management},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {1809-1814},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1894},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323023030},
author = {Anna Strimovskaya and Sergey Barykin and Elena Volkova and Elena Tsyplakova and Galina Sinko and Veronika Kuzmenkova and Alexey Krasilnikov},
keywords = {Supply chain management, control in logistics, efficiency performance, analytical models, integral method of analysis, decision support},
abstract = {Modern tendencies of rapid economic growth, demand-driven market, highly uncertain economic environment with various disruption sources simultaneously with targets for sustainable development explains the increased interest to advanced solutions in supply chain management. The research paper presents a multi-level efficiency control model presenting analytical sequences and integral method of factor analysis for logistics performance indicators planning, controlling and analyzing. The numerical example has demonstrated that the proposed approach works for real economic processes and provides agile options of efficiency control. The designed approach assumes customization for a particular company’ requirements and provides at least 59 options of analytical models sets. This allows claiming the new approach toward decision support enhancement in supply chains.}
}
@article{MADANCHIAN20242101,
title = {Transforming Leadership Practices through Artificial Intelligence},
journal = {Procedia Computer Science},
volume = {235},
pages = {2101-2111},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924008755},
author = {Mitra Madanchian and Hamed Taherdoost and Michele Vincenti and Nachaat Mohamed},
abstract = {The increasing incorporation of artificial intelligence (AI) into various aspects of organizational management has caused a fundamental upheaval in leadership practices. This article provides an in-depth discussion of how AI alters traditional leadership paradigms and promotes more adaptable, data-driven, and customized approaches to leading teams and organizations. By utilizing AI, leaders can enhance communication and collaboration, anticipate talent needs, analyze data in real-time, and eradicate ethical biases. Additionally, they can tailor leadership development to specific individuals. However, issues are associated with this progression, such as ethical concerns, privacy concerns, and the eventual replacement of certain work responsibilities. This paper emphasizes the need for responsible and adaptable leadership in the AI era by envisioning a future in which AI-assisted leadership practices coexist with human intuition and values through compelling case studies and a forward-looking perspective. AI’s integration into leadership necessitates a balanced approach to harness its benefits while preserving vital human qualities, shaping a more adaptable and sustainable future for leadership.}
}
@article{MOURTZIS20211668,
title = {A Methodology for the Assessment of Operator 4.0 Skills based on Sentiment Analysis and Augmented Reality},
journal = {Procedia CIRP},
volume = {104},
pages = {1668-1673},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011793},
author = {Dimitris Mourtzis and John Angelopoulos and Vasilis Siatras and Nikos Panopoulos},
keywords = {Skills, Competencies, Operator 4.0, Augmented Reality, Natural Language Processing},
abstract = {Human-Cyber-Physical Systems are the key to the successful operation of manufacturing systems. Consequently, the need for adequate assessment of human operators and tracking of their skills and competencies evolution emerges. Additionally, the advances in digital technologies encourage the development of supportive and adaptive frameworks for the operation of flexible manufacturing systems. This paper presents an Augmented Reality based methodology for the detailed evaluation of human skills and competencies based on the processing of raw textual data with a Natural Language Processing algorithm, aiming at the provision of technician guidance. The developed framework is tested and validated in an industrial environment.}
}
@article{HOSSAIN2023122745,
title = {Data-driven market effectiveness: The role of a sustained customer analytics capability in business operations},
journal = {Technological Forecasting and Social Change},
volume = {194},
pages = {122745},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122745},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523004304},
author = {Md Afnan Hossain and Shahriar Akter and Venkata Yanamandram and Samuel Fosso Wamba},
keywords = {Retail business operations, Sustained customer analytics, Retail operations capability, Data-driven value generation},
abstract = {This study's objective is to investigate how a business can achieve data-driven market effectiveness through the sustained application of a customer analytics capability to its operations. Despite the abundance of literature on retail technology management, empirical evidence on the effectiveness of a customer analytics capability in promoting sustainable market performance within retail business operations remains scarce. This study presents a model of a sustained customer analytics capability in the context of competitive, data-rich retail business processes, drawing on grounded market orientation capability theory. The study employs a taxonomy of explanation and prediction from an epistemological perspective, employing predominantly positivist methods, where data analysis validates the conceptual customer analytics capability and its sustained critical outcomes. In addition, the study discusses the significant contributions of its findings regarding the acceleration of retail business operational performance in a big data environment and also provides future research directions to resolve any limitations of the current study.}
}
@article{ALI2023102591,
title = {Predicting Stripe Rust Severity in Wheat Using Meteorological Data with Environmental Response Modeling},
journal = {Journal of King Saud University - Science},
volume = {35},
number = {4},
pages = {102591},
year = {2023},
issn = {1018-3647},
doi = {https://doi.org/10.1016/j.jksus.2023.102591},
url = {https://www.sciencedirect.com/science/article/pii/S1018364723000538},
author = {Yasir Ali and Sidra Iqbal and Hafiz Muhammad Aatif and Khalid Naveed and Azhar Abbas Khan and Muhammad Ijaz and Muhammad Murtaza Magsi and Salman Ahmad and Ain Ul Abad Syed and Manzoor Ali Magsi and Rana Khalid Iqbal and Najat A. Bukhari and Ashraf Atef Hatamleh and Ahmed Raza},
keywords = {Epidemiology, Regression model, Stripe rut, Wheat},
abstract = {Objective
The main objective of current investigation was to develop a predictive disease model based upon meteorological data, viz., maximum temperature, minimum temperature, rainfall, relative humidity, and wind speed to predict stripe rust severity (%).
Methods
Five years' data of stripe rust severity on three wheat varieties, namely SA-42, Sandal-73, and Barani-70, continuously cultivated for five years (2013–2017), were collected from experimental trials of Deputy Director of Agriculture Extension Layyah to develop a predictive disease model. For validation of the model, a research trial was conducted in the Research Area of the Department of Plant Pathology, Bahadar Sub-Campus Layyah, during the crop seasons of 2018–2019, following procedures similar to those utilized in five years investigation. The data on epidemiological variables used in the present investigation was collected from the Pakistan Meteorological Observatory at Karor-Layyah. To evaluate the association between meteorological factors and disease severity correlation and regression analysis was performed.
Results
All meteorological variables contributed significantly in disease development and showed 89 % variability in stripe rust severity (%). Root means square error (RMSE) and residual (%) were used to evaluate the model's predictions. Both indices were below 20, showing that the model could accurately predict the progression of disease. The regression equations of 5 years model (Y = -63.11 + 0.96x1 + 1.72x2 + 3.72x3 + 0.43x4) and 2 years model (Y = -40.2 + 1.80x1 + 1.18x2 + 2.29x3 + 0.39x4) validated each other. Scatter plots indicated that environmental factors such as maximum temperature (12.8–22.5 °C), minimum temperature (8.7–14.8 °C), relative humidity (50–85 %), and wind speed (1.3–4.5) influenced the progression of stripe rust epidemic.
Conclusion
Understanding the epidemiology of stripe rust will help us to forecast its progression, allowing wheat growers to more precisely adapt plant protection measures.}
}
@article{AHUMADATELLO2023176,
title = {Human Factors, Innovation and Technology, and Cluster Strategies as Triggers of New Product Development},
journal = {Procedia CIRP},
volume = {119},
pages = {176-181},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.03.090},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004602},
author = {Eduardo Ahumada-Tello and Richard Evans},
keywords = {Cluster strategies, technology and innovation, human factors, new product development},
abstract = {This paper reports the results of a cross-sectional investigation into the organizational factors that influence the development of new products in Mexican organizations. We examine the degrees of complexity to determine the influence of three organizational levels, divided into four variables, on New Product Development (NPD) activities. The first level is related to Human Factors (HF), which focuses on the internal members involved in the NPD activities and their functions. The second level is related to Innovation (Inn) and Technology (Tec), which corresponds to the level of processes established in the organization and during NPD. The final level corresponds to the Cluster Strategies (QS) employed by the organizations, which focuses on their ability to work collaboratively with other organizations outside of their current internal boundaries. The influences of the proposed variables are then analyzed with results showing that QS, Inn, Tec, and HF explain, in a range of 58.8%, the emergence of successful NPD based on R2. The study's findings provide a basis for future research through advanced statistical methods.}
}
@article{EACHEMPATI2021120903,
title = {Validating the impact of accounting disclosures on stock market: A deep neural network approach},
journal = {Technological Forecasting and Social Change},
volume = {170},
pages = {120903},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120903},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003358},
author = {Prajwal Eachempati and Praveen Ranjan Srivastava and Ajay Kumar and Kim Hua Tan and Shivam Gupta},
keywords = {Disclosures, Data intelligence, Analytics, Finance, Machine learning, Deep learning, Stock market, Forecasts, Private decision-making},
abstract = {Firms disclose information either voluntarily or due to the regulator's mandatory requirements, and such disclosures form good sources to know the prospects of a firm. Information in the disclosures and analysts' opinions influence investor-trading behavior, and consequently, affects the asset prices. As sentiments factored in disclosures are a source of market action, this study aims to capture the sentiments from disclosure information to assess asset prices' impact. The paper adopts a deep neural network-based prediction model for conducting sentiment analysis on heterogeneous datasets. We construct a sentiment simulation model of voluntary disclosures to know whether the managers can use the market sentiment as a strategic input to boost market performance by suitably drafting the tone and content of disclosures without compromising their quality and veracity. The Deep Neural Networks with LSTM algorithm is found to outperform the Deep Neural Networks with RNN and other baseline machine learning classifiers in terms of predictive accuracy of the NSE NIFTY50. The variable importance computed also validates that market news, combined with historical indicators, predicts the stock market trend closer to the actual trend.}
}
@article{DOUMPOS20231,
title = {Operational research and artificial intelligence methods in banking},
journal = {European Journal of Operational Research},
volume = {306},
number = {1},
pages = {1-16},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S037722172200337X},
author = {Michalis Doumpos and Constantin Zopounidis and Dimitrios Gounopoulos and Emmanouil Platanakis and Wenke Zhang},
keywords = {Artificial Intelligence, Operational research, Banking},
abstract = {Banking is a popular topic for empirical and methodological research that applies operational research (OR) and artificial intelligence (AI) methods. This article provides a comprehensive and structured bibliographic survey of OR- and AI-based research devoted to the banking industry over the last decade. The article reviews the main topics of this research, including bank efficiency, risk assessment, bank performance, mergers and acquisitions, banking regulation, customer-related studies, and fintech in the banking industry. The survey results provide comprehensive insights into the contributions of OR and AI methods to banking. Finally, we propose several research directions for future studies that include emerging topics and methods based on the survey results.}
}
@article{LI2024200138,
title = {Cross media knowledge information retrieval model based on D-S evidence theory},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200138},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200138},
url = {https://www.sciencedirect.com/science/article/pii/S277294192400067X},
author = {Hongbo Li and Xin Li and Boning Liu and Kaiji Mao and Hemin Xu},
keywords = {Cross media, Information retrieval, D-S evidence theory, Approximation calculation, Deep network},
abstract = {Cross media knowledge information retrieval provides strong support for information processing and utilization in the information society, but there are problems such as heterogeneity in cross media knowledge information. Therefore, a cross media knowledge information retrieval model using D-S evidence theory is proposed, which involves using approximate calculation methods to improve this theory for information fusion, reducing computational complexity, and using deep networks for fine-grained information retrieval to improve retrieval accuracy. The results showed that the improved theory enhanced computational efficiency by about 27.23 %. The memory usage was <60 %, and the average accuracy of information fusion reached 93.14 %. It also exhibited high recall and low false alarm rates. The cross media knowledge information retrieval model proposed in the study achieved accuracy of 92.64 %, 96.49 %, and 97.46 % on the three datasets used in the experiment, respectively. The study provides an effective, computationally efficient, and highly accurate model for cross media knowledge information retrieval, which is expected to promote research and application in this field. The combination of improved D-S evidence theory and deep networks provides a powerful approach to solving the problem of cross media heterogeneous information retrieval, which has a positive promoting effect on the processing and utilization of information in the information society.}
}
@article{ZHANG2024200124,
title = {Hybrid procurement model for the construction of library literature and information resource procurement},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200124},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200124},
url = {https://www.sciencedirect.com/science/article/pii/S277294192400053X},
author = {Chuanyu Zhang and Changsheng Wang},
keywords = {Literature and information resources, Procurement model, Support vector machine, Genetic algorithm, Punishment factor, Procurement forecast, Library},
abstract = {To improve the efficiency of intelligent procurement of library literature and intelligence resources, the study conducts the design of literature and intelligence resources procurement model. The procurement model is constructed by using the support vector machine, and the optimal parameters of the support vector machine are obtained by using the genetic algorithm. The experimental results demonstrated that the mean square error of the proposed model was only 0.03, which was 40 % lower compared with the procurement models based on other optimization algorithms. The average accuracy of the proposed model was as high as 95.18 % and the prediction accuracy was 95.78 % compared to other methods. The accuracy was improved by 15.11 %, 24.57 % and 19.67 % respectively compared to other models. The results show that using genetic algorithm to optimize support vector machine can effectively improve the prediction speed and prediction efficiency of the model. The proposed hybrid procurement model based on genetic algorithm and support vector machine can effectively meet the needs of library literature and intelligence resources procurement construction. The model has positive application significance in library literature and intelligence resources procurement.}
}
@article{SZAFRANKO2022104045,
title = {Application of ANFIS in the preparation of expert opinions and evaluation of building design variants in the context of processing large amounts of data},
journal = {Automation in Construction},
volume = {133},
pages = {104045},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104045},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004969},
author = {Elżbieta Szafranko and Piotr E. Srokosz and M. Jurczak and M. Śmieja},
keywords = {Multicriteria analysis, Construction investment variants, Adaptive neuro-fuzzy inference system, CUDA multithread processing},
abstract = {There are many problems involved in the evaluation of variants of construction projects. One of the most difficult tasks is to establish evaluation criteria and assign them values, and afterwards to determine the degree to which the analyzed variants meet the criteria. This process is carried out based on opinions of experts, which usually constitute a large and heterogeneous set of data that is difficult to elaborate. As such, it raises most doubts and discussions among scientists. Therefore, the authors of this article propose to use a tool that can improve this process. The paper presents an example, and the Adaptive Neuro-Fuzzy Inference System (ANFIS) was used to solve the described problem. In the example, three variants were to be assessed and 17 criteria were used for the assessment, with the possibility of grouping them into four categories of main criteria. The developed ANFIS algorithm was implemented in the Compute Unified Device Architecture (CUDA) technology available in modern Nvidia graphics processors. The performance of the CUDA-ANFIS model was tested on several examples of real construction projects. It was found that the choice of the best investment option is not obvious when the final scores are the result of processing many thousands of data. In the evaluation process, fluctuations in the experts' responses may not only produce hard-to-distinguish final results, but may even reverse the order of the variants in the ranking. It has been shown that this will not happen if ANFIS is used to specifically filter both the input data and the intermediate results of the calculations. In addition, the use of CUDA technology speeds up calculations more than ten times. The new concept can be successfully applied to construct implicit decision models based on real data.}
}
@article{DHAR2022113769,
title = {Walking on air or hopping mad? Understanding the impact of emotions, sentiments and reactions on ratings in online customer reviews of mobile apps},
journal = {Decision Support Systems},
volume = {162},
pages = {113769},
year = {2022},
note = {Business and Government Applications of Text Mining & Natural Language Processing (NLP) for Societal Benefit},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113769},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622000409},
author = {Suparna Dhar and Indranil Bose},
keywords = {Online customer reviews, Emoji analysis, Emotion mining, Sentiment analysis, Mobile apps},
abstract = {Online customer reviews (OCR) are an established source of competitive intelligence. Since OCR reflect perceptions of past users about product features and performance, prospective customers of mobile apps often resort to OCR before they decide to download an app. In addition to the textual content of the OCR, potential users of the app often consult the star ratings provided by the reviewers. In this paper, we determine if the star rating of mobile payment apps suitably captures the appropriate customer perception reflected in the textual content of the OCR for the payment apps. We conduct a three-dimensional analysis of 146,914 reviews from two popular mobile payment apps and uncover the sentiment and emotions captured in the review text, and the reactions in the emojis. Our analysis shows that the consolidated sentiment and the happiness emotion obtained from the OCR of mobile payment apps impacts the star ratings more strongly, than other emotions embedded in the text, and reactions in the emojis. The findings also hold good during the Covid-19 pandemic. We propose a novel perception score that captures the nuances in the OCR more effectively and suggest how mobile app providers can use it to arrange the OCR for better compatibility with content. The results of this research will be useful for mobile app providers that depend on OCR for popularizing their apps and will provide them a novel metric based on content, that can leverage OCR better to influence future downloads.}
}
@article{WANG20231168,
title = {Design of Knowledge Management Based on Support Vector Machine},
journal = {Procedia Computer Science},
volume = {228},
pages = {1168-1176},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.110},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923019415},
author = {Ze Wang},
keywords = {Knowledge Management, Support Vector Machine, Knowledge Classification, System Design},
abstract = {With the rapid development of information technology, knowledge management has become an important means for enterprises, organizations, and individuals to gain advantages in competition. The design and implementation of Knowledge management is one of the key links of knowledge management. This paper discussed the design of Knowledge management based on Support Vector Machine (SVM), in order to improve the efficiency and quality of knowledge management. This paper introduced the application of SVM in Knowledge management, and discussed how to design an excellent Knowledge management. The research results showed that the response time of the Knowledge management designed in this paper was 87 seconds when the number of users was 300. When the number of users exceeded 300, although the response time was longer, its growth rate slowed down. SVM can be used in knowledge classification, entity recognition and relationship extraction, and can greatly improve the efficiency of Information extraction and knowledge management.}
}
@article{2021200042,
title = {Editorial Board},
journal = {Intelligent Systems with Applications},
volume = {9},
pages = {200042},
year = {2021},
issn = {2667-3053},
doi = {https://doi.org/10.1016/S2667-3053(21)00031-4},
url = {https://www.sciencedirect.com/science/article/pii/S2667305321000314}
}
@article{2022iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {201},
pages = {iii-viii},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(22)00531-2},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005312}
}
@article{ZEBA2021120971,
title = {Technology mining: Artificial intelligence in manufacturing},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120971},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120971},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004030},
author = {Gordana Zeba and Marina Dabić and Mirjana Čičak and Tugrul Daim and Haydar Yalcin},
keywords = {Artificial intelligence, Bibliometric, Content analysis, Industry 4.0, Manufacturing},
abstract = {The period of the fourth industrial revolution, called Industry 4.0, is characterized by new, innovative technologies such as: Cloud Computing; the Internet of Things; the Industrial Internet of Things; Big Data; Blockchain; Cyber-Physical Systems; Artificial Intelligence, and so on. Artificial Intelligence technology plays a significant role in modern manufacturing, particularly in the context of the Industry 4.0 paradigm. This article offers a visual and a comprehensive study of the application of Artificial Intelligence in manufacturing. Existing scholarly literature on Artificial Intelligence in manufacturing, within the Web of Science Core Collection databases, is examined in two periods: 1979-2010 and 2011-2019. These periods are viewed, respectively, as before and after the emergence of the term Industry 4.0. Bibliometric and content analysis of relevant literature is conducted and key findings are subsequently identified. The results indicate that the most important topics today are: cyber-physical systems and smart manufacturing; deep learning and big data; and real-time scheduling algorithms.}
}
@article{KIM2023103908,
title = {From technology enablers to circular economy: Data-driven understanding of the overview of servitization and product–service systems in Industry 4.0},
journal = {Computers in Industry},
volume = {148},
pages = {103908},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103908},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000581},
author = {Minjun Kim and Chiehyeon Lim and Juliana Hsuan},
keywords = {Servitization, Product-service system, Industry 4.0, Text mining, Data-driven},
abstract = {Product-based companies worldwide attempt to integrate services into their offerings, embarking on “servitization” as a key strategy. These days, the acceleration of technological innovation (i.e., Industry 4.0) has triggered an emerging IT-driven business paradigm called digital servitization or smart product-service system (PSS) that embeds Industry 4.0 technologies. As a result of these developments, related literature has expanded across different disciplines in recent years. However, understanding and describing literature is not easy considering its volume and variety. Establishing common ground for central concepts is essential for science. Thus, to clarify important topics and research issues on servitization and PSSs in Industry 4.0, we carry out a comprehensive literature review by performing text mining of 419 journal articles. A machine learning approach is applied to learn and identify the specific topics, and the suggested key references are manually reviewed to develop a state-of-the-art overview. A total of 10 key research topics are identified, and the enabler–engineering–goal framework is developed. This study contributes to clarifying a systematized view of dispersed studies of servitization and PSSs in Industry 4.0 across multiple disciplines and encourages further academic discussions and industrial transformation.}
}
@article{OLULEYE2023268,
title = {Modeling the principal success factors for attaining systemic circularity in the building construction industry: An international survey of circular economy experts},
journal = {Sustainable Production and Consumption},
volume = {37},
pages = {268-283},
year = {2023},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2023.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352550923000520},
author = {Benjamin I. Oluleye and Daniel W.M. Chan and Prince Antwi-Afari and Timothy O. Olawumi},
keywords = {Circular economy, Critical success factors, Waste management, Building construction industry},
abstract = {To achieve zero waste and cumber the acute environmental effect of the building construction industry (BCI), circular economy (CE) implementation is pertinent. Such implementation requires the incorporation of certain actionable factors that are critical to its success. However, investigating these factors considering the individualistic variations of developed and developing economies is rarely conducted in the literature. Therefore, this study evaluated the critical success factors (CSFs) for attaining systemic circularity in the BCI of both developed and developing economies. The methodological framework adopted comprises a literature review and a questionnaire survey of 140 CE experts across 39 developed and developing economies. The data collected was analyzed using exploratory factor analysis (EFA), rank agreement analysis (RAA), and fuzzy synthetic evaluation (FSE) techniques. The EFA analysis revealed four principal success factors (PSFs): data-driven digital tools and circularity plan, capacity building and pre-demolition auditing, systemic circularity guidelines and commitment, and circular metric and secondary market development. The RAA results showed that consensus and non-consensus exist between the two groups (developed and developing economies) on the PSFs. The FSE method revealed that all the PSFs are paramount in achieving a successful CE implementation in the two economies. However, the top two in developed economies are systemic circularity guidelines and commitment, and circular metric and secondary market development, while data-driven digital tools and circularity plans, and capacity building and demolition monitoring are the top two in developing economies. The RAA findings underscore the need to be context conscious while adopting the CSFs for CE implementation in the BCI. The FSE findings and the PSF models developed would guide the government and management teams in resource allocation during CE implementation. This study contributes to existing knowledge by providing essential insights into the CSFs that would promote systemic circularity attainment in the BCI of developed and developing economies.}
}
@article{CHEN2023113959,
title = {Prediction of hotel booking cancellations: Integration of machine learning and probability model based on interpretable feature interaction},
journal = {Decision Support Systems},
volume = {170},
pages = {113959},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.113959},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623000349},
author = {Shuixia Chen and Eric W.T. Ngai and Yaoyao Ku and Zeshui Xu and Xunjie Gou and Chenxi Zhang},
keywords = {Machine learning, Cancellation prediction, Bayesian network, Feature interaction},
abstract = {Reliable hotel cancellation prediction can help establish appropriate operational strategies for hotel management. In this sector, personal name records (PNR) data may be the most representative information source for prediction tasks. Despite the popularity of PNR, its inherent lack of availability has been commonly disregarded in the literature. Existing studies have directly input PNR into high-dimensional machine learning (ML) models to achieve cancellation predictions. Another type of model generates cancellation prediction based on the probability modeling of samples. In this study, we propose an interpretable feature interaction method to enrich the existing PNR information. Thereafter, we empirically assess the prediction performance of the two model classes. This study specifically determines whether or not the two methods can cross-fertilize each other to improve cancellation prediction. To do so, we propose a model integrating Bayesian networks (BNs) and Lasso regression for this prediction task. This study utilizes BNs for the probability model consistent with our correlated variables and dichotomous prediction setting. Moreover, we use a linear ML model (i.e., Lasso regression), given its advantages in reducing ineffective predictors and transparency for ranking feature importance. Empirical results show that the proposed integration model has better prediction performance, and the obtained BN estimators and interactive features are the most important predictors. This study contributes to the booking cancellation literature by proposing an interpretable feature interaction and a prediction method integrating two types of effective models. The obtained accurate and interpretable cancellation prediction further contributes to offering practical implications to hoteliers in managerial decision-making.}
}
@article{NACIRI2024242,
title = {Modeling and simulation: A comparative and systematic statistical review},
journal = {Procedia Computer Science},
volume = {232},
pages = {242-253},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924000243},
author = {Lina Naciri and Maryam Gallab and Aziz Soulhi and Safae Merzouk and Mario Di Nardo},
keywords = {Modeling, Simulation, Industry 4.0, Production flow, Optimization},
abstract = {Modeling and simulation took a considerable part of both professional and educational fields during the past few years. With the emergence of new technologies, several modeling and simulation tools, languages and softwares were developed in different areas, and are even introduced in the educational program, which can only prove their importance, especially when it comes to decision-making process. Both concepts also took part of the industry 4.0 paradigm and can be used to test and validate the digital technologies to facilitate their implementation. This variety of modeling and simulation solutions lead us to the following challenging question: which solution to adopt for which case study? The purpose of this article is to set a list of the different modeling and simulation tools, languages and softwares through a statistical literature review, focusing on the industrial field, precisely when it comes to production flow optimization and waste elimination in accordance with lean manufacturing strategy, to determine the pros and cons of each modeling and simulation solution and therefore serve as a referential for companies to identify the best match to their need.}
}
@article{KUMAR202265,
title = {Development of Machine Learning Algorithm for Characterization and Estimation of Energy Consumption of Various Stages during 3D Printing},
journal = {Procedia CIRP},
volume = {107},
pages = {65-70},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200227X},
author = {Rishi Kumar and Rishi Ghosh and Rohan Malik and Kuldip Singh Sangwan and Christoph Herrmann},
keywords = {3D Printing, Machine Learning, Long Short-Term Memory Algorithm, Stage Characterization},
abstract = {Energy usage in industries is one of the major contributors for climate change, biodiversity loss and resource scarcity. Technological advancements in digitalization led by Industry 4.0 facilitates affordable energy monitoring systems. This allows comprehensive understanding of the primary energy needs and improvement in the areas of inefficiency of a modern manufacturing system. Machine learning has the potential to reveal untapped insights, providing decision support for sustainable manufacturing by improving environmental performances, significant savings, and operational opportunities. The objectives of this research paper are to develop a machine learning algorithm for characterization, and to estimate the energy consumption of various stages in 3D printing. Machine learning model is developed using long short-term memory algorithm, and is trained, validated, and deployed for the classification of various stages during 3D printing process. Furthermore, energy consumption in each stage is estimated based on Simpson’s rule. The characterization of stages is useful for understanding the energy consumption in each stage during the 3D printing process and providing decision support to practitioners in improving the areas of energy and time inefficiencies.}
}
@article{PUGLIESE202119,
title = {Machine learning-based approach: global trends, research directions, and regulatory standpoints},
journal = {Data Science and Management},
volume = {4},
pages = {19-29},
year = {2021},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2021.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666764921000485},
author = {Raffaele Pugliese and Stefano Regondi and Riccardo Marini},
keywords = {Machine learning, Artificial intelligence, Research trends, Healthcare, Data governance, Nanotechnology, Cybersecurity},
abstract = {The field of machine learning (ML) is sufficiently young that it is still expanding at an accelerating pace, lying at the crossroads of computer science and statistics, and at the core of artificial intelligence (AI) and data science. Recent progress in ML has been driven both by the development of new learning algorithms theory, and by the ongoing explosion in the availability of vast amount of data (often referred to as "big data") and low-cost computation. The adoption of ML-based approaches can be found throughout science, technology and industry, leading to more evidence-based decision-making across many walks of life, including healthcare, biomedicine, manufacturing, education, financial modeling, data governance, policing, and marketing. Although the past decade has witnessed the increasing interest in these fields, we are just beginning to tap the potential of these ML algorithms for studying systems that improve with experience. In this paper, we present a comprehensive view on geo worldwide trends (taking into account China, the USA, Israel, Italy, the UK, and the Middle East) of ML-based approaches highlighting the rapid growth in the last 5 years attributable to the introduction of related national policies. Furthermore, based on the literature review, we also discuss the potential research directions in this field, summarizing some popular application areas of machine learning technology, such as healthcare, cyber-security systems, sustainable agriculture, data governance, and nanotechnology, and suggest that the "dissemination of research" in the ML scientific community has undergone the exceptional growth in the time range of 2018–2020, reaching a value of 16,339 publications. Finally, we report the challenges and the regulatory standpoints for managing ML technology. Overall, we hope that this work will help to explain the geo trends of ML approaches and their applicability in various real-world domains, as well as serve as a reference point for both academia and industry professionals, particularly from a technical, ethical and regulatory point of view.}
}
@article{KAZEMI2023189,
title = {Movable factory—A systematic literature review of concepts, requirements, applications, and gaps},
journal = {Journal of Manufacturing Systems},
volume = {69},
pages = {189-207},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S027861252300119X},
author = {Zahra Kazemi and Jonas Kjaer Rask and Cláudio Gomes and Emre Yildiz and Peter Gorm Larsen},
keywords = {Automation, Factory-in-a-Box, Industry 4.0, Industry 5.0, Location independent manufacturing, Mobile factory, Mobile manufacturing system, Movable factory, Pop-up factory, Reconfigurable manufacturing systems},
abstract = {The ability to move production between different geographical locations has recently become more important, due to the increasing need for sustainability and faster response times, complying with local regulations, and dealing with brittle international supply chains. To this end, the movable factory concept has been put forward, which is defined as mobile production units that can be installed near the customer’s location. Unfortunately, industrial paradigms such as Industry 4.0 make little if any notice of movable factories but instead focus on the digitalization of their fixed counterparts. In the form of a systematic literature review, this paper breaks down the concept of the movable factory, relates it to the state-of-the-art, and summarizes its main use cases, requirements, research gaps, and opportunities. The review covers over 100 relevant articles published in the past two decades. Compared to existing surveys, we have not only focused on the motivations for movable factories, but also identified research gaps and discuss the impact of modern technologies such as Internet-of-Things, digital twins, and modeling and simulation in fulfilling these gaps. The result is a survey of the state-of-the-art and a list of domains for potential future research on different aspects of movable factories.}
}
@article{MAN20242678,
title = {Probabilistic analysis of tunnel face seismic stability in layered rock masses using Polynomial Chaos Kriging metamodel},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {16},
number = {7},
pages = {2678-2693},
year = {2024},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2023.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1674775523003360},
author = {Jianhong Man and Tingting Zhang and Hongwei Huang and Daniel Dias},
keywords = {Tunnel face stability, Layered rock masses, Polynomial Chaos Kriging (PCK), Sensitivity index, Seismic loadings},
abstract = {Face stability is an essential issue in tunnel design and construction. Layered rock masses are typical and ubiquitous; uncertainties in rock properties always exist. In view of this, a comprehensive method, which combines the Upper bound Limit analysis of Tunnel face stability, the Polynomial Chaos Kriging, the Monte-Carlo Simulation and Analysis of Covariance method (ULT-PCK-MA), is proposed to investigate the seismic stability of tunnel faces. A two-dimensional analytical model of ULT is developed to evaluate the virtual support force based on the upper bound limit analysis. An efficient probabilistic analysis method PCK-MA based on the adaptive Polynomial Chaos Kriging metamodel is then implemented to investigate the parameter uncertainty effects. Ten input parameters, including geological strength indices, uniaxial compressive strengths and constants for three rock formations, and the horizontal seismic coefficients, are treated as random variables. The effects of these parameter uncertainties on the failure probability and sensitivity indices are discussed. In addition, the effects of weak layer position, the middle layer thickness and quality, the tunnel diameter, the parameters correlation, and the seismic loadings are investigated, respectively. The results show that the layer distributions significantly influence the tunnel face probabilistic stability, particularly when the weak rock is present in the bottom layer. The efficiency of the proposed ULT-PCK-MA is validated, which is expected to facilitate the engineering design and construction.}
}
@article{MAHBOUBI2024104004,
title = {Evolving techniques in cyber threat hunting: A systematic review},
journal = {Journal of Network and Computer Applications},
volume = {232},
pages = {104004},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104004},
url = {https://www.sciencedirect.com/science/article/pii/S1084804524001814},
author = {Arash Mahboubi and Khanh Luong and Hamed Aboutorab and Hang Thanh Bui and Geoff Jarrad and Mohammed Bahutair and Seyit Camtepe and Ganna Pogrebna and Ejaz Ahmed and Bazara Barry and Hannah Gately},
keywords = {Threat hunting, Hypothesis, Machine learning, OpenAI voice engine, Cyber threat intelligence, Generative AI},
abstract = {In the rapidly changing cybersecurity landscape, threat hunting has become a critical proactive defense against sophisticated cyber threats. While traditional security measures are essential, their reactive nature often falls short in countering malicious actors’ increasingly advanced tactics. This paper explores the crucial role of threat hunting, a systematic, analyst-driven process aimed at uncovering hidden threats lurking within an organization’s digital infrastructure before they escalate into major incidents. Despite its importance, the cybersecurity community grapples with several challenges, including the lack of standardized methodologies, the need for specialized expertise, and the integration of cutting-edge technologies like artificial intelligence (AI) for predictive threat identification. To tackle these challenges, this survey paper offers a comprehensive overview of current threat hunting practices, emphasizing the integration of AI-driven models for proactive threat prediction. Our research explores critical questions regarding the effectiveness of various threat hunting processes and the incorporation of advanced techniques such as augmented methodologies and machine learning. Our approach involves a systematic review of existing practices, including frameworks from industry leaders like IBM and CrowdStrike. We also explore resources for intelligence ontologies and automation tools. The background section clarifies the distinction between threat hunting and anomaly detection, emphasizing systematic processes crucial for effective threat hunting. We formulate hypotheses based on hidden states and observations, examine the interplay between anomaly detection and threat hunting, and introduce iterative detection methodologies and playbooks for enhanced threat detection. Our review encompasses supervised and unsupervised machine learning approaches, reasoning techniques, graph-based and rule-based methods, as well as other innovative strategies. We identify key challenges in the field, including the scarcity of labeled data, imbalanced datasets, the need for integrating multiple data sources, the rapid evolution of adversarial techniques, and the limited availability of human expertise and data intelligence. The discussion highlights the transformative impact of artificial intelligence on both threat hunting and cybercrime, reinforcing the importance of robust hypothesis development. This paper contributes a detailed analysis of the current state and future directions of threat hunting, offering actionable insights for researchers and practitioners to enhance threat detection and mitigation strategies in the ever-evolving cybersecurity landscape.}
}
@article{HAMZA20222581,
title = {Feature Selection with Optimal Stacked Sparse Autoencoder for Data Mining},
journal = {Computers, Materials and Continua},
volume = {72},
number = {2},
pages = {2581-2596},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.024764},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822014928},
author = {Manar Ahmed Hamza and Siwar Ben {Haj Hassine} and Ibrahim Abunadi and Fahd N. Al-Wesabi and Hadeel Alsolai and Anwer Mustafa Hilal and Ishfaq Yaseen and Abdelwahed Motwakel},
keywords = {Data mining, pattern recognition, feature selection, data classification, SSAE model},
abstract = {Data mining in the educational field can be used to optimize the teaching and learning performance among the students. The recently developed machine learning (ML) and deep learning (DL) approaches can be utilized to mine the data effectively. This study proposes an Improved Sailfish Optimizer-based Feature Selection with Optimal Stacked Sparse Autoencoder (ISOFS-OSSAE) for data mining and pattern recognition in the educational sector. The proposed ISOFS-OSSAE model aims to mine the educational data and derive decisions based on the feature selection and classification process. Moreover, the ISOFS-OSSAE model involves the design of the ISOFS technique to choose an optimal subset of features. Moreover, the swallow swarm optimization (SSO) with the SSAE model is derived to perform the classification process. To showcase the enhanced outcomes of the ISOFS-OSSAE model, a wide range of experiments were taken place on a benchmark dataset from the University of California Irvine (UCI) Machine Learning Repository. The simulation results pointed out the improved classification performance of the ISOFS-OSSAE model over the recent state of art approaches interms of different performance measures.}
}
@article{STIERLE2021113511,
title = {A technique for determining relevance scores of process activities using graph-based neural networks},
journal = {Decision Support Systems},
volume = {144},
pages = {113511},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113511},
url = {https://www.sciencedirect.com/science/article/pii/S016792362100021X},
author = {Matthias Stierle and Sven Weinzierl and Maximilian Harl and Martin Matzner},
keywords = {Process mining, Process analytics, Business process management, Deep learning, Graph neural networks},
abstract = {Process models generated through process mining depict the as-is state of a process. Through annotations with metrics such as the frequency or duration of activities, these models provide generic information to the process analyst. To improve business processes with respect to performance measures, process analysts require further guidance from the process model. In this study, we design Graph Relevance Miner (GRM), a technique based on graph neural networks, to determine the relevance scores for process activities with respect to performance measures. Annotating process models with such relevance scores facilitates a problem-focused analysis of the business process, placing these problems at the centre of the analysis. We quantitatively evaluate the predictive quality of our technique using four datasets from different domains, to demonstrate the faithfulness of the relevance scores. Furthermore, we present the results of a case study, which highlight the utility of the technique for organisations. Our work has important implications both for research and business applications, because process model-based analyses feature shortcomings that need to be urgently addressed to realise successful process mining at an enterprise level.}
}
@article{WIECKOWSKI2021376,
title = {How to determine complex MCDM model in the COMET method? Automotive sport measurement case study},
journal = {Procedia Computer Science},
volume = {192},
pages = {376-386},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.039},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101526X},
author = {Jakub Więckowski and Jarosław Wątróbski},
keywords = {COMET, F1, Prediction, MCDA;},
abstract = {Multi-criteria methods are used in systems designed to support decision-making or for prediction. One of these methods is the Characteristic Objects Method (COMET), which uses expert knowledge to calculate preference values when creating a rule base. The number of Characteristic Objects (COs) pairs necessary to perform comparisons depends on the model’s structure, the number of criteria and characteristic values. In this paper, it was decided to build a complex MCDM model based on the COMET method, which was used to predict the chances of overtaking during pit stops in Formula 1 races. To improve the performance of the model and reduce the necessary pairwise comparisons of COs, it was decided to split the structure into submodels aggregating criteria with similar characteristics to reduce the complexity of the problem. Additionally, the influence of each criterion on the obtained preference values and the final result was examined. By restructuring the model, it was possible to reduce the number of comparisons while maintaining the designed model’s correct operation.}
}
@article{TAO2024103794,
title = {Examining the inconsistent effect of privacy control on privacy concerns in e-commerce services: The moderating role of privacy experience and risk propensity},
journal = {Computers & Security},
volume = {140},
pages = {103794},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103794},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824000956},
author = {Shouzheng Tao and Yezheng Liu and Chunhua Sun},
keywords = {Perceived privacy control, Trust in e-commerce, Internet privacy experience, Risk propensity, B2C e-commerce, Information privacy concerns},
abstract = {Consumer privacy protection has become an important issue and challenge in the development of e-commerce, and consumers' concerns for privacy may lead to negative user experiences and make them more cautious about disclosing personal information. As one of the widely adopted privacy concerns inhibiting approaches, the accumulated information privacy literature indicated that providing privacy controls may not always mitigate privacy concerns, implying that there are potential boundary conditions to be clarified. This study aims to explain the inconsistent effect of perceived privacy control on privacy concerns from the perspective of individual factor differences among consumers. We investigated the roles of trust, Internet privacy experience, and risk propensity in the relationship between perceived privacy control and privacy concerns. We collected empirical data containing 625 representative samples of Chinese consumers based on a mainstream online survey platform in China. The results suggest that trust in e-commerce partially mediates the influence of perceived privacy control on privacy concerns. Internet privacy experience positively moderates the relationship between perceived privacy control and trust in e-commerce, and both Internet privacy experience and risk propensity negatively moderate the relationship between trust in e-commerce and privacy concerns. This study extends existing information privacy and trust literature through the analysis of the mediating effect of trust and the moderating effects of consumers' individual factors. We also discuss the potential positive and negative implications of this research.}
}
@article{MARNEWICK2022100061,
title = {Digitalization of project management: Opportunities in research and practice},
journal = {Project Leadership and Society},
volume = {3},
pages = {100061},
year = {2022},
issn = {2666-7215},
doi = {https://doi.org/10.1016/j.plas.2022.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2666721522000217},
author = {Carl Marnewick and Annlizé L. Marnewick},
keywords = {Digitalization, Digital transformation, Project management, Bibliometric analysis, Technologies, Research focus areas},
abstract = {The fast growth of digital technologies as well as the amount of data that devices and applications collect daily, increasingly drive organisations to radically transform their business models. The impact of digitalization on Information technology (IT) projects are evident through the adoption of agile approaches and DevOps. What is not clear, is how digitalization is impacting the larger project management discipline. A bibliometric analysis of 478 articles provides insights into the state of project management digitalization. At a high level, project management is not yet digitalized but technologies are used as tools to optimise project management processes. The results also highlight the need for continuous learning to adapt to the transformation introduced by digitalization. The introduction of the Project Management Digitalization Research Agenda Cube can be used to guide practitioners and academia to facilitate the digitalization of project management.}
}
@article{ALLAL2024108304,
title = {Leveraging the power of machine learning and data balancing techniques to evaluate stability in smart grids},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108304},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108304},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624004627},
author = {Zaid Allal and Hassan N. Noura and Ola Salman and Khaled Chahine},
keywords = {Electricity generation, Smart grid stability, Machine learning, Binary classification, Regression, Explainable AI, Data imbalance, CatBoost, Ensemble learning, SMOTE, K-means SMOTE},
abstract = {Modernizing traditional power grids into smart grids represents a significant advancement in improving efficiency and sustainability within the energy sector. However, integrating renewable energy sources and the intricate network of interconnected devices has presented new challenges, especially in grid stability. Anticipating and maintaining grid stability ensures a continuous energy supply and prevents potential disruptions. This research paper explores the smart grid stability issue through machine learning. Two distinct scenarios are examined. In the first scenario, the problem is approached as a binary classification task, where grid states are categorized as stable or unstable, a well-established method for addressing grid stability. In the second scenario, an effort is made to predict continuous values that reflect the degree of grid stability. These continuous values are subsequently mapped to a conditional function, facilitating stability detection based on a predefined threshold. To address data imbalance, two balancing strategies, specifically the Synthetic Minority Over-sampling Technique (SMOTE) and K-means SMOTE, are employed in each scenario. The analysis covers a range of regressors and classifiers, focusing on tree-based models, boosting algorithms, ensemble learners, and Multilayer Perceptrons (MLP). The investigation results demonstrate that combining the K-means SMOTE strategy with the CatBoost Classifier in the first scenario achieves the highest accuracy, reaching an impressive 99.6%, and an exceptional ability to detect grid instability, achieving 100% accuracy. Furthermore, the results of the second scenario are promising, with accuracy rates reaching 99.4%, thanks to the utilization of a stacking ensemble learner. Notably, this study introduces Explainable AI, a groundbreaking initiative in this field, to delve into the inner workings of the proposed models and enhance their transparency. This pioneering effort marks a significant step forward in understanding and optimizing the predictive capabilities of machine learning models applied to smart grid stability.}
}
@article{YEOH2022102724,
title = {A systematic synthesis of critical success factors for cybersecurity},
journal = {Computers & Security},
volume = {118},
pages = {102724},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102724},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822001195},
author = {William Yeoh and Shan Wang and Aleš Popovič and Noman H. Chowdhury},
keywords = {Critical success factors, Cybersecurity, systematic literature review, Synthesis, classification, IT capability theory},
abstract = {Extant studies suggest that cybersecurity is critical and among the IT spending priorities of organizations. In response, the literature draws attention to the cybersecurity critical success factors (CSFs) that enable organizations to focus their scarce resources accordingly. Following a systematic literature review method, we analyze and synthesize extant CSF studies on cybersecurity implementation and management for organizations. Then, drawing on the synthesized CSFs and blending them with IT capability theory, we present an overarching cybersecurity CSF framework building upon 79 cybersecurity elements grouped into 11 CSFs under five dimensions of cybersecurity capability: organizational, infrastructural, strategic, process, and external. In addition, the descriptive analysis of the search results reveals the importance of the various factors and capabilities, the trend of the cybersecurity capability dimensions, the frequency and types of research methods, and the contextual impact of the factors. This research makes an important contribution to the literature on cybersecurity management. The CSF framework serves as the foundation for future researchers interested in measuring organizational cybersecurity success. In addition, practitioners can employ the synthesized CSFs and associated elements to guide their cybersecurity management.}
}
@article{ROLF20221693,
title = {Using Knowledge Graphs and Human-Centric Artificial Intelligence for Reconfigurable Supply Chains: A Research Framework},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1693-1698},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.641},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019589},
author = {Benjamin Rolf and Nasser Mebarki and Sebastian Lang and Tobias Reggelin and Olivier Cardin and Harold Mouchère and Alexandre Dolgui},
keywords = {Reconfigurable supply chains, Knowledge graphs, Supply chain management, Human-centric artificial intelligence, Graph neural networks, Open data},
abstract = {Reconfigurable supply chains received increasing interest from academia and industry in the past years, especially because recent events such as the COVID-19 pandemic revealed the vulnerability of present supply chains. Especially the rapid digitalization and the emergence of artificial intelligence in supply chain management create new opportunities for implementing reconfigurable supply chains. In this paper, we propose a concept that uses a knowledge graph and graph-based artificial intelligence to recommend reconfigurations of the supply chain. Due to the importance of these strategic decisions, the concept considers a human-centric approach using a recommender system. The knowledge graph exploits the inherent graph structure of supply chains to create a machine-readable and human-understandable database that also supports reasoning. Finally, we provide three research hypotheses and examine the integration of open-source knowledge graphs and news monitoring tools.}
}
@article{GORSEVSKI2023105632,
title = {A free web-based approach for rainfall-induced landslide susceptibility modeling: Case study of Clearwater National Forest, Idaho, USA},
journal = {Environmental Modelling & Software},
volume = {161},
pages = {105632},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105632},
url = {https://www.sciencedirect.com/science/article/pii/S136481522300018X},
author = {Pece V. Gorsevski},
keywords = {SHALSTAB, Landslide susceptibility, Shiny, Slope stability, Web-based modeling},
abstract = {This study presents an interactive web-based approach for modeling rainfall-induced landslide susceptibility using Free Open Source Software (FOSS). The design is based on the R statistical framework and Shiny package coupled with the shallow slope stability model (SHALSTAB) from SAGA GIS. The easy-to-use real-time application extends the potential of current modeling efforts to non-expert R and GIS users and can also be used in an educational context for classroom teaching activity and enabling research-informed learning. The parsimonious approach (i.e. few parameter inputs) is accomplished in two sequential steps including modeling and validation by the use of site-specific datasets. The approach was tested in a case study on the Clearwater National Forest and the results from the validation showed an overall accuracy of 0.894, kappa of 0.789 and AUC from ROC curve was 0.715. The modeled landslide potential may be used as a decision-support tool for local planning.}
}
@article{WU2023638,
title = {Online course resource recommendation based on deep learning},
journal = {Procedia Computer Science},
volume = {228},
pages = {638-646},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018987},
author = {Yun Wu},
keywords = {deep learning, Online courses, Resources to recommend},
abstract = {In order to further improve the online learning experience and effect of courses for learners, this study, with the help of deep learning neural network algorithm, based on the characteristics of deep learning algorithm such as good processing performance and strong data adaptability, carries out research for online course students' preferences, learner characteristics and other fields, and constructs a course resource recommendation technical algorithm based on deep learning. Through algorithm experiments, it is found that the performance of NCF model is improved by 57.7% compared with that of CF model in the algorithm with RMSE as evaluation index. The performance under MAE evaluation index is improved by 58.5%, indicating that the integration of the deep learning neural network algorithm can better improve the online course recommendation effect and enhance students' online learning experience.}
}
@article{GARCIACERRUD2023562,
title = {Simulation models for public transportation: a state-of-the-art review},
journal = {Procedia Computer Science},
volume = {217},
pages = {562-569},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.252},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023304},
author = {Carmen A. García-Cerrud and Idalia {Flores de la Mota}},
keywords = {Models, simulation, public transport, energy consumption, emissions generation},
abstract = {This article presents a state-of-the-art review regarding transport simulation models currently developed. To identify the variables and types of models in transport simulation and how the operational characteristics and its associated energy consumption and emissions generation merge in a simulation that aids the decision-making process regarding energy consumption. Several simulation models are addressed and studied to propose a new simulation incorporating energy consumption and emission. In specific for public passenger transport, since it is the mean of movement in urban areas and improvements to its energy consumption and emission needs to be addressed to meet the sustainability goals by minimizing the externalities it produces.}
}
@incollection{ANGULO20241,
title = {Chapter 1 - Introduction},
editor = {Cecilio Angulo and Alejandro Chacón and Pere Ponsa},
booktitle = {Cognitive Assistant Supported Human-Robot Collaboration},
publisher = {Academic Press},
pages = {1-23},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-22135-4},
doi = {https://doi.org/10.1016/B978-0-44-322135-4.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443221354000109},
author = {Cecilio Angulo and Alejandro Chacón and Pere Ponsa},
keywords = {Industry , Artificial Internet of Things, Cognitive assistant, Human–Robot Interaction},
abstract = {Internet of Things (IoT) systems are becoming increasingly complex due to the heterogeneity of the elements involved and the demand for real-time processing near to the devices. In this context, Artificial Intelligence (AI) technologies offer powerful capabilities to enhance IoT devices with intelligent services, resulting in the emerging field known as Artificial Internet of Things (AIoT). Operators find themselves at the center of this complexity, tasking with understanding the situation and making effective real-time decisions. Therefore human factors, especially cognitive aspects, become a significant concern. The cognitive aspects of human involvement must be framed together with intelligent artefacts, a systematic approach being necessary within the domain of Joint Cognitive Systems (JCSs). New software development methods, in the form of assistants and wizards, are essential to assist operators in becoming context-aware and reducing their technical workload related to coding or computer-oriented skills. This shift allows them to focus more effectively on the tasks or services at hands. Considering research experiences in the literature regarding the role of human workers in an AIoT environment, this book analyzes the described situation in terms of Human-Centered Cyber-Physical Systems (HCPSs) with the aim of proposing a conceptual framework for these assistance systems at the cognitive level. To validate this proposal in collaborative tasks, several illustrative examples will be presented. This chapter serves as a general introduction, outlining the different topics shaping the basic aspects of the book. Initially, we delve into generic aspects of disruptive technologies within the context of Industry 4.0 or Society 5.0. Following this, we emphasize the significance of the synergy between the Internet of Things and Artificial Intelligence. Moreover, we provide a clear definition of the concept of a cognitive assistant and elucidate its relationship with human–robot teams. Lastly, we offer a detailed overview of the book's objectives and its contents.}
}
@article{XUE2024114082,
title = {Facial expression-enhanced recommendation for virtual fitting rooms},
journal = {Decision Support Systems},
volume = {177},
pages = {114082},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114082},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001574},
author = {Ying Xue and Jianshan Sun and Yezheng Liu and Xin Li and Kun Yuan},
keywords = {Virtual fitting rooms, Facial expression, Recommender system, User behaviors},
abstract = {With the development of Augmented Reality (AR) technology in the retail industry, virtual fitting room (VFR) are considered promising enhancement of e-commerce by providing users with an immersive environment to try on new products, especially fashion products. While allowing users having more vivid impression of products, virtual fitting rooms also offer sellers more channels to collect information on user preferences, which can be used to enhance recommender systems. This study proposes to leverage facial expression recognition technology together with fine-grained human-computer interactions in virtual fitting rooms to personalize product recommendations. This paper proposes a recommendation algorithm based on confidence setting, negative feedback sampling, and matrix factorization to model user behaviors in virtual fitting rooms. We conduct an experiment on 81 subjects to evaluate the proposed method. Experimental results show the proposed method outperforms existing methods using traditional behavior information. Our study provides a strong support to the value of AR in enhancing e-commerce.}
}
@article{LIU2020103199,
title = {Disentangling utilitarian and hedonic consumption behavior in online shopping: An expectation disconfirmation perspective},
journal = {Information & Management},
volume = {57},
number = {3},
pages = {103199},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103199},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617305839},
author = {Fei Liu and Eric T.K. Lim and Hongxiu Li and Chee-Wee Tan and Dianne Cyr},
keywords = {Expectation disconfirmation theory, Utilitarian expectations, Hedonic expectations, Transactional functionalities, Esthetic performance, Transactional frequency},
abstract = {Increasingly, researchers have come to acknowledge that consumption activities entail both utilitarian and hedonic components. Whereas utilitarian consumption accentuates the achievement of predetermined outcomes typical of cognitive consumer behavior, its hedonic counterpart relates to affective consumer behavior in dealing with the emotive and multisensory aspects of the shopping experience. Consequently, while utilitarian consumption activities appeal to the rationality of customers in inducing their intellectual buy-in of the shopping experience, customers’ corresponding emotional buy-in can only be attained through the presence of hedonic consumption activities. The same can be said for online shopping. Because the online shopping environment is characterized by the existence of an IT-enabled web interface that acts as the focal point of contact between customers and vendors, its design should embed utilitarian and hedonic elements to create a holistic shopping experience. Building on Expectation Disconfirmation Theory (EDT), this study advances a research model that not only delineates between customers’ utilitarian and hedonic expectations for online shopping but also highlights how these expectations can be best served through functional and esthetic performance, respectively. Furthermore, we introduce online shopping experience (i.e., transactional frequency) as a moderator affecting not only how customers form utilitarian and hedonic expectations but also how they evaluate the functional and esthetic performances of e-commerce sites. The model is then empirically validated via an online survey questionnaire administered on a sample of 303 respondents. Theoretical contributions and pragmatic implications to be gleaned from our research model and its subsequent empirical validation are discussed.}
}
@article{SEIFY2022406,
title = {Fraud Detection in Supply Chain with Machine Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {406-411},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.427},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322017153},
author = {Mahdi Seify and Mehran Sepehri and Amin Hosseinian-far and Aryana Darvish},
keywords = {Machine Learning, Supply Chain, Fraud, Detection, Pattern Recognition},
abstract = {A variety of fraud in Supply Chains may be detected either in physical parts or in cyber data. We use supervised machine learning to detect various fraud and misinformation in supply chains. The study is based on a car manufacturer concerned with increasing fraud, ranging from fraudulent invoices to inflated prices. Big data is provided for pattern recognition. A macro-level code is presented with actual algorithms developed in Python. The research is continuing, while the current work is presented with promising results.}
}
@article{CHAUHAN2022121508,
title = {Linking circular economy and digitalisation technologies: A systematic literature review of past achievements and future promises},
journal = {Technological Forecasting and Social Change},
volume = {177},
pages = {121508},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121508},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522000403},
author = {Chetna Chauhan and Vinit Parida and Amandeep Dhir},
keywords = {Circular economy, Sustainability, Product-service system (PSS), Circular business model, Artificial intelligence, Internet of things},
abstract = {The circular economy (CE) has the potential to capitalise upon emerging digital technologies, such as big data, artificial intelligence (AI), blockchain and the Internet of things (IoT), amongst others. These digital technologies combined with business model innovation are deemed to provide solutions to myriad problems in the world, including those related to circular economy transformation. Given the societal and practical importance of CE and digitalisation, last decade has witnessed a significant increase in academic publication on these topics. Therefore, this study aims to capture the essence of the scholarly work at the intersection of the CE and digital technologies. A detailed analysis of the literature based on emerging themes was conducted with a focus on illuminating the path of CE implementation. The results reveal that IoT and AI play a key role in the transition towards the CE. A multitude of studies focus on barriers to digitalisation-led CE transition and highlight policy-related issues, the lack of predictability, psychological issues and information vulnerability as some important barriers. In addition, product-service system (PSS) has been acknowledged as an important business model innovation for achieving the digitalisation enabled CE. Through a detailed assessment of the existing literature, a viable systems-based framework for digitalisation enabled CE has been developed which show the literature linkages amongst the emerging research streams and provide novel insights regarding the realisation of CE benefits.}
}
@article{ESMELI2024114339,
title = {Session context data integration to address the cold start problem in e-commerce recommender systems},
journal = {Decision Support Systems},
pages = {114339},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114339},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624001726},
author = {Ramazan Esmeli and Hassana Abdullahi and Mohamed Bader-El-Den and Ali Selcuk Can},
keywords = {Cold-start problem, Recommender systems, Session similarity},
abstract = {Recommender systems play an important role in identifying and filtering relevant products based on the behaviors of users. Nevertheless, recommender systems suffer from the ‘cold-start’ problem, which occurs when no prior information about a new session or a user is available. Many approaches to solving the cold-start problem have been presented in the literature. However, there is still room for improving the performance of recommender systems in the cold-start stage. In this article, we present a novel method to alleviate the cold-start problem in session-based recommender systems. The purpose of this work is to develop a session similarity-based cold-start session alleviation approach for recommendation systems. The developed method uses previous sessions’ contextual and temporal features to find sessions similar to the newly started one. Our results on three different datasets show that, based on the provided Mean Average Precision and Normalised Discounted Cumulative Gain scores, the Session Similarity-based Framework consistently outperforms baseline models in terms of recommendation relevance and ranking quality across three used datasets. Our approach can be used to address the challenges associated with cold start sessions where no previously interacted items are present.}
}
@article{ALTUNTAS2022100098,
title = {A hierarchical clustering based panel data approach: A case study of regional incentives},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100098},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100098},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000416},
author = {Serkan Altuntas and Sibel Selim and Fatma Altuntas},
keywords = {Foreign trade deficit, Panel data analysis, Hierarchical cluster analysis, Data mining, Turkey},
abstract = {The main objective of this research is to propose a novel approach based on panel data analysis and hierarchical cluster analysis to investigate the determinants of foreign trade deficit and cluster provinces of Turkey. This study has two important contributions into the existing literature. First, the proposed approach uses provinces based panel data. Second, the proposed approach uses structural factors as explanatory variables. The estimates show that the number of patent, trademark, and design granted, the number of incentive documents prepared, fixed investments and employment corresponding to the energy sector and employment in agricultural sector significantly affect foreign trade deficit in provinces of Turkey. Afterward, hierarchical cluster analysis is also utilized based on the results obtained from the panel data analysis. The results of the proposed approach show that provinces of Turkey are clustered into three clusters and Istanbul is the only province forming the first cluster.}
}
@article{KHODADADI2024631,
title = {Essential Data Requirements for Industrial Energy Efficiency with Digital Twins: A Case Study Analysis},
journal = {Procedia Computer Science},
volume = {238},
pages = {631-638},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924013073},
author = {Atieh Khodadadi and Sanja Lazarova-Molnar},
keywords = {Data-Driven, Digital Twin, Energy Efficiency, Smart Manufacturing System, Simulation, Industry 4.0},
abstract = {The rapid expansion of industrialization and the increase in energy demand make energy resource management necessary in various industries. Creating digital twins from available streaming data offers a practical way to enhance the efficiency of manufacturing systems. Digital twins allow for detailed analysis of key metrics, such as the energy consumption of complete manufacturing systems. Utilizing digital twins can greatly assist in improving manufacturing processes for reduced energy consumption. In this paper, we investigate the data requirements for the generation of energy-oriented digital twins of manufacturing systems. For this purpose, we developed a case study to illustrate and outline the necessary data requirements for a manufacturing system to enable the collection of data that can be used to extract energy-oriented simulation models. We, subsequently, analyze and generalize our findings that can be applied to a broader context.}
}
@article{EDALATPANAH2024107531,
title = {A hybrid time series forecasting method based on neutrosophic logic with applications in financial issues},
journal = {Engineering Applications of Artificial Intelligence},
volume = {129},
pages = {107531},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107531},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623017153},
author = {Seyyed Ahmad Edalatpanah and Farnaz Sheikh Hassani and Florentin Smarandache and Ali Sorourkhah and Dragan Pamucar and Bing Cui},
keywords = {Forecasting, Time series, Fuzzy logic, Neutrosophic sets, Quantum optimization algorithm, Genetic algorithm, Particle swarm optimization},
abstract = {Rising market demands, economic pressures, and technological advancements have spurred researchers to seek ways to enhance business environments and scientific productivity. Predictive science, crucial in this context, has gained prominence due to the rapid progress in information technology and forecasting algorithms. Time series forecasting, widely used in fields like engineering, economics, tourism, and energy, has inherent limitations with classical statistical methods, leading researchers to explore artificial intelligence and fuzzy logic for more accurate predictions. However, despite extensive efforts to improve accuracy, challenges persist. The research introduces a model aimed at surpassing existing methods in time series forecasting accuracy. This approach combines meta-heuristic optimization algorithms and neutrosophic logic to enhance precision in uncertain and complex environments, promising improved forecasting outcomes. The study shows that the performance of the neutrosophic time series modeling approach is highly dependent on the optimal selection of the universe of discourse and its corresponding intervals. This study selects the quantum optimization algorithm (QOA), genetic algorithm (GA), and particle swarm optimization (PSO) to address this weakness. These optimization algorithms improve the performance of the NTS modeling approach by selecting the global universe of discourse and corresponding intervals from the list of locally optimal solutions. The proposed hybrid model (i.e., NTS-QOA model) is verified and validated with datasets of university enrollment of Alabama (USA), Taiwan futures exchange (TAIFEX) index, and Taiwan Stock Exchange Corporation (TSEC) weighted index. Various experimental results signified the efficiency of the proposed model over existing benchmark models in terms of average forecasting error rate (AFER). This value using the proposed NTS QOA, NTS GA, and NTS PSO method on the university dataset is 0.166, 0.167, 0.164, on the TAIFEX dataset, is 0.081, 0.081, and 0.081, and on the TSEC dataset is 0.09, 0.09, and 0.09, respectively.}
}
@article{HINDLE2020483,
title = {Business analytics: Defining the field and identifying a research agenda},
journal = {European Journal of Operational Research},
volume = {281},
number = {3},
pages = {483-490},
year = {2020},
note = {Featured Cluster: Business Analytics: Defining the field and identifying a research agenda},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719308173},
author = {Giles Hindle and Martin Kunc and Michael Mortensen and Asil Oztekin and Richard Vidgen},
keywords = {Business analytics, Practice of OR, Data science, Topic models, Computational literature review},
abstract = {The special issue on business analytics has been a great endeavor with more than 100 papers received. The call for papers highlighted that business analytics has a clear role to generate competitive advantage in organizations and our focus has been to demonstrate this role through the papers finally selected for the special issue. The editorial aims to provide not only a summary of the papers but also presents our perspective on the current situation of the field through a computational literature review and comparison with the papers in the special issue. Our findings, and discussions on the papers included in the special issue, suggest that business analytics is maturing as a field with significant synergies and opportunities for the operational research community.}
}
@article{LI2024103909,
title = {A collective portfolio selection approach for investment clubs},
journal = {Information & Management},
volume = {61},
number = {2},
pages = {103909},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2023.103909},
url = {https://www.sciencedirect.com/science/article/pii/S037872062300157X},
author = {Yung-Ming Li and Lien-Fa Lin and Min-Cheng Hung},
keywords = {Investment club, Collective intelligence, LSTM, Social network, Text mining, Recommendation mechanism},
abstract = {Recently, with the popularity of social investing platforms, participating in an investment club has become a good choice for investors. Following financial experts in the investment club likely generates more profit as they have higher expertise in planning an investment portfolio. In this study, we propose a portfolio selection mechanism that combines collective intelligence extracted from investors’ opinions and LSTM stock price predictions to infer a club's investment preference and predict the profitability of the extracted investment targets. Based on a club's risk tolerance and investment preference, the proposed mechanism can create an appropriate stock portfolio for the investors in the club. Utilizing StockTwits and stock historical data, the experimental results verify that the proposed portfolio selection mechanism performs better than market indices and other benchmark approaches in the market.}
}
@incollection{2021xvii,
title = {About the editors},
editor = {Gurjit Kaur and Pradeep Tomar and Marcus Tanque},
booktitle = {Artificial Intelligence to Solve Pervasive Internet of Things Issues},
publisher = {Academic Press},
pages = {xvii-xx},
year = {2021},
isbn = {978-0-12-818576-6},
doi = {https://doi.org/10.1016/B978-0-12-818576-6.00026-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185766000265}
}
@incollection{2024493,
title = {Index},
editor = {Dimitris Mourtzis},
booktitle = {Manufacturing from Industry 4.0 to Industry 5.0},
publisher = {Elsevier},
pages = {493-506},
year = {2024},
isbn = {978-0-443-13924-6},
doi = {https://doi.org/10.1016/B978-0-443-13924-6.00021-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139246000211}
}
@incollection{2023399,
title = {Index},
editor = {Chaudhery Mustansar Hussain and Daniel Rossit},
booktitle = {Designing Smart Manufacturing Systems},
publisher = {Academic Press},
pages = {399-407},
year = {2023},
isbn = {978-0-323-99208-4},
doi = {https://doi.org/10.1016/B978-0-32-399208-4.00024-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323992084000246}
}
@article{GILLANI2024123335,
title = {Unpacking Digital Transformation: Identifying key enablers, transition stages and digital archetypes},
journal = {Technological Forecasting and Social Change},
volume = {203},
pages = {123335},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123335},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524001318},
author = {Fatima Gillani and Kamran Ali Chatha and Shakeel Sadiq Jajja and Dongmei Cao and Xiao Ma},
keywords = {Digital Transformation, Digital archetypes, Socio-technical System, Resource Based View, Operational capabilities, Digitalized operations},
abstract = {This study aims to identify recurring Digital Transformation (DT) archetypes within firms and explore the capabilities and values associated with each archetype. Underpinned by the Resource-Based View (RBV) and Socio-Technical Systems (STS) theory, the study provides a framework explicating the enabling factors, their characteristics, interdependencies, operational capabilities, and resultant value creation within the context of digitalizing firm operations. Employing a deductive-inductive approach, an exhaustive literature review combined with an in-depth, multiple case study analysis of sixteen firms provides rich insights into the digital transformation journey. Findings reveal technology, data, human capital, processes, and organization structure as key enablers alongside their critical, complementary roles for transformation. Cross-case analysis identified instances of the DT framework, uncovering three archetypes: process efficiency, responsiveness, and strategic agility. Each archetype exhibits distinct socio-technical enabler configurations and improvements to business processes, resulting in unique operational capabilities and value generation. This research provides actionable guidance for managers in leveraging socio-technical enablers to achieve successful digital transformation. It underscores the need for tailored strategies, recognizing that a one-size-fits-all approach is inadequate. Managers must strategically select desired values and develop the necessary socio-technical system components, processes, and operational capabilities to attain them.
Originality
This study develops digital archetypes of firm operations linking sociotechnical components, operational capabilities, and goals of firm operations. To the best of our knowledge, no prior research has developed such digital archetypes.}
}
@article{ONARI2024111761,
title = {An explainable data-driven decision support framework for strategic customer development},
journal = {Knowledge-Based Systems},
volume = {295},
pages = {111761},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111761},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124003964},
author = {Mohsen Abbaspour Onari and Mustafa Jahangoshai Rezaee and Morteza Saberi and Marco S. Nobile},
keywords = {Credit scoring, Customer development, Decision support framework, Explainable AI, Prisoner’s dilemma, Target setting},
abstract = {Financial institutions benefit from the advanced predictive performance of machine learning algorithms in automatic decision-making for credit scoring. However, two main challenges hamper machine learning algorithms’ applicability in practice: the complex and black-box nature of algorithms that hinder their understandability and the inability to guide rejected customers to have a successful application. Regarding customer relationship management is one of the main responsibilities of financial institutions; they must clarify the decision-making process to guide them. However, financial institutions are not willing to disclose their decision-making procedure to prevent potential risks from customers or competitors side. Hence, in this study, a decision support framework is proposed to clarify the decision-making process and model strategic decision-making to guide rejected customers simultaneously. To do so, after classifying customers in their corresponding groups, the capability of Shapley additive exPlanations method is exploited to extract the most impactful features to the prediction’s outcome globally and locally. Then, based on the benchmarking approach, the equivalent approved peer is found for the rejected customer for target setting to modify the application. To find the optimal modified values for a counterfactual prediction, a multi-objective gamed-based counterfactual explanation model is developed using the prisoner’s dilemma game as the constraint to simulate strategic decision-making. After optimization, the decision is reported to the customers concerning the credential background. A public data set is used to elaborate on the proposed framework. This framework can generate counterfactual predictions successfully by modifying perspective features.}
}
@article{ZHANG2023100619,
title = {Ethical impact of artificial intelligence in managerial accounting},
journal = {International Journal of Accounting Information Systems},
volume = {49},
pages = {100619},
year = {2023},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2023.100619},
url = {https://www.sciencedirect.com/science/article/pii/S1467089523000118},
author = {Chao Zhang and Weidong Zhu and Jun Dai and Yong Wu and Xulong Chen},
keywords = {Artificial intelligence, AI, Managerial accounting, Ethical impact, Ethics},
abstract = {Recent advances in technology have accelerated digitalization and intelligence in modern business. Particularly, the increasing use of Artificial Intelligence (AI) in managerial accounting is expected to accurately measure corporate performance, provide intelligent analyses, and predict the future of a company. However, along with the benefits, ethical concerns of using AI also arise, such as deprofessionalization, data breach, and isolation among accountants. This paper explores the ethical impact of AI in managerial accounting at both pre- and post-adoption stages. Based on 47 interviews conducted with companies, an AI system vendor, and regulators, we found that data security, privacy, and misuse; accountability; accessibility; benefits and challenges; and transparency and trust of AI are among the most common ethical risks in the development and use of AI in managerial accounting. Unique ethical impacts on four types of stakeholders: developers, managers in charge of AI adoption, managerial accountants, and regulators, were also discovered.}
}
@article{GEERTMAN2024102183,
title = {From PSScience to digital planning: Steps towards an integrated research and practice agenda for digital planning},
journal = {Computers, Environment and Urban Systems},
volume = {114},
pages = {102183},
year = {2024},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2024.102183},
url = {https://www.sciencedirect.com/science/article/pii/S0198971524001121},
author = {Stan Geertman and Patrick Witte},
keywords = {Digital planning, Planning support systems/science, Research agenda},
abstract = {Up till now, a widely accepted definition of Digital Planning is missing. Following the Editorial, digital planning is defined as the application of digital technologies and data-driven approaches to enhance efficiency, effectiveness, and inclusivity in planning processes to improve social, economic, and environmental outcomes for a sustainable urban future. It is necessary to clarify the distinction between Digital Planning and two associated terminologies: Planning Support Systems (PSS) and Planning Support Science (PSScience). PSScience and Digital Planning (DP) are envisioned as distinctive but closely interconnected. PSScience acts as the scientific base of the foremost planning practice-oriented Digital Planning. Based on this double-sided distinction and interconnection with PSScience, the relatively new concept of Digital Planning is further elaborated upon, resulting in an integrated research and practice agenda. For both approaches, a quadruple collaboration will be needed between governmental organizations, market parties, societal organizations/individuals, and educational/research institutes.}
}
@article{SHAFIE2024100461,
title = {A cluster-based human resources analytics for predicting employee turnover using optimized Artificial Neural Networks and data augmentation},
journal = {Decision Analytics Journal},
volume = {11},
pages = {100461},
year = {2024},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2024.100461},
url = {https://www.sciencedirect.com/science/article/pii/S2772662224000651},
author = {Mohammad Reza Shafie and Hamed Khosravi and Sarah Farhadpour and Srinjoy Das and Imtiaz Ahmed},
keywords = {Artificial Neural Networks, Human resources analytics, Machine learning, Employee turnover, Cluster analysis, Conditional Generative Adversarial Networks},
abstract = {This study presents an innovative methodology to predict employee turnover by integrating Artificial Neural Networks (ANN) with clustering techniques. We focus on hyperparameter tuning with various input parameters to obtain optimal ANN models. By segmenting data, the study identifies critical turnover predictors, allowing targeted interventions to be implemented to improve the efficiency and effectiveness of retention policies. Data augmentation using Conditional Generative Adversarial Networks (CTGAN) is performed on clusters with imbalanced data. Following this, the optimized ANN models are applied to these augmented clusters, leading to a notable improvement in their performance. We evaluate our optimized ANN models against five other ANN variants and four traditional machine learning models to demonstrate their superior accuracy and recall. The proposed approach achieves operational advantages by shifting away from generalized strategies to more focused, cluster-based policies, which can optimize resource utilization and reduce costs. Because of its practicality and enhanced ability to predict and manage employee turnover, this method, supported by empirical evidence, is a significant advancement in human resource (HR) analytics}
}
@article{KUMAR2022284,
title = {Live Life Cycle Assessment Implementation using Cyber Physical Production System Framework for 3D Printed Products},
journal = {Procedia CIRP},
volume = {105},
pages = {284-289},
year = {2022},
note = {The 29th CIRP Conference on Life Cycle Engineering, April 4 – 6, 2022, Leuven, Belgium.},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.047},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122000476},
author = {Rishi Kumar and P G {Padma Vilochani} and S Kahnthinisha and Omkar Patil and Felipe Cerdas and Kuldip Singh Sangwan and Christoph Herrmann},
keywords = {Live Life Cycle Assessment, 3D Printing, Cyber-Physical Production System, Visibility, Transparency},
abstract = {Sustainable manufacturing aims to deal with the challenges such as climate change, biodiversity loss and resource scarcity by minimising the environmental impacts due to product, processes, and systems through optimal use of energy and resources. Life cycle assessment (LCA) has become an important tool to identify, evaluate and assess the environmental impact of a product, process, or system, along with all the stages of the life cycle of a product. However, several drawbacks such as complexity, uncertainty and impreciseness are also associated with this methodology. Therefore, live LCA as a plausible solution, is gaining popularity with the advancements in Industry 4.o tools and techniques, enhancing ability to collect and analyse live data from various processes, interpret results, identify hotspots, trade-offs, and present better ideas about the environmental impacts in-line with the process. This paper proposes a cyber-physical production system framework for live LCA to estimate the environmental impact of 3D printed products with different combinations of design and process parameters. Also, optimum settings of these parameters are determined, and validated to minimize the environmental impacts. This enables real-time monitoring of the environmental impacts, driving prompt decision support for useful insights to the operator, project manager, business manager, or customer thereby improving the visibility and transparency.}
}
@article{ALABBAD2023105670,
title = {A web-based analytical urban flood damage and loss estimation framework},
journal = {Environmental Modelling & Software},
volume = {163},
pages = {105670},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105670},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000567},
author = {Yazeed Alabbad and Enes Yildirim and Ibrahim Demir},
keywords = {Flood damage, Flood vulnerability, Flood risk, Decision support, HAZUS},
abstract = {Information and communication technology serves a crucial role in communicating flood risk to various stakeholders and facilitating mitigation decisions. While studies extensively utilize flood inundation maps for communicating flood risks, there is a need to integrate a broad spectrum of physical vulnerability parameters into risk estimates. This research aims to build a publicly accessible web platform to analyze and estimate riverine flood-related damages using HAZUS and HEC-FIA damage functions. This framework will provide loss estimation for properties, business interruption, vehicles, bridges, and lives. The analysis is available for two scopes, including community and property. The community extent enables the user to explore socioeconomic flood information for several communities in the State of Iowa. In the property scope, the user can generate outcomes for the impacts of “what if” flood scenarios using user-provided data. The framework offers a guidance tool to help decision-makers with flood management, such as investigating mitigation interventions.}
}
@article{ZELENINA2022377,
title = {MODELING OF MANAGEMENT PROCESSES IN DISTRIBUTED ORGANIZATIONAL SYSTEMS},
journal = {Procedia Computer Science},
volume = {213},
pages = {377-384},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.081},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017720},
author = {Anna Zelenina and David Petrosov and Ekaterina Pleshakova and Aleksey Osipov and M.N. Ivanov and Oleg Choporov and Yuriy Preobrazhenskiy and Nataliy Petrsova and Sergey Roga and Liudmila Lopatnuk and N.V. Morozov},
keywords = {Digital management, optimization, multivariational modeling, system, organization},
abstract = {Because of the upward trend in the development of network organizational systems has recently become the leadership strategy of objects united in this system under the leadership of the control center, the need to focus on the processes of structural transformation of the network is becoming increasingly important in resource management. In the paper the modeling of management processes in distributed organizational systems is carried out. The illustration of the components of the structure of the network organizational system is demonstrated. The features of the structural transformation of network organizational systems, providing an increase in management efficiency are shown. The proposed approach can be used for a wide class of organizational systems.}
}
@article{TIRUNEH2020103348,
title = {Neuro-fuzzy systems in construction engineering and management research},
journal = {Automation in Construction},
volume = {119},
pages = {103348},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103348},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309286},
author = {Getaneh Gezahegne Tiruneh and Aminah Robinson Fayek and Vuppuluri Sumati},
keywords = {Artificial intelligence, Construction engineering and management, Hybrid neuro-fuzzy systems, Content analysis, Literature review},
abstract = {Neuro-fuzzy systems (NFS) can explicitly represent and model the input–output relationships of complex problems and non-linear systems, like those inherent in real-world construction engineering and management (CEM) problems. This paper contributes three things previously lacking in CEM literature: a systematic review and content analysis of published articles related to NFS topics in CEM research; identification of criteria to evaluate different NFS; and recommendations to researchers and industry practitioners in choosing a suitable subset of NFS techniques for solving different types of CEM problems. The literature review reveals that NFS classification methods are based on NFS architecture, learning algorithm, fuzzy method, and application area. This paper systematically categorizes CEM application domains (decision making, prediction/forecasting, evaluation/assessment, system modeling and analysis, simulation, and optimization) and maps them to NFS based on their suitability, which is determined using the performance evaluation criteria of convergence speed, computational complexity, interpretability, accuracy, and local minima trapping.}
}
@article{FILIP20241,
title = {Frontiers of Artificial Intelligent and Quantitative Management: Preface for ITQM 2024},
journal = {Procedia Computer Science},
volume = {242},
pages = {1-8},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.092},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018118},
author = {Florin Gheorghe Filip and Yong Shi and Paul Pocatilu and Cristian-Eugen Ciurea and Jianping Li and James M Tien and Daniel Berg},
abstract = { }
}
@article{HE2023113910,
title = {A privacy-preserving decentralized credit scoring method based on multi-party information},
journal = {Decision Support Systems},
volume = {166},
pages = {113910},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113910},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001816},
author = {Haoran He and Zhao Wang and Hemant Jain and Cuiqing Jiang and Shanlin Yang},
keywords = {Federated learning, Data privacy, Decentralized logistic regression, Credit scoring},
abstract = {With society's wide-scale adoption of information technology, significant information about borrowers is distributed across various parties, information that can be jointly used to improve credit scoring. However, use of such information faces many challenges, such as the problems of preserving privacy and information redundancy. To address these challenges in leveraging multi-source information for credit scoring, we propose a decentralized multi-party method based on logistic regression. Specifically, we formulate a logistic regression model using the vertical federated learning paradigm. To preserve data privacy during multi-party collaborative model training, we use additively homomorphic encryption based on the second-order Taylor series expansion of the loss function and its gradient. To address information redundancy and to improve the performance of the credit scoring model, we use the proposed HE-DPGD algorithm to estimate the distributed coefficients in a privacy-preserving setting. Empirical evaluation shows that the proposed method can leverage information from multiple parties securely and effectively.}
}
@article{JIMENEZ20242800,
title = {Considering cognitive biases in design: an integrated approach},
journal = {Procedia Computer Science},
volume = {232},
pages = {2800-2809},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.097},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002746},
author = {Sofia Holguin Jimenez and Xavier Godot and Jelena Petronijevic and Marc Lassagne and Bruno Daille-Lefevre},
keywords = {Cognitive biases, decision-making, bias classification, design cognition, design activities, metacognition, debiasing methodology},
abstract = {Design is a dynamic, decision-driven process, often guided by intuition and experience. It can be susceptible to cognitive biases, systematic deviations in information processing and decision making, which have been recognized as influential factors affecting expert judgment in multiple domains. Although some studies in the design field have investigated and proposed methods to address specific biases, such as the confirmation bias, there is currently no comprehensive approach in the literature to make designers aware of the various biases that may manifest during the design process. The main contribution of this article is to provide designers with a broad overview of the biases that may be involved within the three principal areas of design cognition: problem formulation, concept generation and concept evaluation. It also proposes a novel and workable methodology to facilitate designers' recognition and mitigation of biases through metacognition, while favoring the implementation of more specific correction strategies.}
}
@article{GIAMBONA2024101916,
title = {Skills in online job ads: An analysis of Italian regions},
journal = {Socio-Economic Planning Sciences},
volume = {94},
pages = {101916},
year = {2024},
issn = {0038-0121},
doi = {https://doi.org/10.1016/j.seps.2024.101916},
url = {https://www.sciencedirect.com/science/article/pii/S0038012124001150},
author = {Francesca Giambona and Adham Kahlawi and Lucia Buzzigoli and Laura Grassini and Cristina Martelli},
keywords = {Online job ads, ESCO, Skill similarity, Skill change, Cluster analysis},
abstract = {Economists and other social scientists are increasingly leveraging web data to tackle socio-economic issues and integrate existing sources of information. The data generated by online platforms and websites offer a wealth of multidimensional information with diverse potential applications in economic analysis. Online job advertisement data, for instance, allows us to enhance our understanding of the labour market by providing timely insights into business demand and the skills required for various job positions. This paper examines skill similarities at the regional level in Italy and the variations in skills sought by businesses over time using Lightcast data for 2019 and 2020. In light of this, we use two measures related to the regional skill similarities, an index for the regional skill changes, jointly with some features of the local economy, in order to provide a picture of Italian regions.}
}
@article{LANGLOTZ2022549,
title = {Concept of hybrid modeled digital twins and its application for an energy management of manufacturing systems},
journal = {Procedia CIRP},
volume = {112},
pages = {549-554},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122012677},
author = {Pascal Langlotz and Matthias Klar and Li Yi and Marco Hussong and Fábio J.P. Sousa and Jan C. Aurich},
keywords = {Digital Twin, Data-driven Models, Physics-based Models, Production},
abstract = {Digital twins are used to digitally map properties and states of real manufacturing systems. Based on relevant data-driven and physics-based digital models, digital twins are employed to monitor, analyze and predict aspects of the corresponding manufacturing system. Therefore, digital twins are used on different manufacturing levels, e.g. the factory, machine, or process level. This paper presents a concept for digital twins that are modeled by physics-based and data-driven models. Furthermore, this concept is validated with the help of a use case that controls an energy management system of a model-scale factory by finding the optimal time for charging and using the battery.}
}
@article{FEI2024102585,
title = {Dempster–Shafer theory-based information fusion for natural disaster emergency management: A systematic literature review},
journal = {Information Fusion},
volume = {112},
pages = {102585},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102585},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003634},
author = {Liguo Fei and Tao Li and Weiping Ding},
keywords = {Dempster–Shafer theory, Information fusion, Emergency management, Quantitative analysis, Natural disaster},
abstract = {The frequency and unpredictability of natural disasters pose serious challenges to emergency management in modern society. Effective emergency management requires not only rapid response, but also accurate assessment of the situation, rational allocation of resources and scientific decision-making. Dempster–Shafer theory (DST), as a powerful information fusion tool, has been widely used in natural disaster emergency management in recent years. This study sorted out the core related articles of DST in various directions in the field of natural disaster emergency management, and formed a systematic literature review. In order to support and guide the completion of this work, keywords were selected according to the rules of Systematic literature review (SLR) and the requirements of article content research to screen and determine the literatures. On this basis, relevant information of the selected literatures was analyzed, including authors, institutions, countries, keywords, etc. Then, according to the theoretical framework of integrated emergency management, fifteen structured research questions are put forward, involving various aspects of integrated emergency management system, and these questions are answered in detail. After that, it discussed the literature, and summarized the contribution and future development direction of DST based on information fusion in natural disaster emergency management. The final results show that in the field of natural disaster emergency management, DST plays various and important roles, among which one of the most important roles is the integration of decision-making evidence at various stages of disasters, so as to make better decisions. By combining with other methods, DST improves its limitations. And in the process to expand their scope of use. Then, it focuses on how to solve the extended theory, practical application dimension and related defects of DST in the field of natural disaster emergency management.}
}
@article{ROSARIO2023100203,
title = {How has data-driven marketing evolved: Challenges and opportunities with emerging technologies},
journal = {International Journal of Information Management Data Insights},
volume = {3},
number = {2},
pages = {100203},
year = {2023},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2023.100203},
url = {https://www.sciencedirect.com/science/article/pii/S2667096823000496},
author = {Albérico Travassos Rosário and Joana Carmo Dias},
keywords = {Data-driven, Marketing, Customer relationship management, Customer data, Big data},
abstract = {This paper explores data-driven marketing, its benefits, and challenges to provide insights and a framework for business leaders and marketers to leverage in their marketing efforts. Using a systematic literature review with bibliometric analysis, we examined a sample of 98 studies indexed in SCOPUS to identify research activity on this topic until May 2023. Data-driven marketing and communications have gained popularity as companies and marketers prioritize leveraging consumer and market data to strategize and make informed decisions. Adopting and integrating various technologies into the company's systems is crucial for companies aiming to achieve a sustainable competitive advantage. By analyzing and interpreting customer data, companies can implement customer-centered marketing initiatives that enhance their experience and satisfaction with the brand, thus strengthening company-customer relationships. However, data-driven marketing faces several challenges, including consumer privacy concerns, data poisoning, information quality issues, and the need for adequate organizational changes.}
}
@article{YI2021100002,
title = {Integrated computer vision algorithms and drone scheduling},
journal = {Communications in Transportation Research},
volume = {1},
pages = {100002},
year = {2021},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2021.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2772424721000020},
author = {Wen Yi and Hans Wang and Yong Jin and Jiannong Cao}
}
@article{DAI2023114036,
title = {Does the new entrant eat my pie or enlarge my pie? Market entry investigation in the online-to-offline on-demand context},
journal = {Decision Support Systems},
volume = {175},
pages = {114036},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114036},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001112},
author = {Hongyan Dai and La Ta and Xun Xu},
keywords = {E-commerce, Online-to-offline mode, On-demand, Market entry, Consumer shopping behavior},
abstract = {The facilitation of information technology has led to the rapid development of online-to-offline commerce, and this mode is attracting increasing businesses to enter the market. Its unique on-demand features affect consumers' shopping behaviors. This study uses a difference-in-difference approach with propensity score matching and machine learning approaches to examine the impact of a new giant online-to-offline entrant on consumers' ordering and spending behavior. We find that overall, the new entrant has a negative impact on consumers' ordering and spending from the perspective of existing providers. This negative impact is amplified if the consumers are highly engaged on the platform. However, the new entrant effect is reduced if the providers use in-house rather than outsourced deliverymen or consumers have a relatively long distance from the new entrant. Furthermore, the new entrant effect is altered if the platform implements technological innovation. On a provider level, the new entrant has a direct negative impact on consumers' ordering and spending from homogenous providers; however, a positive spillover effect is observed in terms of increasing consumers' spending on heterogeneous providers. The direct effect occurs immediately, while the spillover effect appears in a time-lagged pattern. On a product level, we find that the new entrant negatively affects consumers' ordering and spending on substitute products also sold by the new entrant, but that their spending on non-substitute products is boosted. Our findings provide decision support for the new entrant and existing providers to expand the market to stimulate consumers' purchase from both, to enhance their financial performance.}
}
@article{HU2021102404,
title = {The impact of SETA event attributes on employees’ security-related Intentions: An event system theory perspective},
journal = {Computers & Security},
volume = {109},
pages = {102404},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102404},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002285},
author = {Siqi Hu and Carol Hsu and Zhongyun Zhou},
keywords = {Security education, Training, and awareness (SETA), Compliance intention, Extra-role behavioral intention, Event strength, Event system theory},
abstract = {Current research recommends security education, training, and awareness (SETA) programs as an effective way to counter internal security threats and promote employees’ security behaviors. Contrary to the dominant approach, which views SETA as a simple, single construct, we argue that the attributes of SETA are the key determinants of the programs’ impact on individuals. More specifically, we bridge this gap by using event system theory (EST) to conceptualize SETA programs as “organization events” and empirically test the role of SETA attributes in affecting employees’ intentions to comply with security policy (i.e., in-role behavioral intentions) and extra-role behavioral intentions. The results of this research advance the understanding of SETA attributes and their impacts on employees’ compliance intentions and extra-role behavioral intentions from the EST perspective. The results suggest that the novelty of SETA event is more effective in fueling extra-role behavioral intention than compliance intention. The results also suggest that criticality exerts a similar positive influence on both compliance and extra-role behavioral intention, while the disruption of SETA event has a significant negative effect on these two intentions. Further, there is evidence that the negative relationship between SETA event disruption and two security behavioral intentions is stronger when the SETA event is dispersed throughout a wide range of organizational hierarchy levels. The expected research and practical implications are also discussed in the paper.}
}
@article{WANG2020103173,
title = {Recognizing CEO personality and its impact on business performance: Mining linguistic cues from social media},
journal = {Information & Management},
volume = {57},
number = {5},
pages = {103173},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103173},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618307390},
author = {Shichao Wang and Xi Chen},
keywords = {Upper echelons theory, CEO personality, Social media, Business performance},
abstract = {Upper echelons theory suggests that CEO personality will influence organizational performance. However, difficulty in measuring CEO personality restrains related research. We capture linguistic cues CEOs leaving on social media and recognize their personality by text mining. To our knowledge, it is the first study introducing social media text mining approaches into the research stream that empirically inquires and extends upper echelons theory. Then, we investigate the CEO personality’s impact on both operational and financial performance. Results show that CEO Extraversion, Emotional Stability, and Agreeableness improve Cost Efficiency and Profitability, while CEO Conscientiousness reduces them. CEO Openness to Experience negatively influences Profitability, and all facets of CEO personality improve Employee Productivity except for CEO Conscientiousness. The contribution of our research is multi-sided: (1). methodologically, we introduce a text mining approach to measure CEO personality; (2). theoretically, we provide empirical evidence for upper echelons theory; (3). practically, our results help companies evaluate CEO candidates from a personality perspective.}
}
@article{KHANG20213590,
title = {Machine learning for liquidity prediction on Vietnamese stock market},
journal = {Procedia Computer Science},
volume = {192},
pages = {3590-3597},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.132},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018718},
author = {Pham Quoc Khang and Klaudia Kaczmarczyk and Piotr Tutak and Paweł Golec and Katarzyna Kuziak and Radosław Depczyński and Marcin Hernes and Artur Rot},
keywords = {stock market, liquidity, machine learning, prediction},
abstract = {As a critical consideration in investment decisions, stock liquidity has significance for all stakeholders in the market. It also has implications for the stock market’s growth. Liquidity enables investors and issuers to meet their requirements regarding investment, financing or hedging, reducing investment costs and the cost of capital. The aim of this paper is to develop the machine learning models for liquidity prediction. The subject of research is the Vietnamese stock market, focusing on the recent years - from 2011 to 2019. Vietnamese stock market differs from developed markets and emerging markets. It is characterized by a limited number of transactions, which are also relatively small. The Multilayer Perceptron, Long-Short Term Memory and Linear Regression models have been developed. On the basis of the experimental results, it can be concluded that the LSTM model allows for prediction characterized by lowest value of MSE. The results of research can be used for developing the methods for decision support on stock markets.}
}
@article{DESMEDT2023102145,
title = {Process model forecasting and change exploration using time series analysis of event sequence data},
journal = {Data & Knowledge Engineering},
volume = {145},
pages = {102145},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102145},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000058},
author = {Johannes {De Smedt} and Anton Yeshchenko and Artem Polyvyanyy and Jochen {De Weerdt} and Jan Mendling},
keywords = {Process model forecasting, Predictive process modelling, Process mining, Time series analysis, User study},
abstract = {Process analytics is a collection of data-driven techniques for, among others, making predictions for individual process instances or overall process models. At the instance level, various novel techniques have been recently devised, tackling analytical tasks such as next activity, remaining time, or outcome prediction. However, there is a notable void regarding predictions at the process model level. It is the ambition of this article to fill this gap. More specifically, we develop a technique to forecast the entire process model from historical event data. A forecasted model is a will-be process model representing a probable description of the overall process for a given period in the future. Such a forecast helps, for instance, to anticipate and prepare for the consequences of upcoming process drifts and emerging bottlenecks. Our technique builds on a representation of event data as multiple time series, each capturing the evolution of a behavioural aspect of the process model, such that corresponding time series forecasting techniques can be applied. Our implementation demonstrates the feasibility of process model forecasting using real-world event data. A user study using our Process Change Exploration tool confirms the usefulness and ease of use of the produced process model forecasts.}
}
@article{FAHNDRICH2023301617,
title = {Digital forensics and strong AI: A structured literature review},
journal = {Forensic Science International: Digital Investigation},
volume = {46},
pages = {301617},
year = {2023},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2023.301617},
url = {https://www.sciencedirect.com/science/article/pii/S2666281723001294},
author = {Johannes Fähndrich and Wilfried Honekamp and Roman Povalej and Heiko Rittelmeier and Silvio Berner and Dirk Labudde},
keywords = {Strong AI, Digital forensics, Artificial intelligence, Digital investigations},
abstract = {Forensics is an established field of research. Digital forensics started 44 years ago with the Florida Computer Crimes Act (1978) including legislation against the unauthorized modification of data on a computer system. Since then, the field has flourished in different subdomains. The overall definitions and concepts have been specified by a small group of experts. Furthermore, the need for development is created by the amount of digital evidence which is collected concerning most crimes. This paper gives an overview of the state-of-the-art by presenting a structured literature review of digital forensic about methods and concepts using Artificial Intelligence (AI) technologies. The review focuses on science done on topics in strong AI and forensics.}
}
@article{NASEER2023103525,
title = {Moving towards agile cybersecurity incident response: A case study exploring the enabling role of big data analytics-embedded dynamic capabilities},
journal = {Computers & Security},
volume = {135},
pages = {103525},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103525},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823004352},
author = {Ayesha Naseer and Humza Naseer and Atif Ahmad and Sean B Maynard and Adil Masood Siddiqui},
keywords = {Cybersecurity, Incident response, Big data analytics, Dynamic capabilities, Agility},
abstract = {Organizations are at risk of cyber-attacks more than ever before due to the ongoing digitalization of business operations. Industry reports indicate that it is not a matter of if but when organizations become victims of cyber-attacks or breaches. In this research, we argue that organizations must enable agility in their incident response (IR) to quickly respond to diverse cybersecurity threats, and big data analytics (BDA) plays a pivotal role in enabling agility in the IR. Drawing from dynamic capabilities theory, we conducted a field study using a case study approach to examine the following research question: What dimensions of big data analytics-embedded dynamic capabilities enable agility in cybersecurity incident response? We develop a framework that presents five key dimensions of BDA-embedded dynamic capabilities (data consolidation, threat intelligence, incident investigation, analytical skillset, and cybersecurity analytics warehouse) in IR at four specific stages, that is, manual analysis, basic analytics, advanced analytics, and pervasive analytics. The detail of the framework explains how BDA-embedded dynamic capabilities at the pervasive analytics stage enable agility in IR by infusing agile characteristics of flexibility, speed, and learning in IR. This study contributes to the knowledge of IT-embedded dynamic capabilities and cybersecurity IR agility. Detailed recommendations are also provided for potential practitioners.}
}
@article{GHASEMAGHAEI2024103921,
title = {Understanding how algorithmic injustice leads to making discriminatory decisions: An obedience to authority perspective},
journal = {Information & Management},
volume = {61},
number = {2},
pages = {103921},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.103921},
url = {https://www.sciencedirect.com/science/article/pii/S037872062400003X},
author = {Maryam Ghasemaghaei and Nima Kordzadeh},
keywords = {Algorithmic injustice, Algorithmic bias, Trust in AI, Displacement of responsibility, Discrimination, Guilt, Obedience to authority},
abstract = {Unjust algorithmic recommendations can lead decision makers to discriminatory choices, risking harm to individuals or groups. This study addresses this concerning phenomenon and examines its implications. In an experimental study involving 122 managers, we found that algorithmic injustice causes discriminatory decisions without heightened guilt perception. Additionally, trust in data analytics outcomes moderates the impact of algorithmic injustice on discrimination and marginally influences the impact of discriminatory decision making on guilt perception. However, displacement of responsibility has no moderating effect on either relationship. These findings highlight the potential negative consequences of algorithmic decision making, emphasizing the need for caution and awareness.}
}
@article{OLAYA2020113320,
title = {Uplift Modeling for preventing student dropout in higher education},
journal = {Decision Support Systems},
volume = {134},
pages = {113320},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113320},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300750},
author = {Diego Olaya and Jonathan Vásquez and Sebastián Maldonado and Jaime Miranda and Wouter Verbeke},
keywords = {Learning analytics, Uplift modeling, Student dropout, Educational data mining},
abstract = {Uplift modeling is an approach for estimating the incremental effect of an action or treatment at the individual level. It has gained attention in the marketing and analytics communities due to its ability to adequately model the effect of direct marketing actions via predictive analytics. The main contribution of our study is the implementation of the uplift modeling framework to maximize the effectiveness of retention efforts in higher education institutions i.e., improvement of academic performance by offering tutorials. The objective is to improve the design of retention programs by tailoring them to students who are more likely to be retained if targeted. Data from three different bachelor programs from a Chilean university were collected. Students who participated in the tutorials are considered the treatment group, otherwise, they are assigned to the nontreatment group. Our results demonstrate the virtues of uplift modeling in tailoring retention efforts in higher education over conventional predictive modeling approaches.}
}
@article{TESHOME2024109186,
title = {Simulating soil hydrologic dynamics using crop growth and machine learning models},
journal = {Computers and Electronics in Agriculture},
volume = {224},
pages = {109186},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109186},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924005775},
author = {Fitsum T. Teshome and Haimanote K. Bayabil and Bruce Schaffer and Yiannis Ampatzidis and Gerrit Hoogenboom and Aditya Singh},
keywords = {Crop water requirement, DSSAT, Gradient boosting, Simulation, Support vector machine, Variable rate irrigation},
abstract = {Accurate measurement of crop evapotranspiration (ETc) and soil moisture content (SMC) is critical for different purposes, including developing irrigation scheduling practices that improve water use efficiency and crop yield. The objectives of this study were to 1) simulate daily ETc and SMC of green beans and sweet corn under full irrigation and three deficit irrigation rates using the Decision Support System for Agrotechnology Transfer (DSSAT) CROPGRO-Green bean and CERES–Sweet corn models and 2) evaluate the performance of three machine learning models for simulating ETc of green beans and sweet corn. The DSSAT models were calibrated using measured phenological, biomass, and yield data collected during two years of experiments at the University of Florida, Tropical Research and Education Center (TREC) during the winter (dry) seasons of 2020–21 and 2021–22. The experiments were conducted under four irrigation treatments: 100% full irrigation (FI), 75% FI, 50% FI, and 25% FI, with four replications. The simulated daily ETc and SMC under each irrigation treatment were compared with estimated ETc based on changes in SMC and measured SMC, respectively. Estimating ETc based on changes in SMC involved quantifying sub-hourly moisture loss during the drying phase of soil and aggregating the results into daily timesteps. The eXtreme Gradient Boosting (XGB), light gradient-boosting machine (LightGBM), and Support Vector Machine (SVM) models were trained using SMC and weather variables to simulate daily ETc of green beans and sweet corn. The results showed that the CROPGRO and CERES models could reasonably simulate SMC dynamics of green beans and sweet corn plots. The models simulated SMC better for the three deficit irrigation treatments than the full irrigation treatment. The CROPGRO model evaluation index of agreement (d-Stat) and mean absolute error (MAE) for 25% FI treatment were 0.72 and 0.03 cm3 cm−3, and 0.75 and 0.03 cm3 cm−3 for the CERES model. However, the DSSAT models did not perform well in simulating ETc for green beans and sweet corn. Average d-Stat and MAE for the CROPGRO during model evaluation were 0.5- and 1.0-mm day−1 and 0.6- and 1.1-mm day−1 for CERES, respectively. The performance of all machine learning (ML) models for simulating daily ETc of green beans and sweet corn was better than the CROPGRO and CERES models. Incorporating machine learning algorithms into the DSSAT model has the potential to enhance its performance in simulating ETc. Overall, the results suggest that DSSAT and ML models could potentially be used as alternative decision support tools for assessing and managing irrigation strategies and optimizing water use efficiency under different management and environmental conditions.}
}
@article{ZIMMERMANN2022706,
title = {Job Profiles in the Field of Data-Driven Supply Chain Management An Analysis of the Austrian Job Market},
journal = {Procedia Computer Science},
volume = {204},
pages = {706-713},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.085},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008237},
author = {Robert Zimmermann and Patrick Brandtner},
keywords = {Supply Chain Management, Data, Job Advertisments, Data Science, Data Analytics},
abstract = {Supply chains are immersed in extremely complex and dynamic environments. Reducing uncertainty and improving decision making by means of data-driven supply chain management (SCM) has increasingly become an indispensable core of organizational management. To implement data-driven SCM, companies need to possess the right skilled employees. Job advertisements in the field of data-driven SCM come with multiple different titles, sets of tasks, requirements, and desired soft skills. Therefore, determining which job profile typically inherits which specific requirements, tasks and soft skills presents a challenging task. To illuminate this question, we analyzed the entire Austrian SCM job market and extracted all available job profiles, their specific tasks, requirements, and basic salaries. Analyzing these data, we give recommendations about which qualifications and skills an applicant should inherit for a specific job profile. Additionally, we highlight which tasks typically need to be performed and what minimum salaries can be expected for the specific job profiles. Thus, our results help companies to specify their job advertisements in the field of data-driven SCM and provide job applicants with an overview of tasks they can expect, requirements they need to fulfill, and skills they need to acquire. From a scientific point of view, our results contribute to the body of knowledge by providing insights into the Austrian SCM domain, by offering starting points on how to adapt training and education programs and research projects in the university sector.}
}
@article{TAN2024103996,
title = {Privacy-preserving federated learning for proactive maintenance of IoT-empowered multi-location smart city facilities},
journal = {Journal of Network and Computer Applications},
volume = {231},
pages = {103996},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.103996},
url = {https://www.sciencedirect.com/science/article/pii/S1084804524001735},
author = {Zu-Sheng Tan and Eric W.K. See-To and Kwan-Yeung Lee and Hong-Ning Dai and Man-Leung Wong},
keywords = {Internet of Things (IoT), Proactive maintenance management (PMM), Federated Learning (FL), Long short-term memory (LSTM), Fully homomorphic encryption (FHE)},
abstract = {The widespread adoption of the Internet of Things (IoT) and deep learning (DL) have facilitated a social paradigm shift towards smart cities, accelerating the rapid construction of smart facilities. However, newly constructed facilities often lack the necessary data to learn any predictive models, preventing them from being truly smart. Additionally, data collected from different facilities is heterogeneous or may even be privacy-sensitive, making it harder to train proactive maintenance management (PMM) models that are robust to provide services across them. These properties impose challenges that have not been adequately addressed, especially at the city level. In this paper, we present a privacy-preserving, federated learning (FL) framework that can assist management personnel to proactively manage the maintenance schedule of IoT-empowered facilities in different organizations through analyzing heterogeneous IoT data. Our framework consists of (1) an FL platform implemented with fully homomorphic encryption (FHE) for training DL models with time-series heterogeneous IoT data and (2) an FL-based long short-term memory autoencoder model, namely FedLSTMA, for facility-level PMM. To evaluate our framework, we did extensive simulations with real-world data harvested from IoT-empowered public toilets, demonstrating that the DL-based FedLSTMA outperformed other traditional machine learning (ML) algorithms and had a high level of generalizability and capabilities of transferring knowledge from existing facilities to newly constructed facilities under the situation of huge data heterogeneity. We believe that our framework can be a potential solution for overcoming the challenges inherent in managing and maintaining other smart facilities, ultimately contributing to the effective realization of smart cities.}
}
@article{WANG2023113913,
title = {Attentive statement fraud detection: Distinguishing multimodal financial data with fine-grained attention},
journal = {Decision Support Systems},
volume = {167},
pages = {113913},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113913},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001841},
author = {Gang Wang and Jingling Ma and Gang Chen},
keywords = {Financial statement fraud detection, Fusion ambiguity, Ratio-aware attention, Chapter-aware attention, Modality-aware attention},
abstract = {Financial statement fraud caused by listed companies directly jeopardizes the reliability the financial reporting process. Leveraging multimodal information for financial statement fraud detection (FSFD) has recently become of great interest to academic research and industrial applications. Unfortunately, the predictive ability of multimodal information in FSFD remains largely underexplored, particularly the fusion ambiguity embedded in and among multi-modalities. In this study, we propose a novel attention-based multimodal deep learning method, named RCMA, toward an accurate FSFD. RCMA synthesizes a fine-grained attention mechanism including three innovative attention modules, i.e., ratio-aware attention, chapter-aware attention, and modality-aware attention mechanism. The first two attention mechanisms help to liberate the extraordinary predictive power of the financial modality and the textual modality on FSFD, respectively. Moreover, the proposed modality-aware attention mechanism enables better coordination between the two modalities. Furthermore, to ensure effective learning on the attention-based multimodal embedding, we design a novel loss function named Focal and Consistency Loss, or FCL. It considers class-imbalance and modality-consistency simultaneously, to specialize the optimization of FSFD. The experimental results on the real-world dataset show that the proposed RCMA on FSFD task outperformed the state-of-the-art benchmarks. Furthermore, interpretation analysis visualizes the attention weights of different ratio groups, chapters, and modalities from RCMA, and illustrates how these interpretations influence stakeholders' decision process for FSFD.}
}
@article{KOLIOUSIS2024109324,
title = {Artificial intelligence and policy making; can small municipalities enable digital transformation?},
journal = {International Journal of Production Economics},
volume = {274},
pages = {109324},
year = {2024},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2024.109324},
url = {https://www.sciencedirect.com/science/article/pii/S0925527324001816},
author = {Ioannis Koliousis and Abdulrahman Al-Surmi and Mahdi Bashiri},
keywords = {Digital transformation, Strategic decision making, Policy, Generative AI, SEM},
abstract = {This study investigates digital transformation and the usability of emerging technologies in policymaking. Prior studies categorised digital transformation into three distinct phases of digitisation, digitalisation, and digital transformation. They mainly focus on the operational or functional levels, however, this study considers digital transformation at the strategic level. Previous studies confirmed that using new emerging AI-based technologies will enable organisations to use digital transformation to achieve higher efficiency. A novel methodological AI-based approach for policymaking was constructed into three phases through the lens of organisational learning theory. The proposed framework was validated using a case study in the transportation industry of a small municipality. In the selected case study, a confirmatory model was developed and tested utilising the Structural Equation Modelling with data collected from a survey of 494 local stakeholders. Artificial Neural Network was utilised to predict and then to identify the most appropriate policy according to cost, feasibility, and impact criteria amongst six policies extracted from the literature. The results from this research confirm that utilisation of the AI-based strategic decision-making through the proposed generative AI platform at strategic level outperforms human decision-making in terms of applicability, efficiency, and accuracy.}
}
@article{WOSCHANK20201814,
title = {A Holistic Didactical Approach for Industrial Logistics Engineering Education in the LOGILAB at the Montanuniversitaet Leoben},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1814-1818},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.252},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321302},
author = {M. Woschank and Corina Pacher},
keywords = {Industrial Logistics Engineering Education, Industry 4.0, Holistic Didactical Approach, Smart Logistics, Logistics 4.0},
abstract = {The successful implementation of Industry 4.0 initiatives can only be established by focusing on a multitude of complex and partially interdepending requirements and enabling factors. Thereby, the education and further training of employees can be seen as one of the major prerequisites to understand, handle, and further develop existing Industry 4.0 concepts and technologies. This paper reviews the current state of the art regarding the usage of learning labs, as modern teaching and learning methods, for industrial logistics engineering education from a holistic perspective. Moreover, the authors discuss learning within the framework of action-based learning approaches as a reflected form of meaningful social interaction and introduce an exemplary system-theoretical and constructivist approach based on the LOGILAB at the Montanuniversitaet Leoben.}
}
@article{WAGNER2021101694,
title = {Exploring the boundaries and processes of digital platforms for knowledge work: A review of information systems research},
journal = {The Journal of Strategic Information Systems},
volume = {30},
number = {4},
pages = {101694},
year = {2021},
note = {2021 Review Issue},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2021.101694},
url = {https://www.sciencedirect.com/science/article/pii/S096386872100041X},
author = {Gerit Wagner and Julian Prester and Guy Paré},
keywords = {Digital platforms, Knowledge work, Online labor markets, Outsourcing, Crowdwork, Literature review},
abstract = {Digital platforms for knowledge work (DPKW), such as Upwork, Freelancer, and Fiverr, connect clients with millions of workers for a range of knowledge work services, including app development, graphic design, and data analytics. Research on this emergent phenomenon has recently gained traction in terms of publication volume and research diversity. Focusing on the contributions of information systems research, we conducted a literature review to distinguish papers on DPKW from related types of digital platforms, to synthesize what we know about knowledge work on DPKW, and to guide future research. Based on a comprehensive literature search, we derived five boundary conditions, which constitute our definition of DPKW: digitality, value network paradigm, centralized governance, contractual work, and knowledge work. We further developed a conceptual process framework of the constituent processes of DPKW. With this framework, we elaborate on an established process model to distinguish the three macro­level processes of matching, contracting, and executing. We further examined micro­level processes suggested in extant research based on a process linking approach in order to understand how they synchronically instantiate each macro­level process. Emphasizing the significance of the micro­ and macro­level processes and the emergent stage of the literature on DPKW, we offer an agenda for future research and outline implications for practice.}
}
@article{YOSHIKUNI2023100206,
title = {Big data and business analytics enabled innovation and dynamic capabilities in organizations: Developing and validating scale},
journal = {International Journal of Information Management Data Insights},
volume = {3},
number = {2},
pages = {100206},
year = {2023},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2023.100206},
url = {https://www.sciencedirect.com/science/article/pii/S2667096823000526},
author = {Adilson Carlos Yoshikuni and Rajeev Dwivedi and Duanning Zhou and Samuel Fosso Wamba},
keywords = {Big data, Business analytics, Attributes development, Dynamic capabilities, Innovation, Development of scale, Scale validation},
abstract = {In recent years, innovation and competitive advantages have been built through information systems (IS); in particular, big data and business analytics (BDA) capabilities are being highlighted as essential enablers in creating innovation. This research focused on developing a big data scale using the Dynamic Capabilities View (DCV). DCV is based on an organization's ability to sense, seize, and transform capabilities to transform organizations and leverage innovations to remain competitive in the changing business environment. This study is based on convergent and discriminant validity using PLS-SEM with a sample of 191 firms. The proposed model includes twelve traits connected to sensing, seizing, and transforming attributes. The research outcome validates the scale and demonstrates that BDA enables dynamic capabilities and can achieve innovation in organizations when coupled with strategic management practices and IT resources that can be used to measure IT-business value.}
}
@article{TADDEI2022108268,
title = {Circular supply chains in the era of industry 4.0: A systematic literature review},
journal = {Computers & Industrial Engineering},
volume = {170},
pages = {108268},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108268},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222003369},
author = {Emilia Taddei and Claudio Sassanelli and Paolo Rosa and Sergio Terzi},
keywords = {Circular Economy, Circular Supply Chain, Industry 4.0, Life Cycle Management, Systematic Literature Review},
abstract = {The literature already discussed about how the synergic implementation of Circular Economy (CE) and Industry 4.0 (I4.0) paradigms in industrial contexts could enable improvements in Supply Chain (SC) efficiency and competitiveness. However, the experts concentrated on a single topic of circular supply chain (CSC), CE, and I4.0, lacking a systemic approach to the integrated context. To this aim, the paper proposes a systematic literature review investigating and systematizing the knowledge around the circular SC domain enabled by CE and I4.0 and, simultaneously, highlighting the major trends. Starting from six dimensions of analysis (Type of contribution, LC phases, I4.0 technologies, Triple Bottom Line (TBL) layers, CE strategies, and SC typologies), CSC-related articles have been classified basing on five thematic categories (I4.0 enabling technologies, Performance tools and indicators, Challenges and barriers, Business models and strategies, Best practices). Results allowed to provide some peculiarities of each category. From a sustainability side, they confirmed a predominance of economic and environmental aspects over social ones, together with a life cycle perspective. From an I4.0 side, they shown a prevalence and a synergic implementation of internet of things, big data analytics, and cloud technologies. From a CE strategy side, reuse-recycle-remanufacturing, waste management, material and energy efficiency have been identified as prevalent topics. The paper contributes in building an integrated knowledge of the threefold CE, CSC and I4.0 research context. In addition, it proposes a theoretical framework useful to gradually guide practitioners in approaching the CSC transition. Depending on different CSC aspects, managers could easily raise their knowledge about practices, methodologies, and technological solutions helpful to support their CSC activities.}
}
@article{LIU2024114197,
title = {Developing a goal-driven data integration framework for effective data analytics},
journal = {Decision Support Systems},
volume = {180},
pages = {114197},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114197},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000307},
author = {Dapeng Liu and Victoria Y. Yoon},
keywords = {Data interoperability, Data integration, Data management, Ontology, Open data},
abstract = {Data integration plays a crucial role in business intelligence, aiding decision-makers by consolidating data from heterogeneous sources to provide deep insights into business operations and performance. In the big data era, automated data integration solutions need to process high volumes of disparate data robustly and seamlessly for various analytical needs or operational actions. Existing data integration solutions exhibit limited capabilities for capturing and modeling users' needs to execute on-demand data integration. This study, underpinned by affordance theory and the goal definition principles from the Goal-Question-Metric approach, designs and instantiates a goal-driven data integration framework for data analytics. The proposed innovative design automates data integration for non-technical data users. Specifically, it demonstrates how to elicit and ontologize users' data-analytic goals and addresses semantic heterogeneity, thereby recognizing goal-relevant datasets. In a structured evaluation using the context of counter-terrorism analytics, our design artifact shows promising performance in capturing diverse and dynamic user goals for data analytics and in generating integrated data tailored to these goals. Our research establishes a theoretical framework to guide future scholars and practitioners in building smart, goal-driven data integration.}
}
@article{DEEPU2021200048,
title = {A conceptual framework for supply chain digitalization using integrated systems model approach and DIKW hierarchy},
journal = {Intelligent Systems with Applications},
volume = {10-11},
pages = {200048},
year = {2021},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2021.200048},
url = {https://www.sciencedirect.com/science/article/pii/S2667305321000375},
author = {T S Deepu and V Ravi},
keywords = {Supply chain digitalization, DIKW hierarchy, Decision-making factors, Systems approach},
abstract = {A robust and dynamic supply chain can be developed using advanced digital technologies, enabling real-time information sharing. This study provides a conceptual framework for effective supply chain digitalization by considering the key decision-making factors and interrelationships. The key decision-making factors affecting the digitalization process are identified from the literature and consultation with industry and academia experts. A conceptual framework is developed using the integrated systems model approach and DIKW (Data, Information, Knowledge, and Wisdom) hierarchy. Further, an integrated decision support system is also developed for effective decision-making on digitalization. The factors identified were also classified under three major dimensions like organizational, technological, and innovative dimensions for effective management. The findings of the study were validated based on feedback from experts and a comparison with existing literature. The study contributes to the knowledge of supply chain digitalization by introducing a novel method and approach. The sequential methodology and the framework developed can support the decision-making process and development of appropriate design for digitalization.}
}
@article{MALDONADO2021113493,
title = {Redefining profit metrics for boosting student retention in higher education},
journal = {Decision Support Systems},
volume = {143},
pages = {113493},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113493},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621000038},
author = {Sebastián Maldonado and Jaime Miranda and Diego Olaya and Jonathan Vásquez and Wouter Verbeke},
keywords = {Education, Profit metrics, Student dropout, Student retention, Analytics},
abstract = {Student dropout is a major concern in higher education, as it leads to direct economic losses and substantial social costs. Public and private institutions spend considerable resources to prevent student dropout. The efficiency and effectiveness of these investments, however, may be improved by adopting a profit-driven perspective. In this paper, we propose a novel approach for implementing student dropout prediction using data-driven methods. Extending upon profit metrics as used in business analytics, we design a novel performance measure for evaluating predictive models that is tailored to the student dropout problem and that quantifies the net savings of a retention campaign. This metric supports the identification and selection of students to optimally allocate the limited resources for preventing student dropout and to maximize the resulting savings. Experiments were performed using data from three bachelor's programs of a higher education institution containing information on dropouts and participation in a retention program, i.e., tutorials. The proposed metric allows for a better choice of prediction model and classification threshold than conventional approaches and, as a result, yields tangible savings for the institution. Finally, the presented approach and experimental results highlight pathways to design tailored student retention programs.}
}
@article{MEREDITH2020113402,
title = {Drivers of and barriers to decision support technology use by financial report auditors},
journal = {Decision Support Systems},
volume = {139},
pages = {113402},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113402},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301573},
author = {Kirsty Meredith and Jacqueline Blake and Peter Baxter and Donald Kerr},
keywords = {Decision support, Audit technology, Technology acceptance, Audit support systems, Expert systems, Financial auditing},
abstract = {Effective knowledge management and decision-making are essential for professional service firms. Consequences of poor decision-making are particularly significant in audit firms, where decisions affect the performance of financial markets. International corporate regulators and professional accounting bodies have raised serious concerns about the quality of audit decisions currently being made, suggesting the need for modification of current audit practices. Decision support technologies are underutilized in current audit practices. This study investigates the factors that support or hinder auditors' use of decision support technology. Using a systematic search process and thematic analysis technique, it provides a comprehensive qualitative synthesis of recent empirical studies in this area. Building on the work of prior researchers, it contributes to our understanding of how existing theoretical models regarding technology acceptance apply within the context of the audit profession, and responds to calls for research from the broader information system field. It highlights a need for strong collaboration between decision support system specialists and audit professionals and for the professional accounting bodies to be proactive in facilitating this collaboration.}
}
@article{KAMBLE2021120465,
title = {A machine learning based approach for predicting blockchain adoption in supply Chain},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120465},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120465},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312919},
author = {Sachin S. Kamble and Angappa Gunasekaran and Vikas Kumar and Amine Belhadi and Cyril Foropon},
keywords = {Blockchain, Machine learning, Bayesian network, TAM, TOE, Predictive analytics},
abstract = {The purpose of this paper is to provide a decision support system for managers to predict an organization's probability of successful blockchain adoption using a machine learning technique. The study conceptualizes blockchain technology as a dynamic capability that should be possessed by the organization to remain competitive. The factors influencing the blockchain adoption behavior were modeled using the theoretical lens of the Technology Acceptance Model and Technology-organisation-Environment framework. The findings identify competitor pressure, partner readiness, perceived usefulness, and perceived ease of use as the most influencing factors for blockchain adoption. A predictive decision support system was developed using a Bayesian network analysis featuring the significant factors that can be used by the decision-makers for predicting the probability of blockchain adoption in their organization. The prior probability values reported in the study may be used as indicators by the practitioners to predict their blockchain adoption probability. The practitioner will be required to substitute these probability values (high or low), as applicable to their organization to estimate the adoption probability. The use of the decision support system is likely to help the decision-makers to assess their adoption probability and develop future adoption strategies.}
}
@article{SHRESTHA2020103213,
title = {Development and evaluation of a software-mediated process assessment method for IT service management},
journal = {Information & Management},
volume = {57},
number = {4},
pages = {103213},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103213},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308078},
author = {Anup Shrestha and Aileen Cater-Steel and Mark Toleman and Suren Behari and Mohammad Mehdi Rajaeian},
keywords = {Process assessment, IT service management, Decision support system, Design science research, Task technology fit, Staged maturity model},
abstract = {IT service improvements can add immense value to organisations. To improve IT service management (ITSM) processes, a software-mediated process assessment method is proposed with four phases: process identification, process assessment, process capability measurement and process improvement. The international standard for process assessment was applied to measure process capability. This method was trialled at two Australian organisations and positively evaluated in a US foreign exchange trading business. Our empirical evidence challenges the underlying assumption that higher levels of process capability depend on the achievement of lower level process attributes. We conclude that this method can be applied to transparently self-assess processes.}
}
@article{EBRAHIMI2021103520,
title = {Decisional guidance for detecting discriminatory data analytics recommendations},
journal = {Information & Management},
volume = {58},
number = {7},
pages = {103520},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103520},
url = {https://www.sciencedirect.com/science/article/pii/S037872062100094X},
author = {Sepideh Ebrahimi and Khaled Hassanein},
keywords = {Data analytics, Discrimination, Decisional guidance, Ethical decision-making, Moral intensity, Proximity},
abstract = {The use of data analytics (DA) in organizations has raised several ethical concerns including discrimination. While previous studies have mainly focused on the technical antecedents of the issue, this study takes a novel approach and focuses on the human aspect of decision-making when supported by DA tools. The results of our experimental study confirm that the availability of decisional guidance that sheds light on the demographics of data subjects increases DA users’ moral recognition of the situation and their perceived proximity toward data subjects, which in turn help decrease the likelihood of their approval of discriminatory recommendations.}
}
@article{AUBERT2024800,
title = {Operational Research for, with, and by citizens: An overview},
journal = {European Journal of Operational Research},
volume = {316},
number = {3},
pages = {800-814},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.10.037},
url = {https://www.sciencedirect.com/science/article/pii/S037722172300810X},
author = {Alice H. Aubert and Judit Lienert},
keywords = {Decision support systems, OR in government, Stakeholder participation, e-democracy, Community operational research},
abstract = {Interest in citizen participation is increasing generally. Almost all operational research (OR) is engaged with clients, but it is mainly in the areas of Soft and Community OR that wider stakeholder and citizen participation has been a significant focus. It is the involvement of citizens that is the subject of this paper. We surveyed OR literature and compiled a corpus of 62 studies, the earliest from 1970, to systematically characterize the involvement of citizens in OR processes. Our review produced three findings: First, some fields of OR have embraced citizen participation, but this is not yet a major concern outside the field of Community OR. Second, citizen participation in OR processes is often driven by a moral rationale. Third, progress in information and communication technology (ICT) enables broad participation, but traditional processes requiring physical presence can also be participatory. From these insights, we formulate research opportunities for OR. (1) OR may join Community OR's endeavor to engage with and empower citizens who have so far rarely been involved in OR processes. (2) OR may identify benefits and drawbacks of digital OR processes in empirical studies. (3) OR may determine whether involving large numbers of citizens is suitable for the societal scale. (4) OR may research building and maintaining trust. (5) OR may join efforts for data protection of participants. (6) OR may systematically report and reflect on participatory OR processes. (7) OR should continue researching the fair aggregation of individual inputs. Citizen participation in OR is topical and challenging. Pursuing these research opportunities will contribute to OR fulfilling its mandate of better decision-making in close cooperation with all affected stakeholders.}
}
@article{COUSSEMENT2020113325,
title = {Predicting student dropout in subscription-based online learning environments: The beneficial impact of the logit leaf model},
journal = {Decision Support Systems},
volume = {135},
pages = {113325},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113325},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300804},
author = {Kristof Coussement and Minh Phan and Arno {De Caigny} and Dries F. Benoit and Annelies Raes},
keywords = {Learning analytics, Proactive student management, Subscription-based online learning, Student dropout, Logit leaf model, Machine learning},
abstract = {Online learning has been adopted rapidly by educational institutions and organizations. Despite its many advantages, including 24/7 access, high flexibility, rich content, and low cost, online learning suffers from high dropout rates that hamper pedagogical and economic goal outcomes. Enhanced student dropout prediction tools would help providers proactively detect students at risk of leaving and identify factors that they might address to help students continue their learning experience. Therefore, this study seeks to improve student dropout predictions, with three main contributions. First, it benchmarks a recently proposed logit leaf model (LLM) algorithm against eight other algorithms, using a real-life data set of 10,554 students of a global subscription-based online learning provider. The LLM outperforms all other methods in finding a balance between predictive performance and comprehensibility. Second, a new multilevel informative visualization of the LLM adds novel benefits, relative to a standard LLM visualization. Third, this research specifies the impacts of student demographics; classroom characteristics; and academic, cognitive, and behavioral engagement variables on student dropout. In reviewing LLM segments, these results show that different insights emerge for various student segments with different learning patterns. This notable result can be used to personalize student retention campaigns.}
}
@article{ZAOUI2024101800,
title = {Impact of artificial intelligence on aeronautics: An industry-wide review},
journal = {Journal of Engineering and Technology Management},
volume = {71},
pages = {101800},
year = {2024},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2024.101800},
url = {https://www.sciencedirect.com/science/article/pii/S0923474824000055},
author = {Amina Zaoui and Dieudonné Tchuente and Samuel Fosso Wamba and Bernard Kamsu-Foguem},
keywords = {Artificial Intelligence, Use cases, Aeronautics, Performance, Benefits},
abstract = {Curiously, there are few contributions in the scientific literature on the subject of artificial intelligence (AI) and its impact on aeronautics. However, many communications and reports have been published by aeronautic companies about their applications of AI technologies. This article makes an industry-wide review of AI in aeronautics using a three-step sequential approach: (i) a review of AI and its concepts to define and develop a conceptual map; (ii) a selection of 100 use cases from aeronautics companies that use AI technologies (e.g., Airbus, Boeing, Air France, Safran, EasyJet, Dassault Aviation, Altair); and (iii) an analysis of the use cases using the topics defined in the conceptual map. The main results describe a rising interest in the integration of AI technologies by entities in the aeronautic sector. Moreover, the results from the use cases show that the most recurrent technologies are big data analytics, autonomous intelligent systems, predictive analytics, machine learning, and robotics. Another finding is related to the several benefits that motivate companies to integrate AI technologies into their industrial and operational processes. The most frequent benefits include customer satisfaction, saving time, safety and security, cost reduction, better decision making, solving complex problems, and ensuring optimisation and efficiency. It also appears that the performance of companies is positively impacted by using these AI technologies. Such impacts span all operational departments including marketing, where these technologies help satisfy customer needs; the industrial and operational area, which is provided with quality products; and where productivity and economic performance are optimised for more efficiency.}
}
@article{WASILEFSKY2024114198,
title = {Responsible machine learning for United States Air Force pilot candidate selection},
journal = {Decision Support Systems},
volume = {180},
pages = {114198},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114198},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000319},
author = {Devin Wasilefsky and William N. Caballero and Chancellor Johnstone and Nathan Gaw and Phillip R. Jenkins},
keywords = {Gaussian Bayesian networks, SHAP values, Conformal prediction, Interpretable machine learning, Explainable AI},
abstract = {The United States Air Force (USAF) continues to be plagued by a chronic pilot shortage, one that could be exacerbated by an accompanying shortfall in the commercial airlines. As a result, efforts have increased to alleviate this shortage by finding methods to reduce pilot training attrition. We contribute to these efforts by setting forth a decision support system (DSS) for pilot candidate selection using modern machine learning techniques. In view of the recent Responsible Artificial Intelligence Strategy published by the United States Department of Defense, this research leverages interpretable and explainable machine learning methods to create traceable and equitable models that may be responsibly and reliably governed. These models are used to regress candidates’ average merit assignment selection system scores based on information available for selection and prior to training. More specifically, using data provided by the USAF from 2010 to 2018, this paper develops and analyzes multiple interpretable models based on Gaussian Bayesian networks, as well as multiple black-box models rendered explainable by SHAP values and conformal prediction. A preferred pair of interpretable and explainable models is selected and embedded within a DSS for USAF pilot candidate selection boards: the Air Force Pilot Applicant Selection System. The utilization of this DSS is explored, the analyses it enables are discussed, and relevant USAF policymaking issues are examined.}
}
@article{PARKAVI2024111451,
title = {Enhancing personalized learning with explainable AI: A chaotic particle swarm optimization based decision support system},
journal = {Applied Soft Computing},
volume = {156},
pages = {111451},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111451},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624002254},
author = {R. Parkavi and P. Karthikeyan and A. {Sheik Abdullah}},
keywords = {Educational data mining, Machine learning algorithms, Particle Swarm Optimization, Meta-heuristic based algorithms, Explainable Artificial Intelligence Techniques},
abstract = {In the realm of Educational Technology, personalized learning is pivotal, yet predicting students' learning abilities based on learning styles and ICT remains challenging. We propose a decision support system using Machine Learning (ML), swarm intelligence, and explainable artificial (XAI) techniques to assess students' performance. Our model employs Chaotic Particle Swarm Optimization (C-PSO) with Henon execution, outperforming Genetic Algorithm (GA), Ant Colony Optimization (ACO), Firefly Algorithm (FA), Bee Colony Optimization (BCO), Artificial Fish Swarm Algorithm (AFSA), Mayfly Optimization Algorithm (MFOA), Mother Optimization Algorithm (MOA), Fuzzy Self-Tuning PSO (FST-PSO). Evaluating efficiency, effectiveness, and solution quality reveals C-PSO's superiority. The study identifies the significant impact of ICT on self-progress and employs Spearman Rank correlation for statistical validation. Findings suggest C-PSO as an effective tool for optimizing educational data analysis and decision-making. Further exploration in real-world educational settings and comparative analyses with alternative optimization techniques are recommended for future research.}
}
@article{SPIESKE2023109344,
title = {The future of industry 4.0 and supply chain resilience after the COVID-19 pandemic: Empirical evidence from a Delphi study},
journal = {Computers & Industrial Engineering},
volume = {181},
pages = {109344},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109344},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223003686},
author = {Alexander Spieske and Maximilian Gebhardt and Matthias Kopyto and Hendrik Birkel and Evi Hartmann},
keywords = {Supply chain Resilience, Supply Chain Risk, Industry 4.0, Digital supply chain, COVID-19 pandemic, Delphi study},
abstract = {The COVID-19 pandemic has caused major supply chain disruptions and unveiled the pressing need to improve supply chain resilience (SCRES). Industry 4.0 (I4.0) is a promising lever; however, its future in supply chain risk management (SCRM) is highly uncertain and largely unexplored. This paper aims to evaluate I4.0′s potential to improve SCRES in a post-COVID-19 world. Based on current literature and multiple workshops, 13 future projections on potential I4.0 application areas in SCRM were developed. A two-round Delphi study among 64 SCRM experts with digital expertise was conducted to evaluate and discuss the projections regarding their probability of occurrence until 2030, their impact on SCRES, and their desirability. A fuzzy c-means algorithm was applied to cluster the projections based on the expert assessments. The expert evaluations led to three clusters on I4.0 application in SCRM: Four projections on generating data, increasing visibility, and building digital capabilities received considerable approval and are reliable to improve SCRES in 2030. Four projections enabling data sharing and processing were predominantly supported and demonstrated realization potential for 2030. Finally, five projections that require major supply network adaptations were deemed unlikely to improve SCRES in 2030. This paper answers several research calls by presenting empirical evidence on the pathway of I4.0 implementation in SCRM following the COVID-19 pandemic. Moreover, it evaluates a holistic set of technologies and indicates prioritization potentials to achieve SCRES improvements.}
}
@article{GUENTHER2022251,
title = {AI-Based Failure Management: Value Chain Approach in Commercial Vehicle Industry},
journal = {Procedia CIRP},
volume = {109},
pages = {251-256},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122006941},
author = {Robin Guenther and Sebastian Beckschulte and Martin Wende and Hendrik Mende and Robert H. Schmitt},
keywords = {Artificial intelligence, failure detection, failure management, machine learning, production, value chain},
abstract = {This paper describes an artificial intelligence (AI) based failure management approach across the value chain for the commercial vehicle industry by integrating and utilizing lifecycle data for product and production optimization. The amount of available data throughout a product lifecycle has increased significantly in previous years, primarily driven by the development and deployment of cyber-physical systems. While data from a single entity in the value chain already enables failure management-related analysis and services, including AI-based methods such as predictive maintenance, there remains a lack of systematic approaches to utilize data across the entire value chain. This paper proposes an AI-based failure management approach, which relies on integrating a variety of diverse data sources along the value chain. At first, three so-called application areas were defined: process and product optimization, availability optimization, and performance optimization. Consequently, practice-relevant use cases are identified for each area, for which it is shown how failures in the value chain can be proactively eliminated with the support of AI. Methods for predictive analytics are adapted for cross-value chain failure management to derive correlations between different stages of the production process and product usage. Based on these results and human expert knowledge, proactive measures are recommended by a decision support system (DSS) to resolve failures before arising. The commercial vehicle industry serves as an overarching validation case study for the practice-relevant verification of the targeted applications. The paper gives an outlook on the envisaged research work for the realization of holistic failure management.}
}
@article{VIEIRA2020140,
title = {Supply Chain Risk Management: an Interactive Simulation Model in a Big Data Context},
journal = {Procedia Manufacturing},
volume = {42},
pages = {140-145},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305849},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Industry 4.0},
abstract = {Aligned with the Industry 4.0 research and innovation agenda, a Decision Support System is currently being developed with the purpose of enhancing decision-making in risk scenarios at Supply Chains. It is comprised of a Big Data Warehouse and a simulation model. The former stores and provides integrated real data to the simulation model, which models the respective materials and information flows. Thus, the purpose of this paper is to present such tool being used to test scenarios that, contrarily to the traditional simulation approach, incorporate disruptions in an interactive way, meaning that users may fire such events at any desired simulation time and with different parameters. Thus, the tool is used to assess the impact of disruptions in the performance of the system. The conclusions of this paper highlight the benefits that can be obtained with the proposed interactive approach, as it allows a virtualization of the real system to be obtained and, at the same time, use the simulation model to assess what would be the impact of certain disruptions.}
}
@article{DOHALE2024123243,
title = {An integrated MCDM-ML approach for predicting the carbon neutrality index in manufacturing supply chains},
journal = {Technological Forecasting and Social Change},
volume = {201},
pages = {123243},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123243},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524000398},
author = {Vishwas Dohale and Sachin Kamble and Priya Ambilkar and Stefan Gold and Amine Belhadi},
keywords = {Carbon neutrality, Supply chain, Technology acceptance, Sustainable development goal (SDG), Bayesian network, Voting analytical hierarchy process},
abstract = {Organizations across the globe are devising novel approaches to strive for carbon neutrality. Global institutions have manifested the critical need to develop reasonable strategies in every sector to mitigate the impending issues of excessive anthropogenic carbon emission and, in consequence, climate change. World‑leading economies have initiated significant steps by developing zero‑carbon emission policies to monitor the escalating carbon emissions to curb global warming. The clothing industry has a substantial carbon footprint while causing environmental pollution. Based on transition management theory, this study aims to explore and evaluate the critical determinants that can assist in pursuing carbon neutrality in the clothing industry. A decision support system comprising an integrated voting analytical hierarchy process (VAHP) and Bayesian network (BN) method fulfills our purpose. Pertinent literature is reviewed to determine the critical determinants for carbon neutrality (CDs-CN). After that, the VAHP method is employed to prioritize the CDs-CN. Further, the influence of CDs-CN on achieving carbon neutrality is modeled using a BN, predicting the carbon neutrality index (CNI) for the clothing industry. The findings reveal that professional expertise, laws and certifications, technological acceptance, availability of decarbonizing methods, and adequate carbon offsetting are the essential CDs-CN. This research extends the existing knowledge on integrating MCDM-ML techniques to address predictive modelling-based problems involving complex structures. Simultaneously, the present study helps practitioners and policymakers understand the key CDs-CN to successfully build and manage a carbon-neutral clothing industry by adopting the suggested strategies. Finally, recommendations concerning sustainable development goals (SDGs) are provided to achieve carbon-neutral manufacturing supply chains.}
}
@article{GUTIERREZ2020105826,
title = {LADA: A learning analytics dashboard for academic advising},
journal = {Computers in Human Behavior},
volume = {107},
pages = {105826},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218305909},
author = {Francisco Gutiérrez and Karsten Seipp and Xavier Ochoa and Katherine Chiluiza and Tinne {De Laet} and Katrien Verbert},
keywords = {Learning analytics, Visualization, Academic advising, Academic adviser, Data-driven decision-making},
abstract = {From the perspective of Learning and Educational Technologies, academic advising has been one of the most overlooked aspects of academic support systems, despite being critical for the learning process and final success of students. The majority of higher education institutions provides simple technical support to academic advisers with basic descriptive statistics. This article presents the general design and implementation of a Learning Analytics Dashboard for Advisers (LADA), to support the decision-making process of academic advisers through comparative and predictive analysis. Moreover, this work evaluates the use of this tool to support decision-making of actual advisers in two different higher education institutions (University A, University B), compared with more traditional procedures and tools. Results indicate that LADA enables expert advisers to evaluate significantly more scenarios (Median = 2), especially for high advising difficulty cases with students that failed many courses (MedianA=3,MedianB=2.5), in a not-significantly different amount of time. For inexperienced advisers, LADA is perceived as a valuable tool for more accurate and efficient decision-making, as they were able to make informed decisions in a similar amount of time compared to the experts. These results are encouraging for further developments in the field.}
}
@incollection{KANG2024379,
title = {13 - Cloud, fog, edge computing and 5G technologies for industrial automation},
editor = {Dimitris Mourtzis},
booktitle = {Manufacturing from Industry 4.0 to Industry 5.0},
publisher = {Elsevier},
pages = {379-430},
year = {2024},
isbn = {978-0-443-13924-6},
doi = {https://doi.org/10.1016/B978-0-443-13924-6.00013-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139246000132},
author = {Kai Kang and Chenglin Yu and Ray Y. Zhong},
keywords = {Systems engineering, computing, information systems, computer hardware, high-performance computing, human-centered computing, computing methodology, computer security, computer science, computer security and privacy, software engineering, cyber-physical system, telecommunication engineering, automation engineering, control engineering},
abstract = {Industrial automation is an integral part of the revolution and has been upgraded into a whole new level, which reshapes remanufacturing paradigms. The fourth industrial revolution, known as Industry 4.0, also promises increasing automation and real-time oriented control systems in smart factories by integrating cutting-edge technologies, such as cyber-physical systems, Internet of Things, cloud computing, and artificial intelligence. In the context of Industry 4.0, manufacturing resources are autonomous and automated to control themselves in response to different situations by self-awareness, self-optimization, and self-configuration through vertical and horizontal integration. Therefore industrial automation requires the ability of data exchange, data processing, data analytics, and decision-making. Cloud, fog, and edge computing, and 5G technologies provide high-performance computing power and wireless communication systems to achieve this goal. This chapter is devoted to investigating cloud, fog and edge computing, and 5G technologies for industrial automation in the era of Industry 4.0. Definitions of key terms and concepts underpinning these technologies are presented and followed by a taxonomy of their applications in various aspects after Industry 4.0 was first coined. Discussions and the future outlook regarding these technologies are shown to point out their importance, challenges, and opportunities for realization of industrial automation. Finally, future development and roadmap toward Industry 5.0 are designed to present future research directions, such as human centricity, sustainability, and resilience.}
}
@article{ARSLAN2024110461,
title = {A novel approach for multi-criteria decision making: Extending the WASPAS method using decomposed fuzzy sets},
journal = {Computers & Industrial Engineering},
volume = {196},
pages = {110461},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110461},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224005825},
author = {Özlem Arslan and Selcuk Cebi},
keywords = {WASPAS, Decomposed Fuzzy Sets, DF AHP, DF WASPAS, IF AHP, IF WASPAS},
abstract = {Multi-criteria decision-making (MCDM) methods are crucial for addressing complex real-world problems with multiple conflicting criteria. One of the most common MCDM methods is the Weighted Aggregated Sum Product Assessment (WASPAS) method, which combines the weighted sum (WSM) and weighted product (WPM) models for evaluating and ranking alternatives based on criteria. The WASPAS method provides a reliable and balanced approach by integrating these two methods. In this paper, we propose an innovative extension of the WASPAS method by integrating it with Decomposed Fuzzy Sets (DFS). This integration, Decomposed Fuzzy WASPAS (DF WASPAS), allows for a more comprehensive and precise capture of preferences and opinions of decision-makers, considering both optimistic and pessimistic perspectives. The DF WASPAS method is applied to a case study, where criteria weights are determined using DF AHP, and the proposed method is compared with the IF WASPAS method utilizing IF AHP for criteria weighting. The comparative analysis reveals significant differences in alternative rankings obtained using both WASPAS extensions. The proposed DF WASPAS method enhances decision-making accuracy by incorporating decision-makers’ nuanced perspectives and addressing uncertainties in complex problems.}
}
@article{POSZLER2024123403,
title = {The impact of intelligent decision-support systems on humans' ethical decision-making: A systematic literature review and an integrated framework},
journal = {Technological Forecasting and Social Change},
volume = {204},
pages = {123403},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123403},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524001999},
author = {Franziska Poszler and Benjamin Lange},
keywords = {Moral psychology, Ethical decision-making, Moral enhancement, Intelligent decision-support systems, AI ethics},
abstract = {With the rise and public accessibility of AI-enabled decision-support systems, individuals outsource increasingly more of their decisions, even those that carry ethical dimensions. Considering this trend, scholars have highlighted that uncritical deference to these systems would be problematic and consequently called for investigations of the impact of pertinent technology on humans' ethical decision-making. To this end, this article conducts a systematic review of existing scholarship and derives an integrated framework that demonstrates how intelligent decision-support systems (IDSSs) shape humans' ethical decision-making. In particular, we identify resulting consequences on an individual level (i.e., deliberation enhancement, motivation enhancement, autonomy enhancement and action enhancement) and on a societal level (i.e., moral deskilling, restricted moral progress and moral responsibility gaps). We carve out two distinct methods/operation types (i.e., process-oriented and outcome-oriented navigation) that decision-support systems can deploy and postulate that these determine to what extent the previously stated consequences materialize. Overall, this study holds important theoretical and practical implications by establishing clarity in the conceptions, underlying mechanisms and (directions of) influences that can be expected when using particular IDSSs for ethical decisions.}
}
@article{MALANDRI2024114040,
title = {Model-contrastive explanations through symbolic reasoning},
journal = {Decision Support Systems},
volume = {176},
pages = {114040},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114040},
url = {https://www.sciencedirect.com/science/article/pii/S016792362300115X},
author = {Lorenzo Malandri and Fabio Mercorio and Mario Mezzanzanica and Andrea Seveso},
keywords = {eXplainable AI, Contrastive explanation methods for XAI, Post-hoc explainability, XAI Interpretability},
abstract = {Explaining how two machine learning classification models differ in their behaviour is gaining significance in eXplainable AI, given the increasing diffusion of learning-based decision support systems. Human decision-makers deal with more than one machine learning model in several practical situations. Consequently, the importance of understanding how two machine learning models work beyond their prediction performances is key to understanding their behaviour, differences, and likeness. Some attempts have been made to address these problems, for instance, by explaining text classifiers in a time-contrastive fashion. In this paper, we present MERLIN, a novel eXplainable AI approach that provides contrastive explanations of two machine learning models, introducing the concept of model-contrastive explanations. We propose an encoding that allows MERLIN to work with both text and tabular data and with mixed continuous and discrete features. To show the effectiveness of our approach, we evaluate it on an extensive set of benchmark datasets. MERLIN is also implemented as a python-pip package.}
}
@article{MUNDE2024683,
title = {Predictive Modelling of Customer Sustainable Jewelry Purchases Using Machine Learning Algorithms},
journal = {Procedia Computer Science},
volume = {235},
pages = {683-700},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924007427},
author = {Anjali Munde and Jasmandeep Kaur},
keywords = {Sustainable Jewelry purchase prediction, Machine learning, Supervised classification algorithms, Predictive Modelling, Confusion Matrix, Classification report},
abstract = {In order to give businesses useful information, the main goal of this study is to forecast future consumer purchases in the sustainable jewelry sector. Companies launching new goods, services, or modifications to their current offers must anticipate client behaviours. Businesses can learn a lot about how current customers behave by studying their behaviour in order to develop marketing and sales techniques that work. The study uses a variety of statistical and machine learning methods to forecast the most important elements influencing the purchasing of sustainable jewelry in order to achieve this. These variables span a wide range of characteristics, including gender, age, educational attainment, occupation, gross monthly income, marital status, previous jewelry purchase history, purchase intention, preferred shopping locations, online purchase influencers, consumer buying patterns, preferences for branded and non-branded jewelry, and participation in sustainable jewelry practices. In this study, supervised classification algorithms such Naive Bayes, Support Vector Machine, Linear Discriminant Analysis, K-Nearest Neighbor, and Decision Tree are compared over a wide range of machine learning approaches. To increase the predictive model's effectiveness and resilience, ensemble learning techniques including Random Forest, AdaBoost, and Bagging Classifiers are also introduced. A confusion matrix and classification report are the performance indicators used to assess these models. When used on a test dataset, these measures evaluate the model's precision, recall, accuracy, and F1 scores. These measurable metrics are essential for evaluating the models' reliability and efficacy. Notably, the study shows that the approaches of Random Forest, Decision Tree, Bagging Classifier, and Linear Discriminant Analysis continuously produce the highest accuracy rates, all of which stand at a remarkable 100% accuracy. Other used machine learning classifiers, such as Naive Bayes, K-Nearest Neighbor, Support Vector Machine, and AdaBoost algorithms, which provide significantly lower accuracy scores, fall short of this degree of accuracy. The Random Forest Classifier, which boasts a remarkable accuracy score of 100% for the training dataset and a reasonable 53% for the testing dataset, is particularly noteworthy. This classifier surpasses its competitors, demonstrating its potency in forecasting purchases of sustainable jewelry. In order to forecast sustainable jewelry purchases, this study uses a rigorous methodology that draws on a variety of statistical and machine learning models. The Random Forest Classifier is the most accurate and significant model for this predictive task, with Precision, Recall, and F1 scores of 100, according to the results, which were assessed using multiple performance measures like Precision, Recall, and F1 score.}
}
@article{SHAHANA2023122527,
title = {State of the art in financial statement fraud detection: A systematic review},
journal = {Technological Forecasting and Social Change},
volume = {192},
pages = {122527},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122527},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523002123},
author = {T. Shahana and Vilvanathan Lavanya and Aamir Rashid Bhat},
keywords = {Financial statement fraud, Detection, Systematic review, Bibliometric analysis, Automated tools, PRISMA 2020},
abstract = {Over the past few decades, fraud has been increasingly prevalent, with large businesses like Satyam, Enron, and WorldCom making headlines for their deceptive financial reporting practices. In this research, we conducted a systematic review and bibliometric analysis of the literature concerning fraud detection in financial statements. Following a bibliometric analysis, we identified the leading researchers, publications, sources, countries, and collaboration patterns in financial statement fraud detection. Our systematic review covered the following topics: the data analytics tools used, databases used to identify fraudulent firms, the design of control group samples (non-fraudulent firms), the critical dimension reduction tools used, techniques adopted to address data rarity (imbalanced data), explanatory variables used in the model, theoretical framework supporting the fraud indicators, optimization techniques used, the use of evaluation metrics, and significant findings. The systematic review followed the approach provided by Tranfield et al. (2003), and the bibliometric analysis was conducted using the VOSviewer. Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA), 2020 reporting criteria were followed for reporting the systematic review's findings. We provide a brief overview of the existing literature, drawing both conclusions and recommendations for directions in which additional study is warranted. Our results provide valuable information that can be used by future academics, auditors, enforcement agencies, and regulators as they work to create the most effective fraud detection algorithms possible.}
}
@article{HUANG2024114227,
title = {Strategic team design for sustainable effectiveness: A data-driven analytical perspective and its implications},
journal = {Decision Support Systems},
volume = {181},
pages = {114227},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114227},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000605},
author = {Teng Huang and Qin Su and Chuling Yu and Zheng Zhang and Fei Liu},
keywords = {Team design, Sustainable effectiveness, Data-driven analytics, Decision science, Optimization},
abstract = {Teams are building blocks of organizations and essential inputs of organizational success. This article studies a data-driven analytical approach that exploits the rich data accumulated in organizations in the digital era to design teams, including prescribing team composition and formation decisions. We propose to evaluate a team regarding its performance and temporal stability, referred to as sustainable effectiveness (SE). Our approach estimates the team's performance and stability using machine learning models. It then optimizes an integrated objective of the team's performance and stability through mixed-integer programming models formulated according to predictive models. Consequently, this approach mines meaningful team compositions from historical data and guides strategic team formation accordingly. We conduct empirical studies using authentic data from our partner company in the real estate brokerage industry. The findings reveal that teams that adhere to our model's recommendations achieve an average percentage improvement of 153.1% to 156.5% higher than the benchmark teams, particularly when recruiting one or two members in their actual SE during the post-formation period. We further disclose the mechanism underlying this improvement from the perspective of changes in team compositions. Our study provides a decision support tool for team design and ensuing team dynamic management.}
}
@article{POURBEHZADI2024114308,
title = {Enhanced (cyber) situational awareness: Using interpretable principal component analysis (iPCA) to automate vulnerability severity scoring},
journal = {Decision Support Systems},
volume = {186},
pages = {114308},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114308},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624001416},
author = {Motahareh Pourbehzadi and Giti Javidi and C. Jordan Howell and Eden Kamar and Ehsan Sheybani},
keywords = {Cybersecurity, CVE, CVSS, Machine learning, Situational awareness},
abstract = {The Common Vulnerability Scoring System (CVSS) is widely used in the cybersecurity industry to assess the severity of vulnerabilities. However, manual assessments and human error can lead to delays and inconsistencies. This study employs situational awareness theory to develop an automated decision support system, integrating perception, comprehension, and projection components to enhance effectiveness. Specifically, an interpretable principal component analysis (iPCA) combined with machine learning is utilized to forecast CVSS scores using text descriptions from the Common Vulnerabilities and Exposures (CVE) database. Different forecasting approaches, including traditional machine learning models, Long-Short Term Memory Neural Networks, and Transformer architectures (ChatGPT) are compared to determine the best performance. The results show that iPCA combined with support vector regression achieves a high performance (R2 = 98%) in predicting CVSS scores using CVE text descriptions. The results indicate that the variability, length, and details in the vulnerability description contribute to the performance of the transformer model. These findings are consistent across vulnerability descriptions from six companies between 2017 and 2019. The study's outcomes have the potential to enhance organizations' security posture, improving situational awareness and enabling better managerial decision-making in cybersecurity.}
}
@article{BEREZKIN2024270,
title = {Predictive analytics of scientific and technological trends for decision making in university management},
journal = {Procedia Computer Science},
volume = {234},
pages = {270-277},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924003582},
author = {Dmitry Berezkin and Ilya Kozlov and Polina Martynyuk},
keywords = {Decision support system, Forecasting, Scenario analysis, University management},
abstract = {In this paper we analyze the problem of data-driven university management. We propose a concept of an intelligent strategic decision support system (ISDSS) to ensure that informed decisions are made in the management of higher education institutions. We show that the underlying task of various decision support tasks in university management is the analysis and forecasting of scientific and technological trends. We propose an approach to solving this task which includes determining promising emerging and existing technological areas, forecasting the further development of each of the selected areas, generating possible scenarios of their development and preparing suggestions for decision makers.}
}
@article{BURNAY2024102310,
title = {Business intelligence and cognitive loads: Proposition of a dashboard adoption model},
journal = {Data & Knowledge Engineering},
volume = {152},
pages = {102310},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102310},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2400034X},
author = {Corentin Burnay and Mathieu Lega and Sarah Bouraga},
keywords = {Decision support systems, Information overloads, Dashboards adoption, Decision making, Structural equation modeling},
abstract = {Decision makers in organizations strive to improve the quality of their decisions. One way to improve that process is to objectify the decisions with facts. Data-driven Decision Support Systems (data-driven DSS), and more specifically business intelligence (BI) intend to achieve this. Organizations invest massively in the development of BI data-driven DSS and expect them to be adopted and to effectively support decision makers. This raises many technical and methodological challenges, especially regarding the design of BI dashboards, which can be seen as the visible tip of the BI data-driven DSS iceberg and which play a major role in the adoption of the entire system. In this paper, the dashboard content is investigated as one possible root cause for BI data-driven DSS dashboard adoption or rejection through an early empirical research. More precisely, this work is composed of three parts. In the first part, the concept of cognitive loads is studied in the context of BI dashboards and the informational, the representational and the non-informational loads are introduced. In the second part, the effects of these loads on the adoption of BI dashboards are then studied through an experiment with 167 respondents and a Structural Equation Modeling (SEM) analysis. The result is a Dashboard Adoption Model, enriching the seminal Technology Acceptance Model with new content-oriented variables to support the design of more supportive BI data-driven DSS dashboards. Finally, in the third part, a set of indicators is proposed to help dashboards designers in the monitoring of the loads of their dashboards practically.}
}
@article{HOOGSTRA2024114080,
title = {Developing a contextual model of poverty prediction using data science and analytics – The case of Shelby County},
journal = {Decision Support Systems},
volume = {177},
pages = {114080},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114080},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001550},
author = {Brian Hoogstra and Srikar Velichety and Chen Zhang},
keywords = {Poverty, Vector autoregression, Social network analysis, Ensemble modeling and design science},
abstract = {This study builds on the existing poverty literature and leverages data from disparate sources including both big data sources such as satellite images and traditional data sources such as the federal, state, and local agencies to develop a context-specific poverty prediction model using design science. We examine whether and to what extent infrastructure development as measured from the satellite images as well as spatial spillovers helps predict the poverty rate of a given census tract. We also develop and implement a Vector Autoregression (VAR) based ensemble model that combines predictions from daytime and nighttime imaging with adjacent tracts' poverty rates and other economic and demographic factors identified in prior literature. Our results show that daytime imaging and spatial network features have significant predictive value and that a combination of these features gives the best predictive power. In addition, we find that the skewness of poverty rates among adjacent census tracts, not the average, is a significant predictor showing the importance of distribution of poverty around a region. Our work has major implications for researchers using deep learning and network analysis for policy development and decision making.}
}
@article{TRIEU2024114149,
title = {A computer vision-based concept model to recommend domestic overseas-like travel experiences: A design science study},
journal = {Decision Support Systems},
volume = {181},
pages = {114149},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114149},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623002245},
author = {Van-Hau Trieu and Huy Quan Vu and Marta Indulska and Gang Li},
keywords = {Travel decision making, Recommender systems, Decision support systems, Flickr, Concept modelling},
abstract = {Travel location recommendation systems have long been used by travellers for their ability to suggest destinations and potential travel experiences that match travellers' desires. Recently, a new type of domestic travel has emerged, namely domestic overseas-like travel experiences. These experiences are attractive to travellers who have a preference for exotic locations but no desire to travel internationally. Existing travel recommendation systems were not designed for such applications, nor do they have the relevant ability to recommend domestic overseas-like travel experiences to support travel decision making. To address this challenge, this paper focuses on the development of a recommendation model based on the visual content of photos for domestic overseas-like travel experiences and a prototype application. The application uses the latest advancement in computer vision — the concept model — to learn high-level concepts in an overseas travel destination photo collection to identify similar domestic travel experiences. We demonstrate the usability of the prototype application with a large-scale data set of approximately 479,000 travel photos taken in several countries and evaluate its utility and efficacy through four focus groups with target users.}
}
@article{LI2022109227,
title = {A data-driven decision-making framework for personnel selection based on LGBWM and IFNs},
journal = {Applied Soft Computing},
volume = {126},
pages = {109227},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109227},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622004513},
author = {Jiting Li and Renjie He and Tao Wang},
keywords = {Personnel selection, Data analytics algorithms, Multi-criteria decision-making, Competency model, Decision support system, Best–worst method, Intuitionistic fuzzy numbers},
abstract = {For any organization, personnel selection is regarded as one of the most critical and complex issues in human resource management. Due to the fuzziness and ambiguity in personnel selection, it was usually tackled by utilizing multi-criteria decision-making (MCDM) methods. Previously developed MCDM methods for personnel selection have principally depended on experts’ preferences, which may lead to the bias and deviation caused by human cognitive limitations. With the development of information technology, many human resource (HR) data have accumulated within some organizations, making it possible to discover meaningful patterns in HR data based on data analytics algorithms (DAAs). These patterns do not depend on experts’ judgement and can objectively reflect the performance of personnel in several aspects. Thus, it is promising that integrating objective patterns obtained from HR data and experts’ judgement can make personnel selection more systematic and scientific. To tackle this complex scenario, we propose a data-driven decision-making framework that combines DAAs and MCDM method. The proposed framework can explore the underlying patterns in HR data and also consider experts’ judgement during the selection process. In this framework, a data-driven competency-based method is designed to mine out valuable information in HR data. In addition, for assisting multiple experts, a linear group best–worst​ method (LGBWM) is proposed to calculate the weights of criteria, and intuitionistic fuzzy numbers (IFNs) are used to help experts express their preferences. A personnel evaluation and selection (PLEAS) decision support system is also designed and developed to implement the framework better. In order to illustrate the effectiveness of the proposed framework, a real-world case is conducted in a Chinese state-owned company, and the results demonstrate that our proposed framework is valid for solving data-driven personnel selection problems.}
}
@article{WANG2023102503,
title = {Meta-inventory},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {81},
pages = {102503},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102503},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001855},
author = {S.Y. Wang and George Q. Huang},
keywords = {Industry 4.0, Meta-inventory, Digital / cyber inventory, Cyber-physical system, Internet of Things (IoT), Digital twins, VMI (vendor managed inventory)},
abstract = {In an Industry 4.0 Factory, physical entities such as humans, machines and materials are digitized into digital twins (DT) with smart IoT (Internet of Things) devices resulting in Cyber-Physical Production Systems (CPPS). Real-time data analytics builds up traceability and visibility, not only in the physical domain but also cyber space. This paper adds a new concept of cyber-physical inventory or simply meta-inventory to Industry 4.0 CPPS. In addition to physical items, their digital twins are considered as part of production inventory. Traceability and visibility enabled by digital twin can significantly reduce complexity and uncertainties (e.g. lead times and variability) while achieving resilience in case of major disturbances. The CPPS factory hedges the risks through meta-inventory without incurring cost for holding inventory digitally. After reflecting upon the developments of production inventory management corresponding to the evolutionary history of manufacturing systems to Industry 4.0, the paper presents the meta-inventory paradigm within a simple Industry 4.0 compliant supply chain. The factory, the supplier, and the transport implement a VMI (vendor-managed inventory) strategy. Two well-known basic EOQ (Economic Order Quantity) and EPQ (Economic Production Quantity or production Lot Sizing) problems are extended to demonstrate and quantify the impacts of using meta-inventory on the supply chain and the member enterprise. The analyses allow us to unfold key perspectives in more complex production and supply chain systems for further research.}
}
@article{ZHANG2024110256,
title = {Risk-averse behavior and incentive policies: A new perspective on spatial–temporal traceability supervision in construction logistics supply chains},
journal = {Computers & Industrial Engineering},
volume = {192},
pages = {110256},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110256},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224003772},
author = {Mengdi Zhang and Qiao Shen and Zhiheng Zhao and Shuaian Wang and George Q. Huang},
keywords = {Logistics, Spatial-temporal traceability, Risk-averse behavior, Evolutionary game, Construction supply chain},
abstract = {Spatial-temporal decision support systems (STDSS) serve as a crucial strategy for enhancing operational governance and mitigating risk within construction logistics. However, construction supply chains involve many opaque stakeholders, hindering whole industry compliance monitoring. This research formulates a tripartite evolutionary game model that scrutinizes the strategic interactions among government regulators, carriers, and contractors, thereby offering insights into the collaborative supervision outcomes shaped by these stakeholder engagements. Government regulators choose intelligent supervision incentives versus regular oversight. Carriers decide whether to invest in STDSS or not. Contractors cooperate by enrolling or declining STDSS. As key STDSS investors, carriers’ decisions are investigated under varying risk preferences in an extension model. This examines how government incentives influence long-term intelligent supervision and risk aversion behavior emergence to improve safety and quality across construction supply chains. Our findings indicate that increased adverse events motivate government regulators to adopt STDSS incentives for oversight, though carriers and contractors are not necessarily prompted to implement STDSS themselves. Escalating risk aversion reduces carrier STDSS adoption likelihood as they maintain basic services. Carriers perceiving contractor free riding as unfair competition also demotivates STDSS rollout. Although larger subsidies initially raise STDSS implementation probability, carriers become unwilling to adopt STDSS over time even with greater subsidies. In summary, adverse events drive regulator but not necessarily carrier and contractor STDSS adoption, while risk aversion, perceived fairness and changing subsidy effectiveness over time shape carrier decisions.}
}
@article{ZHONG2024103988,
title = {Firm profiling and competition assessment: A design science approach},
journal = {Information & Management},
volume = {61},
number = {5},
pages = {103988},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.103988},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624000703},
author = {Hao Zhong and Chuanren Liu},
keywords = {Firm profiling, Inter-firm competition assessment, Competitor identification, Heterogeneous network embedding, Design science research, Information system design theory},
abstract = {Extensive efforts have been made by both academics and practitioners to understand the inter-firm competitive relationship owing to its profound impacts on multiple key business goals, e.g., company benchmarking, marketing strategy planning, and talent acquisition. However, it has never been an easy task to fully characterize firms and assess the competitive relationship among them, mainly due to the challenge of information heterogeneity. In this regard, we propose a novel IT artifact for firm profiling and inter-firm competition assessment guided by Information System Design Theory (ISDT). We start by constructing a Heterogeneous Occupation Network (HON) using employees’ occupation details and educational attainments. Then we adopt a Metapath2vec-based heterogeneous network embedding model to learn firms latent profiles (embeddings). Using the firm features and embeddings as input, we train multiple supervised classifiers to assess the competitive relationship among the firms. Following the principles of design as a search process, we demonstrate the effectiveness of our IT artifact through extensive experimental studies and detailed discussions. Our research has also discovered that the occupation and education specifics of employees are key factors in identifying potential competitors of a focal firm.}
}
@article{MOORE2024100578,
title = {A framework for modelling customer invoice payment predictions},
journal = {Machine Learning with Applications},
volume = {17},
pages = {100578},
year = {2024},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2024.100578},
url = {https://www.sciencedirect.com/science/article/pii/S2666827024000549},
author = {Willem Roux Moore and Jan H. {van Vuuren}},
keywords = {Machine learning, Survival analysis, Survival boost, Decision support systems, Invoice payment, Payment prediction},
abstract = {By offering clients attractive credit terms on sales, a company may increase its turnover, but granting credit also incurs the cost of money tied up in accounts receivable (AR), increased administration and a heightened probability of incurring bad debt. The management of credit sales, although eminently important to any business, is often performed manually, which may be time-consuming, expensive and inaccurate. Such an administrative workload becomes increasingly cumbersome as the number of credit sales increases. As a result, a new approach towards proactively identifying invoices from AR accounts that are likely to be paid late, or not at all, has recently been proposed in the literature, with the aim of employing intervention strategies more effectively. Several computational techniques from the credit scoring literature and particularly techniques from the realms of survival analysis or machine learning have been embedded in the aforementioned approach. This body of work is, however, lacking due to the limited guidance provided during the data preparation phase of the model development process and because survival analytic and machine learning techniques have not yet been ensembled. In this paper, we propose a generic framework for modelling invoice payment predictions with the aim of facilitating the process of preparing transaction data for analysis, generating relevant features from past customer behaviours, and selecting and ensembling suitable models for predicting the time to payment associated with invoices. We also introduce a new sequential ensembling approach, called the Survival Boost algorithm. The rationale behind this method is that features generated by a survival analytic model can enhance the efficacy of a machine learning classification algorithm.}
}
@article{AFZAL202418,
title = {Intelligent faculty evaluation and ranking system based on N-framed plithogenic fuzzy hypersoft set and extended NR-TOPSIS},
journal = {Alexandria Engineering Journal},
volume = {109},
pages = {18-28},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.08.071},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824009591},
author = {Usman Afzal and Muhammad Rayees Ahmad and Nazek Alessa and Nauman Raza and Fathea M.O. Birkea and Salem Alkhalaf and Nader Omer},
keywords = {Faculty evaluation, Multi-criteria decision making (MCDM), Plithogenic hypersoft set (PHSS), N-framed PHSS, Extended NR-TOPSIS},
abstract = {This paper proposes an intelligent faculty evaluation and ranking system in a fuzzy environment, with a focus on semester-wise evaluation rather than annual evaluation. This approach is warranted because a university’s academic goals may vary across semesters, affecting the selection and weights of performance indicators related to teaching effectiveness, research output, and official responsibilities. To achieve this objective, the authors introduce the concept of N-framed plithogenic hypersoft set (PHSS), where N represents the number of frames or semesters. Three types of N-framed PHSS are introduced, and an efficient rank reversal-free multi-criteria decision-making technique, namely NR-TOPSIS, is extended by embedding N-framed PHSS in the algorithm of NR-TOPSIS, termed as ENR-TOPSIS. The modified algorithm is capable of extracting data from a source file and producing the desired results. The procedure is implemented for faculty evaluation in a double-framed fuzzy environment, and sensitivity analysis is performed for the proposed ENR-TOPSIS. The developed framework combines intelligent systems that are adaptable and contain additional characteristics, making it more versatile and increasing its accuracy and transparency, in line with SDG-4, which focuses on quality education.}
}
@article{GUPTA2021120986,
title = {Big data and firm marketing performance: Findings from knowledge-based view},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120986},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120986},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004182},
author = {Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen},
keywords = {Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view},
abstract = {A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.}
}
@article{MITSANIS2024108733,
title = {A 3D functional plant modelling framework for agricultural digital twins},
journal = {Computers and Electronics in Agriculture},
volume = {218},
pages = {108733},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.108733},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924001248},
author = {Christos Mitsanis and William Hurst and Bedir Tekinerdogan},
keywords = {Digital Twin, Precision Agriculture, Functional structural plant modelling (FSPM), 3d plant phenotyping},
abstract = {Digital twins are a core industry 4.0 technology enabling the virtual replication of real-world objects, mimicking behaviours and states throughout their lifespan. While digital twins have shown significant benefits in industries such as manufacturing, transportation, and healthcare, their application in agriculture is still within its infancy. Their realisation also poses significant challenges, such as the creation of dynamic agricultural objects (e.g., plants). Existing literature on digital twins in agriculture identifies their limited ability to monitor physical objects without predictive capabilities and that there is a significant lack of 3D representations of plants with functional attributes. Yet, incorporating 3D representations of plants with underlying functionality in a digital twin can greatly improve growth, yield, and disease prediction accuracy. This enhancement enables various applications, such as assessing and developing pruning strategies, providing education to growers, guiding pruning robots, and optimizing spraying techniques. To that end, Functional Structural Plant Modelling presents a potential solution by representing the 3D architecture of plants and incorporating the functionality of different plant parts. By conducting a domain analysis of 3D plant phenotyping and FSPM, this study addresses the specific needs of digital twins in agriculture regarding FSPM. The investigation bridges the existing knowledge gap by identifying crucial concepts, including 3D plant modelling with underlying functionality and 3D plant phenotyping for digital twins. Specifically, a framework for 3D FSPM integration into agricultural digital twins is proposed. The framework not only acknowledges the associated requirements and challenges identified in existing literature but also lays foundation for the advancement of digital twins in the agricultural domain.}
}
@article{KUMAR2022108455,
title = {Applications of the internet of things for optimizing warehousing and logistics operations: A systematic literature review and future research directions},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108455},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108455},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004892},
author = {Devinder Kumar and Rajesh {Kr Singh} and Ruchi Mishra and Samuel {Fosso Wamba}},
keywords = {IoT, Industry 4.0, Logistics and Warehousing, PRISMA approach, TMC framework, Systematic literature review},
abstract = {The introduction of Industry 4.0 technologies such as the Internet of Things (IoT), artificial intelligence (AI), cloud computing, and others has revolutionised the traditional warehousing and logistics industry, resulting in significant changes to various operations and decision-making. Despite the undeniable importance of this topic, the primary research on the impact of IoT technology is inconsistent and scattered. The present study aims to review state-of-art literature on the application of IoT technology in the warehousing and logistics field and suggests a path for the future research through an in-depth analysis of studies done in this area. Sixty-four research articles were carefully selected after a thorough search of the Scopus and EBSCO databases, covering the period from January 2011 to 07th December 2021, to examine the applications of the IoT in the warehousing and logistics business. These articles were thoroughly reviewed and classified in terms of year-wise distribution, major publication outlets, types of study, and highly cited papers to understand the evolution and ongoing trends in this field. The findings reveal that majority of the studies on IoT in the warehousing and logistics domain have been conducted in developed countries. While logistics has been widely investigated, studies on the warehousing domain are limited. Also, there is an under-presentation of various theories in IoT research. The study highlights various gaps by synthesising existing literature and provides a fertile ground for conducting future research in this domain. Supply chain practitioners and researchers will find this review timely and valuable, which offers several valuable implications for them.}
}
@article{DECAIGNY2024114217,
title = {Hybrid black-box classification for customer churn prediction with segmented interpretability analysis},
journal = {Decision Support Systems},
volume = {181},
pages = {114217},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114217},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000502},
author = {Arno {De Caigny} and Koen W. {De Bock} and Sam Verboven},
keywords = {XAI, Customer retention, Hybrid algorithm, Explainable analytics},
abstract = {Customer retention management relies on advanced analytics for decision making. Decision makers in this area require methods that are capable of accurately predicting which customers are likely to churn and that allow to discover drivers of customer churn. As a result, customer churn prediction models are frequently evaluated based on both their predictive performance and their capacity to extract meaningful insights from the models. In this paper, we extend hybrid segmented models for customer churn prediction by incorporating powerful models that can capture non-linearities. To ensure the interpretability of such segmented hybrid models, we introduce a novel model-agnostic approach that extends SHAP. We extensively benchmark the proposed methods on 14 customer churn datasets on their predictive performance. The interpretability aspect of the new model-agnostic approach for interpreting hybrid segmented models is illustrated using a case study. Our contributions to decision making literature are threefold. First, we introduce new hybrid segmented models as powerful tools for decision makers to boost predictive performance. Second, we provide insights in the relative predictive performance by an extensive benchmarking study that compares the new hybrid segmented methods with their base models and existing hybrid models. Third, we propose a model-agnostic tool for segmented hybrid models that provide decision makers with a tool to gain insights for any hybrid segmented model and illustrate it on a case study. Although we focus on customer retention management in this study, this paper is also relevant for decision makers that rely on predictive modeling for other tasks.}
}
@article{ABOSULIMAN2021107555,
title = {Computer vision assisted human computer interaction for logistics management using deep learning},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107555},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107555},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621004997},
author = {Shougi Suliman Abosuliman and Alaa Omran Almagrabi},
keywords = {Deep learning, Human-computer interaction, Long short term memory, Logistics management, Decision making},
abstract = {Human-Computer Interaction is the secret to technological advancement in the area of logistics and supply chain. The key challenges are the degree of energy transferred to devices, like automated vehicles and robotic equipment, and lack of belief in intelligent decision-making, which may overrule the system in the event of misperceptions of automated decisions. This paper presents an efficient Logistics Management Framework Using Deep Learning (eLMF-DL) to implement the computer vision-assisted Human-Computer Interaction (HCI) in the logistic management sector. With a hybrid CNN-LSTM network, eLMF-DL implements a single-stage or one-step convergence optimum decision-support design model that intelligently combines production maximization and demand forecasting. The architecture with the integration of convolutional neural network and long short-term memory network models the machine dynamics and relationships in assorted diverse logistics services demand. To determine uncertainties through dynamic delivery and optimal decisions on allocating logistical service power, the eLMF-DL results in the highest performance.}
}
@article{SONG2022103595,
title = {Can people experience romantic love for artificial intelligence? An empirical study of intelligent assistants},
journal = {Information & Management},
volume = {59},
number = {2},
pages = {103595},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000076},
author = {Xia Song and Bo Xu and Zhenzhen Zhao},
keywords = {Artificial intelligence, Intelligent assistant, Intimacy, Passion, Use},
abstract = {Along with the development of artificial intelligence (AI), more IT applications based on AI are being created. A personal intelligent assistant is an AI application that provides information, education, consulting, or entertainment to users. Due to their high levels of cognitive and emotional capabilities, we assume that users can form humanlike relationships with intelligent assistants, therefore, we develop a research model based on the theory of love. Data were collected from users of intelligent assistants through a survey. The results indicate that users can develop intimacy and passion for an AI application similar to that experienced with human beings. These feelings are related to users’ commitment, promoting the usage of an intelligent assistant, influenced by AI factors (performance efficacy and emotional capability), and moderated by human trust disposition.}
}
@article{NIU2023847,
title = {Sports Training Strategies Based on Data Mining Technology},
journal = {Procedia Computer Science},
volume = {228},
pages = {847-856},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.112},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923019439},
author = {Mengran Niu},
keywords = {Data mining technology, Sports training, Strategy, Improved Apriori algorithm, decision support},
abstract = {According to the demand of sports athletes' scientific training, combined with data mining technology, a set of sports training model which is suitable for China's national conditions and has a higher level of scientific research is established. Starting from the data mining technology, this paper expounds its connotation and several common methods in detail. Based on the demand of sports training, this paper studies the quality control of sports training and the statistical system of tactical combat. On this basis, an improved output method of Aprilio algorithm is introduced to enhance the correctness of systematic evaluation. On this basis, three algorithms, classic, DC-Apriori and modified Apriori, are simulated and compared. The results show that the improved Apriori algorithm studied in this project can provide better decision support for the training model.}
}
@article{ALI2020105442,
title = {The role of decision support systems in smallholder rubber production: Applications, limitations and future directions},
journal = {Computers and Electronics in Agriculture},
volume = {173},
pages = {105442},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105442},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919321441},
author = {Muhammad Fadzli Ali and Ammar Abdul Aziz and Siti Hawa Sulong},
keywords = {Agricultural decision support systems, Model-based decision support systems, Smallholder rubber production},
abstract = {Rubber tree (Hevea brasiliensis) is the primary source of natural rubber, an important raw material for industrial and non-industrial products. Smallholders account for 85% of global rubber production. This highlights the impact of increased volatility in global rubber prices on smallholders’ livelihoods. Decision support systems have been widely applied in agriculture to address complex issues and maintain profitability in crop production. They can facilitate the improvement of production through enhanced farm management practices and the development of effective smallholder-oriented policies. However, little attention has been paid to the potential role of decision support systems can play in smallholder rubber production. Here, we conduct a two-part systematic literature review to identify the factors that influence rubber production and explore how they has been incorporated in the design of existing decision support systems. We discuss the current role of decision support systems in improving smallholder rubber production and identify the limitations and opportunities for future improvements. Despite being a smallholder crop, there is a paucity of research on the influence of social and institutional factors on rubber production. To date, decision support systems in rubber production have been focused on predicting growth and yield, assessing profitability and viability, evaluating alternative farm management practices under uncertainties, and formulating policies for sustainable rubber production. We identified three main limitations: (1) the extensive data requirement; (2) the lack of incorporation of social and institutional factors; and (3) the difficulty of communicating model results to smallholders. We developed a framework and suggest improvement strategies for the development of improved decision support systems in rubber production. This framework aims to enhance the decision-making process by the key stakeholders to improve smallholder rubber production.}
}
@article{MISHRA2024108508,
title = {An integrated picture fuzzy standard deviation and pivot pairwise assessment method for assessing the drivers of digital transformation in higher education institutions},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108508},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108508},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624006663},
author = {Arunodaya Raj Mishra and Pratibha Rani and Dragan Pamucar and Adel Fahad Alrasheedi and Vladimir Simic},
keywords = {Digital transformation, Picture fuzzy sets, Higher education institutions, DSS, PIPRECIA},
abstract = {Information and communication technologies play a significant role in all aspects of modern society. It has become a subject of high significance in all perspectives, mainly in the workstation. Consequently, the main objective for universities and schools is to develop future specialists for handling the problems and finding the effective outcomes by their digital competency as a significant ability. In higher education institutions, assessing the key drivers of digital transformation is a complicated decision-making problem as the multiple factors are involved in it. To this aim, a novel picture fuzzy information-based decision-making framework is proposed to assess the key drivers of digital transformation in higher education institutions. In this regard, an improved score function is proposed to rank the picture fuzzy numbers. In addition, it is used to determine the weights of decision experts. Next, some generalized Dombi aggregation operators are proposed to aggregate the picture fuzzy information. Further, an integrated criteria weighting model is presented based on the combination of objective weights through standard deviation-based model and subjective weights through pivot pairwise relative criteria importance assessment (PIPRECIA) model in the context of picture fuzzy sets. To rank the alternatives, a picture fuzzy extension of alternative ranking order model accounting for two step normalization (AROMAN) approach is proposed using the score function, Dombi operators and integrated weighting model. The usefulness of the proposed approach is illustrated using a case study of key drivers' assessment of digital transformation in higher education institutions. Sensitivity and comparative analyses are discussed to reveal the consistency, robustness and efficiency of introduced model. This study offers a new decision-making framework, which makes a significant contribution to the key drivers’ assessment process from uncertainty perspective.}
}
@article{CHAI2023574,
title = {The Process and Algorithm Analysis of Text Mining System Based on Artificial Intelligence},
journal = {Procedia Computer Science},
volume = {228},
pages = {574-581},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018902},
author = {Xiaoliang Chai and Songxiao Xu and Shilin Li and Junyu Zhao},
keywords = {Artificial Intelligence, Text Mining, Mining System, Implementation Process},
abstract = {The rapid development of the Internet leads to the rapid growth of network information, we call it information explosion. The Internet is full of information, and it is difficult for users to find this information and useful knowledge of the ocean. The Web has become the world's largest information repository, and there is an urgent need for efficient access to the valuable knowledge of vast amounts of web information. The purpose of this paper is to study the process and algorithm analysis of text mining system based on artificial intelligence. This paper presents an algorithm of document feature acquisition based on genetic algorithm. Selecting suitable features is an important task in specific text classification and information retrieval. Finding appropriate feature vectors to represent the text will undoubtedly help with subsequent sorting and grouping. Based on the genetic algorithm of variable length chromosome, this paper improves the crossover, mutation and selection operations, and proposes an algorithm to obtain text feature vectors. This method has a wide range of applications and good results.}
}
@article{KOSURU2023100778,
title = {An intelligent energy minimization algorithm with virtual machine consolidation for sensor-based decision support system},
journal = {Measurement: Sensors},
volume = {27},
pages = {100778},
year = {2023},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2023.100778},
url = {https://www.sciencedirect.com/science/article/pii/S2665917423001149},
author = {Sivarama Krishna Kosuru and Divya Midhunchakkaravarthy and Mohammed Ali Hussain},
keywords = {Artificial intelligence, Cloud computing, Data analytics, Intelligent system, Sensor processing, Service level agreement virtual machine},
abstract = {Cloud computing has many benefits for businesses because of its distinctive qualities, including scalability, flexibility, on-demand service, and security. A good task scheduler is required to increase the efficiency of a cloud system, which performs numerous jobs simultaneously. On-demand access to resources that have been virtualized is made available as a service without the need for further waiting. For task scheduling issues based on a makespan limitation, energy consumption is decreased, significantly reducing energy cost. Additionally, the complexity of scheduling issues has increased primarily due to the application's lack of a makespan constraint. Unfortunately, reducing the energy cloud services use presents special research issues and difficulties. More precisely, because of the diversity of servers found in cloud centers, it is challenging to choose the best servers for cloud-based decision support systems to reduce energy usage. The presented approach is innovative and could be applied for complex applications while maintaining an average energy consumption for the running resources, which is a big challenge. The current process finds an optimized energy minimization in the cloud with a combination of Virtual Machine consolidation. The outputs were considered in terms of Energy, Virtual Machine Migration, Performance Degradation, Aggregated Ideal Time Factor, and Aggregated Overload Time Fraction. The obtained results in terms of the existing state of art approaches are far better than the competing approaches.}
}
@article{ZABALALOPEZ2024,
title = {A survey of data-centric technologies supporting decision-making before deploying military assets},
journal = {Defence Technology},
year = {2024},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2024.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S221491472400182X},
author = {Alexandra Zabala-López and Mario Linares-Vásquez and Sonia Haiduc and Yezid Donoso},
keywords = {Data-centric technologies, Military, Analytics, Machine learning, Data science, Artificial intelligence},
abstract = {In a time characterized by the availability of vast amounts of data, the effective utilization of information is critical for timely decision-making in military operations. However, processing large amounts of data requires computational resources and time. Therefore, decision makers have used data-centric technologies to take advantage of public and private data sources to support military operations. This survey explores the integration and application of data-centric technologies, such as data analytics, data science, and machine learning, to optimize decision-making workflows within military contexts supporting the deployment of military assets and resources. To address the information gap, this article presents a literature review, specifically a survey. Our survey examines the use of the mentioned technologies to process and analyze information that contributes to the phases of situational awareness, and planning in military environments. We then introduce a taxonomy of the approaches associated with implementing these technologies in military scenarios. Furthermore, we discuss relevant factors for the seamless integration of data-centric technologies into military decision-making processes, and reveal the importance of specialized personnel, architectures, and cybersecurity issues in the task of developing prototypes and models. The findings of this paper aim to provide valuable insights for military institutions, offering a deeper understanding of the use of data-centric technologies as innovative practices to enhance the effectiveness of military decision-making.}
}
@article{HUANG2024123269,
title = {Diagnosis with incomplete multi-view data: A variational deep financial distress prediction method},
journal = {Technological Forecasting and Social Change},
volume = {201},
pages = {123269},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123269},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524000659},
author = {Yating Huang and Zhao Wang and Cuiqing Jiang},
keywords = {Financial distress prediction, Incomplete multi-view data, Deep learning, Variational inference, View fusion},
abstract = {The proliferating of big data from both financial and non-financial aspects, has been flourishing multi-view-data-based financial distress prediction. However, when various data views, e.g., report texts, forum posts, and legal judgments, are jointly utilized, modeling challenges, such as heterogeneities in distribution and completeness among data views, may be inevitably raised. To this end, we propose a variational deep financial distress prediction method (VDFDP). The proposed method consists of three modules: a view-specific encoder module to learn a latent representation for each view, a view fusion module to learn a joint representation by transferring knowledge from all views considering different degrees of completeness, and a financial distress decoder module to map joint representation to financial distress status. Empirical evaluation using Chinese listed company data shows that VDFDP significantly outperformed all benchmarked financial distress prediction methods. It can more effectively leverage incomplete multi-view data and more accurately predict financial distress. Our study also provides valuable insights and practical implications for stakeholders, such as investors and companies themselves, to effectively identify risk signals and make risk management decisions.}
}
@article{SCHAEFER2021156,
title = {Framework of Data Analytics and Integrating Knowledge Management},
journal = {International Journal of Intelligent Networks},
volume = {2},
pages = {156-165},
year = {2021},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666603021000208},
author = {Camilla Schaefer and Ana Makatsaria},
keywords = {Data analytics, Knowledge management, Big data, Business intelligence, Data discovery},
abstract = {Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users’ process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.}
}
@article{PAWAR20243075,
title = {Improving Student Performance Prediction Through Feature Selection: Insights from ‘Offee’ Assessment Data during the Covid-19 Pandemic},
journal = {Procedia Computer Science},
volume = {235},
pages = {3075-3084},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.291},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924009712},
author = {Mousmi Pawar and Jyotshna Dongardive},
keywords = {Prediction, Feature Selection, Support Vector Machine, Decision Tree},
abstract = {Post-pandemic student performance prediction has gained considerable attention within the academic community. Educational institutions worldwide have adopted blended learning approaches for delivering education and assessments. In this study, we harnessed data from the ‘Offee’ assessment platform based in Mumbai, India, spanning from September 2020 to October 2022. This dataset allows us to analyze the real impact of the Covid-19 pandemic on examinations. To tackle the challenges posed by high-dimensional data, we employed feature selection techniques to retain only the most significant features for predicting student performance. Specifically, we assessed the effectiveness of two decision tree-based methods, J48 classifier and Random Forest, in conjunction with the ranker search method using wrapper subset evaluation. The resulting feature set was then utilized to train and test a Support Vector Machine model for predicting student performance. Our findings reveal that both J48 and Random Forest feature selection methods yield impressive precision accuracy rates for student performance prediction, achieving 93.14% and 86.24%, respectively. In contrast, using the dataset without feature selection resulted in lower precision accuracy, standing at 79.9%, and consumed more time during model training and development. In summary, this study underscores the significance of feature selection in enhancing the accuracy of student performance prediction compared to utilizing the dataset without such feature curation.}
}
@article{AWAN2021120766,
title = {Big data analytics capability and decision-making: The role of data-driven insight on circular economy performance},
journal = {Technological Forecasting and Social Change},
volume = {168},
pages = {120766},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120766},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521001980},
author = {Usama Awan and Saqib Shamim and Zaheer Khan and Najam Ul Zia and Syed Muhammad Shariq and Muhammad Naveed Khan},
keywords = {Big data analytics, Data-driven insights, Big data analytics capabilities, Decision-making, Circular economy, Manufacturing firms},
abstract = {Big data analytics (BDA) is a revolutionary approach for sound decision-making in organizations that can lead to remarkable changes in transforming and supporting the circular economy (CE). However, extant literature on BDA capability has paid limited attention to understanding the enabling role of data-driven insights for supporting decision-making and, consequently, enhancing CE performance. We argue that firms drive decision-making quality through data-driven insights, business intelligence and analytics (BI&A), and BDA capability. In this study, we empirically investigated the association of BDA capability with CE performance and examined the mediating role of data-driven insights in the relationship between BDA capability and decision-making. Data were collected from 109 Czech manufacturing firms, and partial least squares structural equation modeling was applied to analyze the data. The results reveal that BDA capability and BI&A are positively associated with decision-making quality. This effect is stronger when the manufacturer utilizes data-driven insights. The results demonstrate that BDA capability drives decision-making quality in organizations, and data-driven insights do not mediate this relationship. BI&A is associated with decision-making quality through data-driven insights. These findings offer important insights to managers, as they can act as a reference point for developing data-driven insights with the CE paradigm in organizations.}
}
@article{MUMALI2022107964,
title = {Artificial neural network-based decision support systems in manufacturing processes: A systematic literature review},
journal = {Computers & Industrial Engineering},
volume = {165},
pages = {107964},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.107964},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222000341},
author = {Fredrick Mumali},
keywords = {Decision support systems, Intelligent decision support, Artificial neural networks, Manufacturing, Systematic literature review},
abstract = {The use of artificial neural network models to enrich the analytical and predictive capabilities of decision support systems in manufacturing has increased. The growing complexity and uncertainty in the manufacturing sector demand improved decision-making to ensure low operations costs, high productivity, and sustainable use of resources. Artificial neural networks have the inherent capacity to analyze the most uncertain and complex patterns in unstructured decision problems. This review aims to synthesize and provide a comprehensive summary of recent studies on artificial neural network-based decision support systems as applied in manufacturing processes. First, the specific processes in manufacturing where artificial neural network-based decision support systems are used are analyzed. A total of 99 multi-disciplinary publications on artificial neural network-based decision support systems published between 2011 and 2021 are retrieved and processed following a rigorous execution of the designated acceptance criteria and quality assessment. A review of the selected studies indicates a growing interest in applying artificial neural networks in decision support systems. Product and process design, performance evaluation, and predictive maintenance are the main application areas identified. A growing tendency to combine artificial neural network models with other intelligent tools, notably fuzzy logic, and genetic algorithm, is noted to overcome drawbacks such as slow convergence when training the algorithms. Further research should extend to other tools for enriching the performance of artificial neural networks in manufacturing processes.}
}
@incollection{MEKIC202497,
title = {Chapter 5 - Data-driven educational decision-making model for curriculum optimization},
editor = {Santi Caballé and Joan Casas-Roma and Jordi Conesa},
booktitle = {Ethics in Online AI-based Systems},
publisher = {Academic Press},
pages = {97-118},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-18851-0},
doi = {https://doi.org/10.1016/B978-0-443-18851-0.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443188510000020},
author = {Edis Mekić and Irfan Fetahović and Kristijan Kuk and Brankica Popović and Petar Čisar},
keywords = {Machine learning, artificial intelligence, big data, curriculum, ethical optimization},
abstract = {This chapter explores the implementation of machine learning, artificial intelligence (AI), and big-data analyses in the decision-making model of curriculum optimization. Preparing the curriculum is only a small part of the overall process of planning and implementing the education cycle. Data obtained and analyzed with machine learning techniques from archives can provide insight into the past efficiency of teaching, help us to predict future trends, and change the curriculum according to the expected outcomes. However, implementation of these tools is limited due to the number of ethical challenges, such as security, possible stereotype-based decisions, and generalizations. In this chapter, we should explain the cycles of education planning and implementation, as well as the appropriate machine learning techniques used to support decisions and the ethical consequences of doing so.}
}
@article{MARINO2020106392,
title = {A microgrid energy management system based on chance-constrained stochastic optimization and big data analytics},
journal = {Computers & Industrial Engineering},
volume = {143},
pages = {106392},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106392},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220301261},
author = {Carlos Antonio Marino and Mohammad Marufuzzaman},
keywords = {Sustainability, Microgrid, Big data, Spark streaming, Stochastic optimization, Wind power},
abstract = {A Microgrid (MG) is a promising distributed technology to solve todays energy challenges. They are changing how electricity is produced, transmitted, and distributed, enabling to capture massive amounts of data from sensors, and other electrical infrastructures. However, recent advances in modeling and optimization of MG neither integrate the use of big data technologies aggressively nor focus on developing an optimal operational strategy for a single building. To bridge this gap, this research proposes to use Apache Spark to enhance the performance of a scalable stochastic optimization model for an MG for multiple buildings, and to ensure that a significant portion of the wind power output will be utilized. The decision model is formulated as a chance constraint two-stage optimization problem to obtain operation decisions for a behind-the-meter topology. The comparison between the current practice of using historical data and integrating Apache Spark technologies demonstrates the superiority of the streaming data as energy management strategy. Experiments under different settings show that using big data strategy, the model can (1) achieve more cost savings of the total system, (2) increase resiliency to power disturbances, and (3) build a data analytics framework to enhance the decision-making process.}
}
@article{CHAI202011,
title = {A multi-source heterogeneous data analytic method for future price fluctuation prediction},
journal = {Neurocomputing},
volume = {418},
pages = {11-20},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.07.073},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220311875},
author = {Lei Chai and Hongfeng Xu and Zhiming Luo and Shaozi Li},
keywords = {Futures price movement prediction, Heterogeneous multiple source information, Features extraction, Relation map, Multivariate Gaussian mixtures, Hidden Markov Model},
abstract = {Most previous works on future market price forecasting only utilize the historical transaction data, while ignoring many other valuable factors. Recently, many research works propose multiple-source data-based predicting approaches in the stock market. Although the futures market and the stock market are very similar, the futures market still has its uniqueness. Most importantly, the subject matter of futures is usually commodity entities with prominent competing products or upstream, downstream industries, which can significantly influence the price. Therefore, it is essential to propose a future specific analysis framework by considering different factors. In this study, we constructed a Multi-source Heterogeneous Data Analysis (MHDA) method for future price prediction by integrating multiple-source information, i.e., trading data, news event data, and investor comments. Firstly, we first constructed a relation map to capture all related news events from upstream and downstream commodities and then built a future-specific sentiment dictionary to accurately quantify the sentiment impact of related news and investor comments during the feature extraction. Finally, we model the quantified multi-source heterogeneous information by an extended Hidden Markov Model to capture the underlying temporal dependency in the data. Evaluations on the data of palm oil futures from 2016.9 to 2017.9 show the effectiveness of our proposed framework.}
}
@article{IULAMANOVA202185,
title = {Decision Support in the Automated Compilation of Individual Training Module Based on the Emotional State of Students},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {85-90},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.424},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321018619},
author = {Adelina Iulamanova and Diana Bogdanova and Vitaliy Kotelnikov},
keywords = {affective computing, student emotional state, distance learning, fuzzy decision trees, Nearest Neighbors algorithm},
abstract = {Distance education has become very relevant in the context of the global coronavirus pandemic. There is an acute issue of increasing the efficiency of this process. Students face problems of self-organization and lack of motivation to learn. Considering their emotional state and individual characteristics let create a system that adapts to each student and make education more individual. In this work, the problem of decision support is described in the automated compilation of an individual training module in distance education based on considering the emotional states of students. The work formulates the existing problems of distance education. An overview of existing research in the field of accounting and recognition of emotions in education is given. The problem of managing the process of distance learning, considering the emotional states of students and their individual characteristics, is given. It is proposed to introduce emotional support in the learning process and select the form of presentation of the material adequate to the student’s state. The results of an experiment on 80 students with different personality types are presented. The analysis of the results showed that the emotional state of the students, who used the developed decision support system in teaching, became much better. After training, the level of knowledge of students, who used the developed decision support system, became higher.}
}
@article{ERAZORAMIREZ2022105525,
title = {HydroLang: An open-source web-based programming framework for hydrological sciences},
journal = {Environmental Modelling & Software},
volume = {157},
pages = {105525},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105525},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002250},
author = {Carlos {Erazo Ramirez} and Yusuf Sermet and Frank Molkenthin and Ibrahim Demir},
keywords = {Scientific visualization, Hydrological analysis, Software libraries, Web frameworks, Neural networks},
abstract = {This paper introduces HydroLang, an open-source and integrated community-driven computational web framework for hydrology and water resources research and education. HydroLang employs client-side web technologies and standards to carry out various routines aimed at acquiring, managing, transforming, analyzing, and visualizing hydrological datasets. HydroLang consists of four major high-cohesion low-coupling modules: (1) retrieving, manipulating, and transforming raw hydrological data, (2) statistical operations, hydrological analysis, and model creation, (3) generating graphical and tabular data representations, and (4) mapping and geospatial data visualization. To demonstrate the framework's capabilities, portability, and interoperability, two detailed case studies (assessment of lumped models and construction of a rainfall disaggregation model) have been presented. HydroLang's unique modular architecture and open-source nature allow it to be easily tailored into any use case and web framework, and it encourages iterative enhancements with community involvement to establish the comprehensive next-generation hydrological software toolkit.}
}
@article{JABBARI2022113671,
title = {A collaborative decision support system for multi-criteria automatic clustering},
journal = {Decision Support Systems},
volume = {153},
pages = {113671},
year = {2022},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113671},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621001810},
author = {Mona Jabbari and Shaya Sheikh and Meysam Rabiee and Asil Oztekin},
keywords = {Automatic clustering, Evolutionary algorithms, Multi-objective, DEA, BWM},
abstract = {Automatic clustering is a challenging problem, especially when the decision-maker has little or no information about the nature of the dataset and the criteria of interest. There is a lack of generalizability in the current validity indexes (VI) for automatic clustering algorithms, as each considers a limited number of objectives and mostly ignores the other aspects of clustering validation. The proposed framework benefits from collaboration among selected evolutionary algorithms. A mixed-integer non-linear programming model is developed, and a framework is proposed for a six-step decision support system to solve it. The decision-maker (DM) selects the quantitative (primary) VIs and the evolutionary algorithms. Given DM's knowledge on the dataset and VIs, DM can incorporate qualitative (secondary) VIs. DM determines the quality threshold for each VI and runs the evolutionary algorithms separately. The DSS then saves the best obtained value of VIs in order to prepare the input necessary to construct the aggregated function. Based on the selected primary VIs, a new normalized aggregated function is developed and solved repeatedly using the randomly selected or predefined weights of importance. Eventually, DM employs a proper DEA model to define the final clustering output among all possible solutions. Given multiple efficient solutions, the best-worst method and a multi-criteria decision-making approach are applied to find the final output. The applicability of the proposed approach is illustrated on a synthetic and two secondary datasets, and the result at each step is discussed in detail.}
}
@article{IRFAN2024112180,
title = {Modeling barriers to the adoption of metaverse in the construction industry: An application of fuzzy-DEMATEL approach},
journal = {Applied Soft Computing},
volume = {167},
pages = {112180},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112180},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624009542},
author = {Muhammad Irfan and Abishek Rauniyar and Jin Hu and Atul Kumar Singh and Sathvik Sharath Chandra},
keywords = {Metaverse, Barriers, Fuzzy-DEMATEL, Construction industry},
abstract = {The idea of the metaverse has been rather popular in many different sectors since it provides immersive virtual reality where users may connect and participate. Leveraging metaverse technologies—such as virtual reality and augmented reality—for project visualization, collaborative design, and training simulations—in the building industry is attracting increasing attention. Notwithstanding its possible advantages, there is a clear knowledge vacuum on the obstacles preventing the general acceptance of metaverse technology in building. The study intends to close this gap by means of a literature review and analysis of the identified barriers using the fuzzy-DEMATEL technique, therefore separating the causal links among them. There were 26 obstacles found in the literature review and expert comments, arranged technically, organizationally, environmentally, socially, and economically. The most important obstacles shown by results are Security Concerns, Resistance to Change, Lack of Expertise, Siloed Departments, and Training and Education Needs. The results of this research provide building companies and legislators with important new perspectives and direction in developing plans to remove obstacles and encourage the use of metaverse technology. Moreover, the findings of the study provide a road map for industry players in tackling the important issue of restricted data capacities, thereby enabling a better and more successful integration of metaverse technology into building methods.}
}
@article{BRUZZONE2020548,
title = {Enabling Strategic Decisions for the Industry of Tomorrow},
journal = {Procedia Manufacturing},
volume = {42},
pages = {548-553},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305710},
author = {Agostino Bruzzone and Marina Massei and Kirill Sinelshnkov},
keywords = {Strategic Engineering, Decision Making, Industrial Plants, Production, Simulation, Artificial Intelligence, Data Analytics},
abstract = {The paper proposes an innovative approach related to the use of Strategic Engineering, new discipline combining Simulation, AI (Artificial Intelligence) & Data Analytics, to support decision making in Industries thanks to the support of the paradigm Industry 4.0. The paper proposes demonstrations of this approach effectiveness through examples and case studies related to different industrial sectors, including Industrial Plant Engineering, Iron & Steel, Oil & Glass and Hollow Glass.}
}
@article{ALKASASBEH2021101959,
title = {An integrated decision support system for building asset management based on BIM and Work Breakdown Structure},
journal = {Journal of Building Engineering},
volume = {34},
pages = {101959},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2020.101959},
url = {https://www.sciencedirect.com/science/article/pii/S2352710220335919},
author = {Maha Al-Kasasbeh and Osama Abudayyeh and Hexu Liu},
keywords = {Asset management, Building information modeling, Database management system, WBS, Life cycle},
abstract = {The asset management phase of a building constitutes approximately 60% of its total lifecycle costs. However, significant unnecessary expenses occur in current asset management practices because of the separation of the design and construction phases from operation and maintenance and the lack of an effective building asset management system covering all types of buildings with numerous components. Asset management decision-making is inherently a process that requires the assimilation of a multitude of data, processes, and software systems. Therefore, this paper proposes an integrated decision support system for building asset management that addresses the systematization and coordination of lifecycle data. In this approach, a lifecycle work breakdown structure (WBS) for the asset management system is developed to provide a unified hierarchy to categorize and organize building assets. It is then used to develop the WBS-based integrated building lifecycle asset management model through a relational database management system (DBMS). Since building information modelling (BIM) is a rich information-based platform with large volumes of asset data needed, it is integrated with the WBS-based DBMS to achieve an effective integrated building asset management. A case study of an educational building is presented and discussed in the paper to demonstrate how the proposed automated solution is used for asset management. The research contributes to the body of knowledge by formalizing a WBS-based unified asset inventory hierarchy and mapping BIM data to the proposed hierarchy for effective asset management.}
}
@article{BERNOVSCHI2024719,
title = {Mitigating Bias in Aesthetic Quality Control Tasks: An Adversarial Learning Approach},
journal = {Procedia Computer Science},
volume = {232},
pages = {719-725},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924000711},
author = {Denis Bernovschi and Alex Giacomini and Riccardo Rosati and Luca Romeo},
keywords = {Adversarial learning, Bias mitigation, Aesthetic quality control, Industry 4.0},
abstract = {Aesthetic quality control (AQC) is an essential step in smart factories to ensure that product quality meets the desired standards. This operation includes assessing factors such as color, texture, and shape. In the context of AQC, bias can arise when the criteria used to evaluate the aesthetics of a product are subjective and influenced by personal preferences. Bias can also occur due to the background or other objective factors like the geometry of the material. This work will focus on applying an adversarial learning strategy to a pre-trained DL architecture for improving the generalization performance of a predictive model tailored explicitly for solving AQC task classification. Experimental results on a benchmark AQC dataset highlighted the robustness of the proposed methodology for learning only relevant components related to quality classes rather than other confusing traits, enabling the mitigation of the identified bias.}
}
@article{SERMET2022105010,
title = {GeospatialVR: A web-based virtual reality framework for collaborative environmental simulations},
journal = {Computers & Geosciences},
volume = {159},
pages = {105010},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.105010},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002934},
author = {Yusuf Sermet and Ibrahim Demir},
keywords = {Virtual reality, Geospatial visualization, Disaster management, Decision-support systems, Web-based interaction},
abstract = {This research introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be served on any web platform and accessed via desktop and mobile devices and virtual reality headsets. The framework can generate realistic simulations of desired locations entailing the terrain, elevation model, infrastructures, dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, occupancy, traffic, weather). These layers enable in-situ visualization of useful data to aid public, scientists, officials, and decision-makers in acquiring a bird's eye view of the current, historical, or forecasted condition of a community. The framework incorporates multiuser support to allow different stakeholders to remotely work on the same VR environment and observe other users' actions and 3D positions via avatars in real-time, and thus, presenting the potential to be utilized as a virtual incident command center or a meeting room. GeospatialVR's purpose is to enhance existing web-based cyberinfrastructure systems with the integration of immersive geospatial capabilities to assist the development of next-generation information and decision support systems powered by virtual reality. Finally, several case studies have been developed for flooding, wildfire, transportation, and public safety.}
}
@article{LIN2024103924,
title = {How does artificial intelligence affect the environmental performance of organizations? The role of green innovation and green culture},
journal = {Information & Management},
volume = {61},
number = {2},
pages = {103924},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.103924},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624000065},
author = {Jiabao Lin and Yanyun Zeng and Shaowu Wu and Xin (Robert) Luo},
keywords = {Artificial intelligence use, Green innovation, Green culture, Environmental performance, Agribusiness},
abstract = {Despite the importance of artificial intelligence (AI) in improving environmental performance, the mechanisms and boundary conditions through AI use affects environmental performance remain unclear. Using a sample of Chinese agricultural firms, the empirical results verify the positive impact of AI use on environmental performance via green innovation of product and process. Moreover, our findings reveal that green culture positively moderates the impact of AI use on green product innovation, and it strengthens the positive mediation effect of green product innovation on the relationship between AI use and environmental performance. This study provides insights and enriches the existing understanding of AI-enable green innovation, thus making practical implications for firms to achieve sustainable development.}
}
@article{MICUS2023100277,
title = {Methods to analyze customer usage data in a product decision process:A systematic literature review},
journal = {Operations Research Perspectives},
volume = {10},
pages = {100277},
year = {2023},
issn = {2214-7160},
doi = {https://doi.org/10.1016/j.orp.2023.100277},
url = {https://www.sciencedirect.com/science/article/pii/S221471602300012X},
author = {Christian Micus and Simon Schramm and Markus Boehm and Helmut Krcmar},
keywords = {Customer usage data, Product development, Customer behavior, Big data, Data analytics, Product decision process},
abstract = {To remain competitive, companies must decide on new, desirable products. This can be achieved by integrating insights how customers use a product into the process of deciding on a new product. Currently, this process is primarily based on market research that can only reveal the intention of consumers. Through the digitization of products, companies have access to large amounts of customer data that allow the application of data analytics methods. We provide a taxonomy of artificial intelligence, machine learning and data analysis, so that the notion of data analytics can be defined. Thus, the terms customer usage data, as well as a generic, five-stage product decision process (PDP) are defined and differentiated from consumer data and the product development process. Eventually, we show which data analytics methods on customer usage data can be used in order to tackle current challenges within the PDP. We incorporate the results of our structured literature review by connecting selected examples to our concept of the PDP. Our insights help to apply the proper data analytics methods in the PDP and thereby address the interplay between product decision and product development. Finally, future research directions for data analytics methods on customer usage data are put forward.}
}
@incollection{INDRAKUMARI2020165,
title = {Chapter Seven - The growing role of integrated and insightful big and real-time data analytics platforms},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {165-186},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S006524581930052X},
author = {Ranganathan Indrakumari and Thangamuthu Poongodi and Palanimuthu Suresh and Balusamy Balamurugan},
keywords = {Big data, Real-time data analytics, Big data platform},
abstract = {Digitization era is altering several industries which include the way in which the data is analyzed and it is inferred that about 2.7 Zettabytes of data exist in the digital world today. By 2020 the data generated per second for every human being will approximate amount to 1.7 megabytes and the volume of data would double every 2 years thus reach the 40 ZB point by 2020. Interactive Data Corporation (IDC) estimated that by the end of year 2020, the e-commerce transactions B2B and B2C will hit 450 billion per day on the internet. The advent of Big and real time Data has triggered disruptive changes in many fields and the exploding volume of different sources of data like heterogeneous data, data integration, spatio-temporal correlation of data, batch analytics and real-time analytics, data sharing, semantic interoperability requires the development of a scalable platform that can fuse multiple data layers to handles the data intelligently. In Big Data approaches, the challenge is not anymore to collect the data, but to draw valuable conclusions by properly analyzing them. The growth in Unstructured Data generated by business is irrefutable and they are under more pressure to preserve it for longer periods of time. To be clear, exploiting the collected data has been always considered by practitioners and researchers, but the huge velocity, heterogeneity and enormity of massive stream of real-time data shove the limits of the current storage, management and processing capabilities. Admittedly, the traditional method of Extract, Transform and Load (ETL) are challenged and cannot be applied on the emerging opportunistically and crowed sensed data streams. Some of these data streams are structured in a way that serve only one predefined purpose and cannot be directly used for other means. Yet, there are emerging unstructured data such as context-based data from the internet and social media as well as credit card transactions that is not clear if they can be used to better understand the mobility patterns. The analytical company Gartner states that by 2020 there will be over 26 billion interconnected devices. It is obvious, that they will produce massive amounts of meaningful data. Those data can be used for many applications such as real-time industrial equipment monitoring, traffic planning, automated maintenance, etc. Therefore, it is essential to develop modern system abstractions that allow us to resourcefully process huge and new data streams. This enormous amount of data urges the growth of integrated and insightful big and real-time data analytics Platforms. The upcoming contemporary technology like digital twin, integrates historical data from past machine usage to the current data. It uses sensors to collect the real-time data, working status and other operational data attached to the physical model. These components send the relevant data via a cloud-based system to the other side of the bridge with the help of data analytics platform which produces the required insights. The big and real-time data analytics Platforms assist to perform useful operations on data analytics as a complete package. For this purpose, data analytics platform are used to acquire constructive insight from the huge volume of data. Data analytics platform is an ecosystem of technologies and services that can help the businesses in increasing revenues, enhance operational efficiency, stabilize marketing campaigns and customer service efforts, respond more quickly to emerging market trends and gain a competitive edge over rivals. The data analytics platform finds the pattern and relationships in data by applying statistical techniques and communicates the results generated by analytical models to executives and end users to make decisions with the help of data visualization tools that display data on a single screen and can be updated in real time as new information becomes available. Big data and real-time data analytics platform supports the full spectrum of data types, protocols and integration to speed up and simplify the data wrangling process. The big data and real time platform provides accurate data, increase efficiency in the workspace, gives answers to complex questions along with security and hence it plays the key role in business analytics.}
}
@article{G2020235,
title = {Subjective Areas of Improvement: A Personalized Recommendation},
journal = {Procedia Computer Science},
volume = {172},
pages = {235-239},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920313600},
author = {Suganya G and Premalatha M and Piyush Dubey and Aryan Raj Drolia and Srihari S},
keywords = {E-Learning, Support Vector Machine, Improvement strategies},
abstract = {Gen Z-ers, being independent, self-confident and autonomous as their key characteristics are proven to be technologically more advanced than their previous generations. To reach the milestone of understanding and satisfying the Gen-Z community, and to make use of the digital adherence of this generation, a personalized recommender system using machine learning is designed that can recommend the areas which the students should strengthen themselves to lead and be distinct among their peers. The proposed work uses Support Vector Machine to predict the area of improvement which the student need to focus on. The proposed system will also recommend the list of online courses and materials that the users can make use of to strengthen themselves. The model is built using Python flask and Jupyter notebook and is tested using one public dataset and a private dataset. The results are convincing and accurate enough in identifying the proper areas that require improvement.}
}
@article{ZARGHAMI2022113831,
title = {Measuring project resilience – Learning from the past to enhance decision making in the face of disruption},
journal = {Decision Support Systems},
volume = {160},
pages = {113831},
year = {2022},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113831},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001026},
author = {Seyed Ashkan Zarghami and Ofer Zwikael},
keywords = {Decision support systems, Decision making, Disruptive events, Project, Resilience, Weibull distribution},
abstract = {Although projects are regularly exposed to disruptive events, the literature lacks an effective measurement system for project resilience. This gap presents challenges for decision makers because of the consequent lack of quantitative information about the level of resilience and its impact on project performance throughout a project's life. We argue that managers can be supported by a priori information about past similar projects as well as new data that evolve during disruption and recovery stages to enhance decision making by key project leaders, such as funders when approving new projects, project managers when developing the detailed plan, and project owners when approving corrective actions following a major disruption. Therefore, this paper develops a mathematical model to measure the level of project resilience by predicting disruption and recovery profiles based on past similar completed projects, as well as actual events unique to the project at hand. We illustrate and validate the model based on a portfolio of 43 major projects that faced disruptions from various sources. Our results provide the first empirical evidence to measure the impact of project resilience on the disruption and recovery behavior of real-life projects. The outputs of this research can be used as a decision support system that enables managers to make informed decisions throughout a project's life.}
}
@article{YANG2024123251,
title = {Artificial intelligence adoption in a professional service industry: A multiple case study},
journal = {Technological Forecasting and Social Change},
volume = {201},
pages = {123251},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123251},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524000477},
author = {Jiaqi Yang and Yvette Blount and Alireza Amrollahi},
keywords = {Artificial intelligence, Professional services, TOE framework, Adoption, Firm size},
abstract = {This study explores the factors influencing AI adoption in professional service firms. Grounded in the Technological-Organizational-Environmental (TOE) framework, we employed a qualitative, multiple case study approach, investigating three auditing firms of varying sizes through interviews and secondary document reviews. Our findings reveal six factors influencing AI adoption, including technology affordances and constraints, the firm's innovation management approaches and AI readiness, the competition environment, and the regulatory environment. Noteworthily, these factors vary significantly among the three firms. Larger firms, often operating in an environment with high AI penetration, primarily perceive the operating affordance of AI rather than marketing affordance. This means their AI adoption encompasses greater scale and depth than smaller firms. However, this expansive adoption exposes them to a widening gap in regulatory frameworks, hindering AI adoption. Moreover, smaller firms are characterized by weaker AI readiness, positioning them disadvantageously to mitigate the constraints imposed by AI. This study contributes to existing literature by offering a more holistic perspective on AI adoption in professional services.}
}
@article{PHAN2023113940,
title = {A decision support framework to incorporate textual data for early student dropout prediction in higher education},
journal = {Decision Support Systems},
volume = {168},
pages = {113940},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.113940},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623000155},
author = {Minh Phan and Arno {De Caigny} and Kristof Coussement},
keywords = {Decision support framework, Learning analytics, Student dropout prediction, Textual data, doc2vec, Segmentation},
abstract = {Managing student dropout in higher education is critical, considering its substantial impacts on students' lives, academic institutions, and society as a whole. Using predictive modeling can be instrumental for this task, as a means to identify dropouts proactively on the basis of student characteristics and their academic performance. To enhance these predictions, textual student feedback also might be relevant; this article proposes a hybrid decision support framework that combines predictive modeling with student segmentation efforts. A real-life data set from a French higher education institution, containing information of 14,391 students and 62,545 feedback documents, confirms the superior performance of the proposed framework, in terms of the area under the curve and top decile lift, compared with various benchmarks. In contributing to decision support system research, this study (1) proposes a new framework for automatic, data-driven segmentation of students based on textual data; (2) compares multiple text representation methods and confirms that incorporating student textual feedback data improves the predictive performance of student dropout models; and (3) establishes useful insights to help decision-makers anticipate and manage student dropout behaviors.}
}
@article{ONILE2024142,
title = {Smartgrid-based hybrid digital twins framework for demand side recommendation service provision in distributed power systems},
journal = {Future Generation Computer Systems},
volume = {156},
pages = {142-156},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000906},
author = {Abiodun E. Onile and Eduard Petlenkov and Yoash Levron and Juri Belikov},
keywords = {Demand side recommender systems, Hybrid digital twins, Industry 4.0, Distributed grid, Smart grid},
abstract = {Electricity consumers face challenges in selecting an optimal energy-saving plan, and this is a sustainability problem. To set consumers focus on sustainable energy management, developments around ”Industry 4.0” are needed to achieve an optimal balance between cost and energy consumption with a focus on cutting-edge machine-learning models and smart services introduction. Energy modelling is crucial for confronting the challenges introduced by the energy transition. This research is to promote digital twin (DT) technology in smart energy grids. The hybrid digital twin model is developed based on smart grid end-users’ electrical components. This approach emulates the component specification of the reference smart grid system and encourages a reduction in net energy consumption. Additionally, the application of AI connects smart meters, IoT devices, and the assets of the smart grids to the DT recreation of demand-side end-users for energy efficiency recommendation provision; this setup improves energy management, energy efficiency, and the usage of renewable energy resources. The novelty of this study is that the recent scope of demand-side recommendation schemes has been extended in the direction of cutting-edge Industry 4.0 hybrid DTs. Additionally, a prototype system and its functional characteristics have been proposed with the potential to pave the path for utilization in microgrid environments. Simulation results show a unique level of parallelism between the reference system and the developed model, along with a 36.8% reduction in net energy consumption following implementation of the recommendation advice.}
}
@article{ZHENG2023110114,
title = {Evolutionary machine learning builds smart education big data platform: Data-driven higher education},
journal = {Applied Soft Computing},
volume = {136},
pages = {110114},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110114},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623001321},
author = {Lu Zheng and Cong Wang and Xue Chen and Yihang Song and Zihan Meng and Ru Zhang},
keywords = {Evolutionary algorithm, Machine learning, Big data, Personalized course recommendation, Smart education},
abstract = {The development of machine learning has promoted the construction of smart education platforms. It is of great significance to deeply investigate the usage of machine learning techniques in smart education. In this paper, we explore how to utilize evolutionary algorithms and machine learning algorithms to build a smart education big data platform to promote the intelligent development of higher education and better assist the establishment of the smart education system. We combine evolutionary algorithms and machine learning models to build a personalized course recommendation model for the smart education big data platform, which uses deep belief networks and swarm intelligence evolutionary algorithms to recommend relevant content based on the interests of learners. We take advantage of the feature extraction of deep belief networks and combine supervised learning and unsupervised learning to design an intelligent recommendation model for teaching content. At the same time, we utilize the advantages of evolutionary algorithms to tune the model parameters to obtain the best parametric model. We compare our method with other methods on public dataset to show the model performance.}
}
@article{PARRETTI2020105069,
title = {Who, why and how: stakeholder attitudes toward marine non-indigenous species management in Portuguese Atlantic Islands},
journal = {Ocean & Coastal Management},
volume = {188},
pages = {105069},
year = {2020},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2019.105069},
url = {https://www.sciencedirect.com/science/article/pii/S096456911930938X},
author = {Paola Parretti and João Canning-Clode and Armando B. Mendes and Ana Cristina Costa},
keywords = {NIS, Management, Stakeholder involvement, Azores, Madeira},
abstract = {A key aspect for a successful management of marine non-indigenous species (NIS) is the cooperation with local stakeholders. In this study we assessed stakeholders' baseline knowledge and perceptions on marine NIS foreseeing support for their management in the Azores and Madeira archipelagos (Portugal). Survey questionnaires were designed to assess: i) current knowledge and stakeholder perception of NIS and associated problems; ii) influence of communication media on NIS dissemination; iii) stakeholder willingness to be involved in NIS management actions and; iv) which factors predict stakeholder participation in NIS management actions. Face-to face survey questionnaires were administered in recreational marinas of São Miguel (Azores) and Madeira islands. Four groups of stakeholders classified based on their putative relation with NIS were selected as target. In total 214 survey questionnaires were collected and analysed. Our results showed no significant differences between stakeholders based in São Miguel and the ones from Madeira. Overall a strong baseline knowledge on NIS and associated consequences was found but a weak accuracy to identify NIS examples was demonstrated. Types of communication media revealed to be stakeholder-specific and information-specific. Decision tree predictive models showed three powerful factors to forecast the respondent's participation in NIS management actions: 1- to belong to one of the four stakeholder groups; 2- to have a good level of NIS knowledge; and 3- to have the perception that NIS introduction is an important issue. Despite the positive attitude exhibited by stakeholders in engaging NIS management, our results highlighted a consensus to attribute governmental responsibility to such activities. The results of this work constitute a baseline to develop further management actions aiming to reduce the local spread of marine NIS and so contributing to a better environmental status of Portuguese marine waters.}
}
@article{DOYLEKENT2022159,
title = {A Research Cluster's Vision for a Pilot Factory in the South East Technological University of Ireland},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {39},
pages = {159-164},
year = {2022},
note = {21st IFAC Conference on Technology, Culture and International Stability TECIS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.12.028},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322030683},
author = {Mary Doyle-Kent and Brenda O'Neill and Breda Walsh Shanahan and John Organ and Liam Doyle and Sinead O'Neill and Orlagh Costello and Noeleen Donnelly},
keywords = {Human Centered, Third Level Education, Engineering, Inclusivity, Diversity, ICT, Continuous Adult Learning, Micro Credentials, Industry 4.0, Industry 5.0},
abstract = {A research activity proposed for the South East Technological University (SETU), Waterford in Ireland from its CONNEXIONS Catalyst pathway grant is the development of an interdisciplinary research cluster within INSYTE, which spans the schools of Business, Education, Science, Engineering and Industry. It aims to co-evolve cutting-edge models of educational delivery. This research cluster has emerged from the work of the INSYTE Centre located in the INSYTE-Cooley Research Lab (ICRL) in the Digitization Hub of the Luke Wadding Library at SETU, Waterford. Educational techniques have evolved over the past decades and these advances have not necessarily filtered down to various disciplines, e.g. engineering. The focus of this research cluster is to have the ‘Learner’ at the center of the ‘Learning Experience’ and to wrap the technology around them. This in turn, will increase the adoption and usage of such technologies. It will do this by cross-fertilization of knowledge over diverse domains to produce customised human-centred solutions for communities. Equality, diversity and inclusion are critical strands in this research. This paper shows that the European Union (EU) is supportive of changes in education required to close the skills gap in engineering and to enable reskilling to support Industry 5.0. In addition to the creation of cutting-edge models of educational delivery the vision of this multidisciplinary human centered research cluster is the establishment of a state-of-the-art pilot factory as an Irish national initiative in SETU, Waterford.}
}
@article{PETRESCU20221090,
title = {Collaborative decision-making in online education},
journal = {Procedia Computer Science},
volume = {199},
pages = {1090-1094},
year = {2022},
note = {The 8th International Conference on Information Technology and Quantitative Management (ITQM 2020 & 2021): Developing Global Digital Economy after COVID-19},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922001399},
author = {Daniela Petrescu and Dumitru Enache and Luminita Duta},
keywords = {collaborative decision making, online education, learning during pandemic},
abstract = {During the last two years, marked by the COVID-19 pandemic, education was almost entirely pushed to online learning. Different online platforms, software and applications were used by instructors to accomplish the objectives of educational curricula. However, these tools do not provide a complete view on the learners’ progress in accumulating skills and competences since there are no instruments to measure the efficiency of the remote educational process. This problem can be resolved by integrating collaborative facilities using decision-making modules in the digital space of e-learning. This paper emphasizes the importance of decision support systems in online education.}
}
@article{ISSAH2023100204,
title = {A systematic review of the literature on machine learning application of determining the attributes influencing academic performance},
journal = {Decision Analytics Journal},
volume = {7},
pages = {100204},
year = {2023},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2023.100204},
url = {https://www.sciencedirect.com/science/article/pii/S2772662223000449},
author = {Iddrisu Issah and Obed Appiah and Peter Appiahene and Fuseini Inusah},
keywords = {Machine learning, Data mining, Demographic, Pre-tertiary, Predictive analytics, Learning analytics},
abstract = {Academic institutions operate in an extremely demanding and competitive environment. Some difficulties confronting most schools are delivering high-quality education to the students, developing systems for evaluating student performance, analyzing performance, and recognizing the future demands of their learners. Also, due to the paradigm shift due to the computerization of school data management, educational stakeholders, including the machine learning (ML) community, have taken an interest in analyzing performance traits using academic and non-academic factors. This systematic literature review identifies various machine learning methods based on 84 selected publications. It shows how researchers have been able to pattern-map student characteristics and their influence on school performance. An attempt is made to answer how the overall study coverage of student characteristics and the ML methods are employed to predict students’ performance. An analysis of the 84 papers highlights that, student characteristics predominantly influencing performance are academic and demographic attributes. The study further shows that classification and decision trees are the most widely used methods and algorithms. The review also reveals population and practical knowledge gaps due to a lack of research on basic academic performance and prescription of intervention plans for averting poor performance through mapping these influential characteristics to student accomplishment. To bridge these perceived gaps, the scope of the population sample needs a benchmarked dataset and embedding the appropriate intervention outlines that will map the learner’s performance early in their school life.}
}
@article{MELESSE20222743,
title = {Digital Twin for Inventory Planning of Fresh Produce},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2743-2748},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.134},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021474},
author = {Tsega Y. Melesse and Matteo Bollo and Valentina Di Pasquale and Stefano Riemma},
keywords = {Time-series, Digital Twin, Predictive Forecast, Inventory Planning, Fruits},
abstract = {The management of perishable food inventory demands special attention. Fruits quickly lose their freshness and perish if they are not consumed within a specified period. It is critical to develop a management tool based on the Internet of Things that can efficiently integrate all the dynamic data associated with various types of resources in real-time along the supply chain. This research is part of a comprehensive supply chain framework developed to analyze food bank logistics supply chain interactions. The study will mainly focus on the use of historical time-series data to create a digital twin that can anticipate future events. The digital twin framework was built based on the operational trend of the Italian food bank to strengthen the decision support system related to the fresh food inventory. The SAP Analytics Cloud was used to create a solution that would help the organization better satisfy consumer needs by reducing fruit waste in the inventory.}
}
@article{LIU20221603,
title = {A comparative study on the effect of data imbalance on software defect prediction},
journal = {Procedia Computer Science},
volume = {214},
pages = {1603-1616},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.349},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020610},
author = {Yanbin Liu and Wen Zhang and Guangjie Qin and Jiangpeng Zhao},
keywords = {software defect prediction, data imbalance, sampling techniques, machine learning},
abstract = {In the current stage, software defect prediction is suffering the imbalanced data problem. Traditional methods are insensitive to defect-prone modules and tend to predict defect-prone modules as defect-free modules. To deal with this problem, sampling techniques are adopted to rebalance the defect-prone and defect-free data to train the predictive model in order to improve the performance. However, it is not clear on the combined effect of the sampling techniques and the machine learning classifiers on the performance of software defect prediction. The intent of the paper is to study the performance impact on defect prediction incurred by different combinations of sampling techniques and machine learning classifiers. Specifically, we investigate three types of sampling techniques as resampling, spread subsampling and SMOTE (Synthetic Minority Over-sampling Technique), and five types of machine learning classifiers as C4.5, naive Bayes, logistic regression, support vector machine and deep learning to study their combined effect on defect prediction. By using the Friedman test and Nemenyi test, we find that there isn't an optimal method among all the 12 combinations in defect prediction. However, support vector machine and deep learning have produced the best performance stably among all the investigated projects. With ANOVA analysis, we find that the sampling techniques have great impact on the outcomes of defect prediction because they produce different data distributions for model training. Nevertheless, the sampling proportion has significant impacts on TPR (True Positive Ratio) and FPR (False Positive Ratio) while it can merely influence the AUC (Area under Curve) and Balance of logistic regression. We explain the experimental results in the paper.}
}
@article{WU2024114176,
title = {Predicting financial distress using current reports: A novel deep learning method based on user-response-guided attention},
journal = {Decision Support Systems},
volume = {179},
pages = {114176},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114176},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000095},
author = {Chenyang Wu and Cuiqing Jiang and Zhao Wang and Yong Ding},
keywords = {Financial distress, Current report, User response, Deep learning, Attention mechanism},
abstract = {Effective financial distress prediction (FDP) can discover a company's potential financial risks and support relevant decisions in a timely manner. Previous studies on FDP have mostly focused on using financial indicators and periodic reports. Compared with periodic reports, current reports disclose major events in a timelier manner. But leveraging the information in current reports involves the critical challenges of capturing the complex semantics and measuring the importance of heterogeneous events. To this end, we propose a novel deep learning method, a user-response-guided deep attention network (URGDAN), to predict financial distress using current reports. In the proposed method, we construct a deep learning architecture to integrate financial indicators, current report texts, and user responses. URGDAN leverages the user responses to current reports to guide the semantic feature representation of the reports, it also identifies event information that has a significant correlation with company financial distress. Empirical evaluation shows that URGDAN significantly improves predictive performance and can accurately determine the importance of different current reports. Our work provides practical implications for creditors and investors.}
}
@article{MAJEED2021102026,
title = {A big data-driven framework for sustainable and smart additive manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {102026},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102026},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302374},
author = {Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin},
keywords = {Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization},
abstract = {From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.}
}
@article{SHAIK2023100003,
title = {Sentiment analysis and opinion mining on educational data: A survey},
journal = {Natural Language Processing Journal},
volume = {2},
pages = {100003},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2022.100003},
url = {https://www.sciencedirect.com/science/article/pii/S2949719122000036},
author = {Thanveer Shaik and Xiaohui Tao and Christopher Dann and Haoran Xie and Yan Li and Linda Galligan},
keywords = {Sentiment analysis, Opinion mining, Student feedback, AI, BERT, Deep learning},
abstract = {Sentiment analysis AKA opinion mining is one of the most widely used NLP applications to identify human intentions from their reviews. In the education sector, opinion mining is used to listen to student opinions and enhance their learning–teaching practices pedagogically. With advancements in sentiment annotation techniques and AI methodologies, student comments can be labelled with their sentiment orientation without much human intervention.​ In this review article, (1) we consider the role of emotional analysis in education from four levels: document level, sentence level, entity level, and aspect level, (2) sentiment annotation techniques including lexicon-based and corpus-based approaches for unsupervised annotations are explored, (3) the role of AI in sentiment analysis with methodologies like machine learning, deep learning, and transformers are discussed, (4) the impact of sentiment analysis on educational procedures to enhance pedagogy, decision-making, and evaluation are presented. Educational institutions have been widely invested to build sentiment analysis tools and process their student feedback to draw their opinions and insights. Applications built on sentiment analysis of student feedback are reviewed in this study. Challenges in sentiment analysis like multi-polarity, polysemous, negation words, and opinion spam detection are explored and their trends in the research space are discussed. The future directions of sentiment analysis in education are discussed.}
}
@article{SYMITSI2021605,
title = {The informational value of employee online reviews},
journal = {European Journal of Operational Research},
volume = {288},
number = {2},
pages = {605-619},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720305269},
author = {Efthymia Symitsi and Panagiotis Stamolampros and George Daskalakis and Nikolaos Korfiatis},
keywords = {Analytics, Employee online reviews, Topic modeling, Big data, Decision processes},
abstract = {This paper investigates the informational value of online reviews posted by employees for their employer, a rather untapped source of online information from employees, using a sample of 349,550 reviews from 40,915 UK firms. We explore this novel form of electronic Word-of-Mouth (e-WOM) from different perspectives, namely: (i) its information content as a tool to identify the drivers of job satisfaction/dissatisfaction, (ii) its predictive ability on firm financial performance and (iii) its operational and managerial value. Our approach considers both the rating score as well as the review text through a probabilistic topic modeling method, providing also a roadmap to quantify and exploit employee big data analytics. The novelty of this study lies in the coupling of structured and unstructured data for deriving managerial insights through a battery of econometric, financial and operational research methodologies. Our empirical analyses reveal that employee online reviews have informational value and incremental predictability gains for a firm's internal and external stakeholders. The results indicate that when models integrate structured and unstructured big data there are leveraged opportunities for firms and managers to enhance the informativeness of decision support systems and in turn, gain competitive advantage.}
}
@article{LI2022121355,
title = {Evaluating the impact of big data analytics usage on the decision-making quality of organizations},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121355},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121355},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521007861},
author = {Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo},
keywords = {Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms},
abstract = {Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.}
}
@incollection{SALINI202335,
title = {Chapter 3 - Digital twin and artificial intelligence in industries},
editor = {Rajesh Kumar Dhanaraj and Ali Kashif Bashir and Vani Rajasekar and Balamurugan Balusamy and Pooja Malik},
booktitle = {Digital Twin for Smart Manufacturing},
publisher = {Academic Press},
pages = {35-58},
year = {2023},
isbn = {978-0-323-99205-3},
doi = {https://doi.org/10.1016/B978-0-323-99205-3.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323992053000146},
author = {S. Salini and B. Persis Urbana Ivy},
keywords = {Digital twins, cyber twins, DTS, artificial intelligence, taxonomy, meta-dimension},
abstract = {Applications of digital twin (DT) contribute to smart manufacturing via the integration of the cyber domain and the real world. Artificial Intelligence (AI) that is based on machine learning (ML) is generally recognized as being among the most promising technological developments in the manufacturing industry. Despite this, machine learning techniques call for enormous quantities of high-quality training datasets, and in the case of supervised ML, human labeling of such datasets is often necessary. Collecting and analyzing historical performance data to uncover operational insights has been a long-standing driver of efficiency and innovation for a wide variety of businesses, across all industrial sectors and even farther afield. However, the possible advantages of this method have not yet been leveraged to their maximum. There is a tendency for vast volumes of data, spanning anything from the current state of subsurface assets to the attitude that customers express on social media postings, to go mostly unrecorded and unanalyzed. This may be a problem for businesses. However, due to the limitations of existing IoT platforms in representing complex industrial machines, supporting production line-based application testing, and the lack of cost models for application cost/benefit analysis, the development of such Industry 4.0 applications is currently quite costly. Cyber Twins (CTs) are an extension of DTs that we propose using to facilitate the rapid and low-cost creation of Industry 4.0 applications. CTs may incorporate machine simulators and provide semantic descriptions of the machines they model, which makes it possible to test applications without putting the production line at risk or incurring additional costs. The applications for Industry 4.0 are based on CT and the accompanying cost models are the primary topics of this research. Second, the paper provides empirical results that connect existing academic literature to the interdependencies between edge components and internal and external services and systems in the Industry 4.0 paradigm. The novel aspect of this research is a new method for creating a DT, or a virtual representation that acts as the real-time digital counterpart of a physical object or process as defined by a conceptual model. This is the chapter’s primary contribution. The approach that was used to conduct the research for this article was conceptually similar to an investigation of complex, linked, and coupled systems using the grounded theory. This research provides an outline of how to increase AI development in IoT networks via integrating human-computer interactions across different knowledge management systems. These interactions take place in different information management systems. In addition, the previously used real-time controllable method has been improved, as well as optimized. Additionally, it performs simulation testing on the method in both a serial and a parallel fashion. The relevant mining time of the improved algorithm is shown to be substantially less than the time required by the conventional data mining method when applied to the same dataset, as shown by the test results when the ideal scenario of the parallel algorithm has been achieved. The amount of time required for conventional mining is around three and a half times as long as the amount of time required for data mining in this study, and the amount of power that the optimization method requires to operate has been decreased to 20W. With the help of AI, the manufacturing model (DT) and the decision support system are able to include all of the information generated by these subsystems. DT data technology facilitates the identification of pertinent information that can be processed and used to inform managerial decision-making. Applications of AI may open a wide variety of doors in the manufacturing industry, leading to the creation of new business models and the improvement of existing procedures. A formal model was used to define the landscape to guarantee the feasibility of conducting an in-depth analysis of the status and progression of the landscape, while taking into account DT and other technologies. The adoption of DT and Industrial Internet of Things for modeling of a real enterprise’s production process was taken into consideration. However, Digital Twin Store (DTS) has stricter standards, particularly in regard to the real-time engagement that is expected. A new technique that satisfies the aforementioned characteristics may be found in the use of AI, which is an efficient way of boosting the intelligence of the physical shop floor. In this piece of research, a conceptual framework for AI-enhanced DTS in interaction is presented. The real-time engagement is significantly increased by AI-enhanced DTS, thanks to its predictive control. The AI-enhanced interaction implementation technique in DTS is also discussed in length in this presentation. Finally, the DTS has implemented the necessary technology to support interaction.}
}
@article{EKPENYONG2024110091,
title = {Intelligent optimal preventive replacement maintenance policy for non-repairable systems},
journal = {Computers & Industrial Engineering},
volume = {190},
pages = {110091},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110091},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224002122},
author = {Moses Effiong Ekpenyong and Nse Sunday Udoh},
keywords = {Feature engineering, Knowledge base, Loglogistic distribution, Replacement maintenance policy, Non-repairable system, System availability and reliability},
abstract = {This paper developed optimal maintenance policies for industrial machines to ensure uninterrupted production. Analytical and intelligent replacement models were constructed using real-time data from a commercial photocopier operator’s expenditure logbook between 2012 and 2023. A goodness-of-fit test affirmed a log-logistic failure distribution with shape and scale parameters of 1.7233 and 763.9220, respectively. Probability functions from this distribution established replacement models, determining optimal preventive maintenance conditions. Through feature engineering additional features crucial for operational and cost optimization were abstracted, with 4,881 unique data points simulated from boundary conditions defined by the original or raw data. Analytical results showed that the failure distribution-based replacement model outperformed the hazard function-based model in minimum replacement time and unit maintenance cost over time, while key parameters such as availability, reliability, and failure occurrence probability remained constant at 96 %, 94 %, and 0.07 %, respectively. Machine learning models (Extreme Gradient Boosting: XGBoost, Support Vector Machine: SVM, and Multilayer Perceptron: MLP) demonstrated very high accuracy in availability (XGBoost = 99.49 %, SVM = 99.18 %, RF = 100 %, MLP = 99.80 %), reliability (XGBoost = 99.49 %, SVM = 99.18 %, RF = 100 %, MLP = 99.80 %), and maintenance cost (XGBoost = 100 %, SVM = 100 %, RF = 100 %, MLP = 100 %), exceeding results of the analytical models.}
}
@article{DLUGOSCH2022103285,
title = {Combining analytics and simulation methods to assess the impact of shared, autonomous electric vehicles on sustainable urban mobility},
journal = {Information & Management},
volume = {59},
number = {5},
pages = {103285},
year = {2022},
note = {Big Data Analytics for Sustainability},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103285},
url = {https://www.sciencedirect.com/science/article/pii/S037872061930182X},
author = {Oliver Dlugosch and Tobias Brandt and Dirk Neumann},
keywords = {Urban mobility, Sustainability, Simulation, Decision support, Shared autonomous electric vehicles},
abstract = {Urban mobility is currently undergoing three fundamental transformations with the sharing economy, electrification, and autonomous vehicles changing how people and goods move across cities. In this paper, we demonstrate the valuable contribution of decision support systems that combine data-driven analytics and simulation techniques in understanding complex systems such as urban transportation. Using the city of Berlin as a case study, we show that shared, autonomous electric vehicles can substantially reduce resource investments while keeping service levels stable. Our findings inform stakeholders on the trade-off between economic and sustainability-related considerations when fostering the transition to sustainable urban mobility.}
}
@article{TAHERDOOST20241649,
title = {A Critical Review on Cybersecurity Awareness Frameworks and Training Models},
journal = {Procedia Computer Science},
volume = {235},
pages = {1649-1663},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924008329},
author = {Hamed Taherdoost},
keywords = {Cybersecurity, Awareness, Frameworks, Education, Training, SecurityLifecycle},
abstract = {Due to its inherent weaknesses, protecting data and maintaining its integrity is crucial in the changing world of cybersecurity. This evaluation employs a quantitative methodology to evaluate several cybersecurity awareness and training approaches to illuminate their efficacy in lowering costs and security incidents for a company while enhancing cyber resilience and security posture. Our thorough research, which is supported by actual evidence, emphasizes the real advantages of clear cybersecurity awareness and training activities. These measures enable businesses to drastically reduce security incidents and realize verifiable cost reductions. Additionally, these initiatives act as accelerators for improving overall cyber resilience, therefore bolstering an organization’s security posture. This in-depth analysis offers a data-driven perspective on these models’ respective capabilities and highlights the critical role that strategic cybersecurity education and awareness play in defending against a constantly changing threat environment.}
}
@article{ZBIKOWSKI2021102555,
title = {A machine learning, bias-free approach for predicting business success using Crunchbase data},
journal = {Information Processing & Management},
volume = {58},
number = {4},
pages = {102555},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102555},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000595},
author = {Kamil Żbikowski and Piotr Antosiuk},
keywords = {Startups, Supervised learning, XGBoost, Crunchbase, Look-ahead bias},
abstract = {Predicting the success of a business venture has always been a struggle for both practitioners and researchers. However, thanks to companies that aggregate data about other firms, it has become possible to create and validate predictive models based on an unprecedented amount of real-world examples. In this study, we use data obtained from one of the largest platforms integrating business information – Crunchbase. Our final training set consisted of 213 171 companies. This work aims to create a predictive model based on machine learning for the purpose of forecasting a company’s success. Many similar attempts have been made in recent years. Plenty of those experiments, often conducted with the use of data gathered from several different sources, reported promising results. However, we found that very often they were significantly biased by their use of data containing information that was a direct consequence of a company reaching some level of success (or failure). Such an approach is a classic example of the look-ahead bias. It leads to very optimistic test results, but any attempt at using such an approach in a real-world scenario may result in dramatic consequences. We designed our experiments in a way that would prevent the leaking of any information unavailable at the decision moment to the training set. We compared three algorithms – logistic regression, support vector machine, and the gradient boosting classifier. Despite the conscious decision to limit the number of predictors, we reached very promising results in terms of precision, recall, and F1 scores which, for the best model, were 57%, 34%, and 43% respectively. The best outcomes were obtained with the gradient boosting classifier. We give detailed information about the importance of different features, with the top three being country and region that the company operates in and the company’s industry. Our model can be applied directly as a decision support system for different types of venture capital funds.}
}